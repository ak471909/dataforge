================================================================================
TRAINING DATA BOT - COMPLETE PROJECT CODE
================================================================================

Total files: 73
Generated: C:\Users\nandu\Documents\my_training_data_bot
================================================================================


================================================================================
FILE: api\__init__.py
================================================================================

"""API module for Training Data Bot."""

# from api.server import app

# __all__ = ["app"]


================================================================================
FILE: api\dependencies.py
================================================================================

"""
Shared dependencies for API routes.
"""

from fastapi import HTTPException
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from training_data_bot import TrainingDataBot

# Global bot instance
_bot_instance = None


def set_bot(bot: "TrainingDataBot") -> None:
    """Set the global bot instance."""
    global _bot_instance
    _bot_instance = bot


def get_bot() -> "TrainingDataBot":
    """Get the global bot instance."""
    if _bot_instance is None:
        raise HTTPException(status_code=503, detail="Bot not initialized")
    return _bot_instance


================================================================================
FILE: api\middleware\__init__.py
================================================================================

"""API middleware module."""

from api.middleware.auth import AuthMiddleware
from api.middleware.rate_limit import limiter

__all__ = ["AuthMiddleware", "limiter"]


================================================================================
FILE: api\middleware\auth.py
================================================================================

"""
API Key authentication middleware.
"""

from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware
import os
from dotenv import load_dotenv

load_dotenv()


class AuthMiddleware(BaseHTTPMiddleware):
    """API key authentication middleware."""
    
    def __init__(self, app):
        super().__init__(app)
        self.api_key = os.getenv("API_KEY", "dev-key-12345")
        print(f"[AUTH] Loaded API key from env: {self.api_key}")
        # Paths that don't require authentication
        # Paths that don't require authentication
        self.public_paths = {
            "/", 
            "/health", 
            "/docs", 
            "/openapi.json", 
            "/redoc",
            "/api/v1/monitoring/health/basic",
            "/api/v1/monitoring/health/detailed",
            "/api/v1/monitoring/metrics",
            "/api/v1/monitoring/status"
        }
    
    async def dispatch(self, request: Request, call_next):
        """Check API key for protected endpoints."""
        # Skip auth for public paths
        if request.url.path in self.public_paths:
            return await call_next(request)
        
        # Check for API key in header
        api_key = request.headers.get("X-API-Key")

        # Debug logging
        print(f"[AUTH] Request path: {request.url.path}")
        print(f"[AUTH] Received API key: {api_key}")
        print(f"[AUTH] Expected API key: {self.api_key}")
        
        if not api_key:
            raise HTTPException(
                status_code=401,
                detail="API key missing. Include 'X-API-Key' header."
            )
        
        if api_key != self.api_key:
            raise HTTPException(
                status_code=403,
                detail="Invalid API key"
            )
        
        return await call_next(request)


================================================================================
FILE: api\middleware\rate_limit.py
================================================================================

"""
Rate limiting middleware.
"""

from slowapi import Limiter
from slowapi.util import get_remote_address


limiter = Limiter(key_func=get_remote_address)


================================================================================
FILE: api\routes\__init__.py
================================================================================

"""API routes module."""

# from api.routes import documents, processing, datasets

# __all__ = ["documents", "processing", "datasets"]


================================================================================
FILE: api\routes\datasets.py
================================================================================

"""
Dataset management endpoints.
"""

from fastapi import APIRouter, HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Optional
from pathlib import Path
from uuid import UUID

from training_data_bot.core import ExportFormat
from api.dependencies import get_bot

router = APIRouter()


class ExportRequest(BaseModel):
    """Request model for dataset export."""
    format: str = "jsonl"
    split_data: bool = True


@router.get("/list")
async def list_datasets():
    """List all created datasets."""
    bot = get_bot()
    
    datasets = []
    for dataset_id, dataset in bot.datasets.items():
        datasets.append({
            "id": str(dataset.id),
            "name": dataset.name,
            "description": dataset.description,
            "total_examples": dataset.total_examples,
            "created_at": dataset.created_at.isoformat()
        })
    
    return {
        "total": len(datasets),
        "datasets": datasets
    }


@router.get("/{dataset_id}")
async def get_dataset(dataset_id: str):
    """Get details of a specific dataset."""
    bot = get_bot()
    
    try:
        ds_uuid = UUID(dataset_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid dataset ID")
    
    if ds_uuid not in bot.datasets:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    dataset = bot.datasets[ds_uuid]
    
    return {
        "id": str(dataset.id),
        "name": dataset.name,
        "description": dataset.description,
        "total_examples": dataset.total_examples,
        "train_split": dataset.train_split,
        "validation_split": dataset.validation_split,
        "test_split": dataset.test_split,
        "task_distribution": {k.value: v for k, v in dataset.task_distribution.items()},
        "created_at": dataset.created_at.isoformat()
    }


@router.get("/{dataset_id}/examples")
async def get_dataset_examples(
    dataset_id: str,
    limit: int = 10,
    offset: int = 0
):
    """Get examples from a dataset with pagination."""
    bot = get_bot()
    
    try:
        ds_uuid = UUID(dataset_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid dataset ID")
    
    if ds_uuid not in bot.datasets:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    dataset = bot.datasets[ds_uuid]
    examples = dataset.examples[offset:offset + limit]
    
    return {
        "dataset_id": str(dataset.id),
        "total_examples": len(dataset.examples),
        "offset": offset,
        "limit": limit,
        "examples": [
            {
                "id": str(ex.id),
                "input_text": ex.input_text[:200] + "..." if len(ex.input_text) > 200 else ex.input_text,
                "output_text": ex.output_text[:200] + "..." if len(ex.output_text) > 200 else ex.output_text,
                "task_type": ex.task_type.value,
                "quality_scores": ex.quality_scores
            }
            for ex in examples
        ]
    }


@router.post("/{dataset_id}/export")
async def export_dataset(dataset_id: str, request: ExportRequest):
    """Export a dataset to file."""
    bot = get_bot()
    
    try:
        ds_uuid = UUID(dataset_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid dataset ID")
    
    if ds_uuid not in bot.datasets:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    dataset = bot.datasets[ds_uuid]
    
    # Validate format
    valid_formats = ["jsonl", "json", "csv", "parquet"]
    if request.format not in valid_formats:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid format: {request.format}. Valid: {valid_formats}"
        )
    
    # Create export directory
    export_dir = Path("output/api_exports")
    export_dir.mkdir(parents=True, exist_ok=True)
    
    # Export path
    output_path = export_dir / f"{dataset.name}.{request.format}"
    
    try:
        # Export dataset
        result_path = await bot.export_dataset(
            dataset=dataset,
            output_path=output_path,
            format=ExportFormat(request.format),
            split_data=request.split_data
        )
        
        return {
            "status": "success",
            "dataset_id": str(dataset.id),
            "export_path": str(result_path),
            "format": request.format,
            "split_data": request.split_data
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{dataset_id}/download")
async def download_dataset(dataset_id: str, format: str = "jsonl"):
    """Download a dataset file."""
    bot = get_bot()
    
    try:
        ds_uuid = UUID(dataset_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid dataset ID")
    
    if ds_uuid not in bot.datasets:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    dataset = bot.datasets[ds_uuid]
    
    # Find exported file
    export_dir = Path("output/api_exports")
    file_path = export_dir / f"{dataset.name}_train.{format}"
    
    if not file_path.exists():
        raise HTTPException(
            status_code=404,
            detail="File not exported yet. Export dataset first."
        )
    
    return FileResponse(
        path=file_path,
        filename=f"{dataset.name}.{format}",
        media_type="application/octet-stream"
    )


@router.delete("/{dataset_id}")
async def delete_dataset(dataset_id: str):
    """Delete a dataset."""
    bot = get_bot()
    
    try:
        ds_uuid = UUID(dataset_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid dataset ID")
    
    if ds_uuid not in bot.datasets:
        raise HTTPException(status_code=404, detail="Dataset not found")
    
    del bot.datasets[ds_uuid]
    
    return {"status": "success", "message": "Dataset deleted"}


================================================================================
FILE: api\routes\documents.py
================================================================================

"""
Document management endpoints.
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import List, Optional
from pathlib import Path
import shutil
from uuid import uuid4

from training_data_bot.core import DocumentType
from api.dependencies import get_bot
from training_data_bot.core import get_logger

logger = get_logger("api.documents")

router = APIRouter()

# Temporary upload directory
UPLOAD_DIR = Path("temp/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)


@router.post("/upload")
async def upload_documents(
    files: List[UploadFile] = File(...),
    description: Optional[str] = Form(None)
):
    """
    Upload one or more documents for processing.
    
    Supports: PDF, DOCX, TXT, MD, HTML, CSV, JSON
    """
    bot = get_bot()
    uploaded_files = []
    
    try:
        logger.info(f"Received upload request with {len(files)} files")
        
        file_paths = []
        
        for file in files:
            # Validate file extension
            file_ext = Path(file.filename).suffix.lower().lstrip('.')
            
            if file_ext not in ['pdf', 'docx', 'txt', 'md', 'html', 'csv', 'json']:
                raise HTTPException(
                    status_code=400,
                    detail=f"Unsupported file type: {file_ext}"
                )
            
            # Generate unique filename
            unique_filename = f"{uuid4()}_{file.filename}"
            file_path = UPLOAD_DIR / unique_filename
            
            logger.info(f"Saving file to: {file_path}")
            
            # Save file
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            file_paths.append(str(file_path))
            
            uploaded_files.append({
                "filename": file.filename,
                "path": str(file_path),
                "size": file_path.stat().st_size,
                "type": file_ext
            })
        
        # Load documents into bot
        logger.info(f"Loading {len(file_paths)} documents into bot")
        documents = await bot.load_documents(file_paths)
        logger.info(f"Successfully loaded {len(documents)} documents")
        
        return {
            "status": "success",
            "files_uploaded": len(uploaded_files),
            "documents_loaded": len(documents),
            "files": uploaded_files
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
@router.get("/list")
async def list_documents():
    """List all uploaded documents."""
    bot = get_bot()
    
    documents = []
    for doc_id, doc in bot.documents.items():
        documents.append({
            "id": str(doc.id),
            "title": doc.title,
            "type": doc.doc_type.value,
            "word_count": doc.word_count,
            "char_count": doc.char_count,
            "source": doc.source,
            "created_at": doc.created_at.isoformat()
        })
    
    return {
        "total": len(documents),
        "documents": documents
    }


@router.get("/{document_id}")
async def get_document(document_id: str):
    """Get details of a specific document."""
    bot = get_bot()
    
    from uuid import UUID
    try:
        doc_uuid = UUID(document_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid document ID")
    
    if doc_uuid not in bot.documents:
        raise HTTPException(status_code=404, detail="Document not found")
    
    doc = bot.documents[doc_uuid]
    
    return {
        "id": str(doc.id),
        "title": doc.title,
        "type": doc.doc_type.value,
        "word_count": doc.word_count,
        "char_count": doc.char_count,
        "source": doc.source,
        "content_preview": doc.content[:500] + "..." if len(doc.content) > 500 else doc.content,
        "created_at": doc.created_at.isoformat(),
        "metadata": doc.metadata
    }


@router.delete("/{document_id}")
async def delete_document(document_id: str):
    """Delete a document."""
    bot = get_bot()
    
    from uuid import UUID
    try:
        doc_uuid = UUID(document_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid document ID")
    
    if doc_uuid not in bot.documents:
        raise HTTPException(status_code=404, detail="Document not found")
    
    del bot.documents[doc_uuid]
    
    return {"status": "success", "message": "Document deleted"}


================================================================================
FILE: api\routes\monitoring.py
================================================================================

"""
Monitoring and observability endpoints.
"""

from fastapi import APIRouter
from typing import Dict
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from monitoring.health import HealthCheck
from monitoring.metrics import metrics
from api.dependencies import get_bot

router = APIRouter()


@router.get("/health/basic")
async def health_basic():
    """Basic health check."""
    try:
        bot = get_bot()
        return {
            "status": "healthy",
            "bot_initialized": True
        }
    except:
        return {
            "status": "unhealthy",
            "bot_initialized": False
        }


@router.get("/health/detailed")
async def health_detailed():
    """Detailed health check with system metrics."""
    return HealthCheck.comprehensive_check()


@router.get("/metrics")
async def get_metrics():
    """Get application metrics."""
    return metrics.get_metrics()


@router.get("/status")
async def get_status():
    """Get comprehensive application status."""
    bot = get_bot()
    bot_stats = bot.get_statistics()
    
    return {
        "status": "running",
        "health": HealthCheck.comprehensive_check(),
        "metrics": metrics.get_metrics(),
        "bot_statistics": bot_stats
    }


@router.post("/metrics/reset")
async def reset_metrics():
    """Reset metrics counters."""
    metrics.reset()
    return {"status": "success", "message": "Metrics reset"}


================================================================================
FILE: api\routes\processing.py
================================================================================

"""
Document processing endpoints.
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from pathlib import Path

from training_data_bot.core import TaskType
from api.dependencies import get_bot

router = APIRouter()


class ProcessRequest(BaseModel):
    """Request model for document processing."""
    document_paths: List[str]
    task_types: List[str] = ["qa_generation", "summarization"]
    quality_filter: bool = True
    chunk_size: Optional[int] = None
    chunk_overlap: Optional[int] = None


class ProcessUploadedRequest(BaseModel):
    """Request model for processing uploaded documents."""
    task_types: List[str] = ["qa_generation", "summarization"]
    quality_filter: bool = True


@router.post("/process")
async def process_documents(request: ProcessRequest):
    """
    Process documents from file paths.
    
    Args:
        document_paths: List of file paths to process
        task_types: Types of tasks to generate
        quality_filter: Whether to filter by quality threshold
        chunk_size: Optional chunk size override
        chunk_overlap: Optional chunk overlap override
    """
    bot = get_bot()
    
    # Validate task types
    valid_tasks = ["qa_generation", "summarization", "classification"]
    for task in request.task_types:
        if task not in valid_tasks:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid task type: {task}. Valid: {valid_tasks}"
            )
    
    # Convert to TaskType enums
    task_types = [TaskType(task) for task in request.task_types]
    
    try:
        # Load documents
        documents = await bot.load_documents(request.document_paths)
        
        # Process documents
        dataset = await bot.process_documents(
            documents=documents,
            task_types=task_types,
            quality_filter=request.quality_filter
        )
        
        # Evaluate dataset
        report = await bot.evaluate_dataset(dataset)
        
        return {
            "status": "success",
            "dataset_id": str(dataset.id),
            "documents_processed": len(documents),
            "examples_generated": len(dataset.examples),
            "quality_score": report.overall_score,
            "quality_passed": report.passed,
            "task_types": request.task_types
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/process-uploaded")
async def process_uploaded_documents(request: ProcessUploadedRequest):
    """
    Process all uploaded documents.
    
    Args:
        task_types: Types of tasks to generate
        quality_filter: Whether to filter by quality threshold
    """
    bot = get_bot()
    
    if not bot.documents:
        raise HTTPException(
            status_code=400,
            detail="No documents uploaded. Upload documents first."
        )
    
    # Validate task types
    valid_tasks = ["qa_generation", "summarization", "classification"]
    for task in request.task_types:
        if task not in valid_tasks:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid task type: {task}. Valid: {valid_tasks}"
            )
    
    # Convert to TaskType enums
    task_types = [TaskType(task) for task in request.task_types]
    
    try:
        # Process all uploaded documents
        documents = list(bot.documents.values())
        
        dataset = await bot.process_documents(
            documents=documents,
            task_types=task_types,
            quality_filter=request.quality_filter
        )
        
        # Evaluate dataset
        report = await bot.evaluate_dataset(dataset)
        
        return {
            "status": "success",
            "dataset_id": str(dataset.id),
            "documents_processed": len(documents),
            "examples_generated": len(dataset.examples),
            "quality_score": report.overall_score,
            "quality_passed": report.passed,
            "task_types": request.task_types
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status")
async def get_processing_status():
    """Get current processing status and statistics."""
    bot = get_bot()
    stats = bot.get_statistics()
    
    return {
        "status": "ready",
        "documents_loaded": stats["documents"]["total"],
        "datasets_created": stats["datasets"]["total"],
        "total_examples": stats["datasets"]["total_examples"],
        "statistics": stats
    }


================================================================================
FILE: api\server.py
================================================================================

"""
FastAPI server for Training Data Bot.

Provides REST API endpoints for document processing and dataset management.
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from pathlib import Path
import sys

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from training_data_bot import TrainingDataBot
from training_data_bot.core import get_logger
from api.dependencies import set_bot
from api.middleware.auth import AuthMiddleware
from api.middleware.rate_limit import limiter

logger = get_logger("api.server")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    logger.info("Starting Training Data Bot API server...")
    bot_instance = TrainingDataBot()
    
    # Configure AI client if API key is available
    import os
    openai_key = os.getenv("TDB_OPENAI_API_KEY")
    if openai_key:
        bot_instance.set_ai_client(
            provider="openai",
            api_key=openai_key,
            model="gpt-3.5-turbo"
        )
        logger.info("AI client configured with OpenAI")
    else:
        logger.warning("No OpenAI API key found - AI generation disabled")
    
    set_bot(bot_instance)
    logger.info("Bot initialized successfully")
    
    yield
    
    # Shutdown
    logger.info("Shutting down API server...")
    await bot_instance.cleanup()
    logger.info("Shutdown complete")

# Create FastAPI app
app = FastAPI(
    title="Training Data Bot API",
    description="Enterprise-grade training data curation bot for LLM fine-tuning",
    version="0.1.0",
    lifespan=lifespan
)


# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure this for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Add authentication middleware
app.add_middleware(AuthMiddleware)

from monitoring.metrics import metrics
import time

@app.middleware("http")
async def track_metrics(request, call_next):
    """Track request metrics."""
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    metrics.record_request(
        endpoint=request.url.path,
        duration=duration,
        status_code=response.status_code
    )
    
    return response


# Add rate limiting
app.state.limiter = limiter


# Import and include routers AFTER app is created
from api.routes import documents, processing, datasets, monitoring


app.include_router(documents.router, prefix="/api/v1/documents", tags=["Documents"])
app.include_router(processing.router, prefix="/api/v1/processing", tags=["Processing"])
app.include_router(datasets.router, prefix="/api/v1/datasets", tags=["Datasets"])
app.include_router(monitoring.router, prefix="/api/v1/monitoring", tags=["Monitoring"])


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "name": "Training Data Bot API",
        "version": "0.1.0",
        "status": "running",
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    from api.dependencies import get_bot
    try:
        bot = get_bot()
        bot_ready = True
    except:
        bot_ready = False
    
    return {
        "status": "healthy",
        "bot_initialized": bot_ready
    }


@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Handle HTTP exceptions."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code
        }
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "api.server:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )


================================================================================
FILE: documentation_architecture.py
================================================================================

"""
Project Architecture Documentation Generator

Creates a complete visual documentation of the project structure with:
- Directory tree
- File descriptions
- Code statistics
- Module relationships

Usage: python document_architecture.py
"""

from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict


def count_lines(file_path: Path) -> Tuple[int, int, int]:
    """
    Count lines in a file.
    
    Returns:
        (total_lines, code_lines, comment_lines)
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        lines = content.split('\n')
        total = len(lines)
        
        code = 0
        comments = 0
        in_multiline_comment = False
        
        for line in lines:
            stripped = line.strip()
            
            # Skip empty lines
            if not stripped:
                continue
            
            # Check for multiline comments
            if '"""' in stripped or "'''" in stripped:
                in_multiline_comment = not in_multiline_comment
                comments += 1
                continue
            
            if in_multiline_comment:
                comments += 1
            elif stripped.startswith('#'):
                comments += 1
            else:
                code += 1
        
        return total, code, comments
    except Exception:
        return 0, 0, 0


def get_file_description(file_path: Path) -> str:
    """Extract description from file docstring."""
    try:
        content = file_path.read_text(encoding='utf-8')
        lines = content.split('\n')
        
        in_docstring = False
        docstring_lines = []
        
        for line in lines:
            stripped = line.strip()
            
            if '"""' in stripped:
                if in_docstring:
                    break
                in_docstring = True
                # Get content after opening quotes
                after_quotes = stripped.split('"""', 1)[1]
                if after_quotes and after_quotes != '"""':
                    docstring_lines.append(after_quotes)
                continue
            
            if in_docstring and stripped:
                docstring_lines.append(stripped)
        
        return ' '.join(docstring_lines[:2]) if docstring_lines else "No description"
    except Exception:
        return "Error reading file"


def build_tree(directory: Path, prefix: str = "", exclude_patterns: List[str] = None) -> List[str]:
    """Build directory tree structure."""
    if exclude_patterns is None:
        exclude_patterns = [
            '__pycache__', '.git', 'venv', 'env', '.venv', 
            'logs', 'temp', '.pytest_cache', 'output',
            '.egg-info', 'build', 'dist'
        ]
    
    tree_lines = []
    
    try:
        items = sorted(directory.iterdir(), key=lambda x: (not x.is_dir(), x.name))
        items = [item for item in items if not any(pattern in str(item) for pattern in exclude_patterns)]
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            tree_lines.append(prefix + current_prefix + item.name)
            
            if item.is_dir():
                extension = "    " if is_last else "â”‚   "
                tree_lines.extend(build_tree(item, prefix + extension, exclude_patterns))
    except PermissionError:
        pass
    
    return tree_lines


def generate_architecture_doc(output_file: Path):
    """Generate complete architecture documentation."""
    project_root = Path(__file__).parent
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # Header
        f.write("=" * 80 + "\n")
        f.write("TRAINING DATA BOT - PROJECT ARCHITECTURE DOCUMENTATION\n")
        f.write("=" * 80 + "\n\n")
        
        # Table of Contents
        f.write("TABLE OF CONTENTS\n")
        f.write("-" * 80 + "\n")
        f.write("1. Project Overview\n")
        f.write("2. Directory Structure\n")
        f.write("3. Module Breakdown\n")
        f.write("4. File Descriptions\n")
        f.write("5. Code Statistics\n")
        f.write("6. Dependencies\n")
        f.write("\n\n")
        
        # 1. Project Overview
        f.write("=" * 80 + "\n")
        f.write("1. PROJECT OVERVIEW\n")
        f.write("=" * 80 + "\n\n")
        f.write("Project Name: Training Data Bot\n")
        f.write("Version: 0.1.0\n")
        f.write("Purpose: Enterprise-grade training data curation bot for LLM fine-tuning\n")
        f.write("\nArchitecture Pattern: Separate Project & Package (Industry Standard)\n")
        f.write("\nKey Features:\n")
        f.write("- Multi-format document loading (PDF, DOCX, TXT, MD, HTML, CSV, JSON, URLs)\n")
        f.write("- Automatic text preprocessing and chunking\n")
        f.write("- AI-powered task generation (Q&A, Classification, Summarization)\n")
        f.write("- Quality evaluation and filtering\n")
        f.write("- Multi-format export (JSONL, JSON, CSV, Parquet)\n")
        f.write("\n\n")
        
        # 2. Directory Structure
        f.write("=" * 80 + "\n")
        f.write("2. DIRECTORY STRUCTURE\n")
        f.write("=" * 80 + "\n\n")
        
        tree_lines = build_tree(project_root)
        f.write("my_training_data_bot/\n")
        for line in tree_lines:
            f.write(line + "\n")
        f.write("\n\n")
        
        # 3. Module Breakdown
        f.write("=" * 80 + "\n")
        f.write("3. MODULE BREAKDOWN\n")
        f.write("=" * 80 + "\n\n")
        
        modules = {
            "training_data_bot/": "Main package - contains all core functionality",
            "training_data_bot/core/": "Foundation - data models, config, logging, exceptions",
            "training_data_bot/sources/": "Document loading - PDF, DOCX, web, unified loader",
            "training_data_bot/preprocessing/": "Text processing - chunking and preparation",
            "training_data_bot/ai/": "AI integration - OpenAI, Anthropic providers",
            "training_data_bot/tasks/": "Task generation - Q&A, classification, summarization",
            "training_data_bot/evaluation/": "Quality assessment - multi-metric evaluation",
            "training_data_bot/storage/": "Data export - JSONL, JSON, CSV, Parquet formats",
            "config/": "Configuration files - production, development, staging",
            "scripts/": "Operational scripts - production runner, batch processing",
            "output/": "Generated training data outputs",
            "documents/": "Input documents for processing",
        }
        
        for module, description in modules.items():
            f.write(f"{module}\n")
            f.write(f"  â†’ {description}\n\n")
        
        f.write("\n")
        
        # 4. File Descriptions
        f.write("=" * 80 + "\n")
        f.write("4. FILE DESCRIPTIONS\n")
        f.write("=" * 80 + "\n\n")
        
        package_dir = project_root / "training_data_bot"
        python_files = sorted(package_dir.rglob("*.py"))
        
        current_dir = None
        for py_file in python_files:
            if '__pycache__' in str(py_file):
                continue
            
            rel_path = py_file.relative_to(project_root)
            file_dir = str(rel_path.parent)
            
            if file_dir != current_dir:
                current_dir = file_dir
                f.write(f"\n{file_dir}/\n")
                f.write("-" * 80 + "\n")
            
            description = get_file_description(py_file)
            f.write(f"\n{py_file.name}\n")
            f.write(f"  {description}\n")
        
        f.write("\n\n")
        
        # 5. Code Statistics
        f.write("=" * 80 + "\n")
        f.write("5. CODE STATISTICS\n")
        f.write("=" * 80 + "\n\n")
        
        stats_by_module = defaultdict(lambda: {"files": 0, "total_lines": 0, "code_lines": 0, "comment_lines": 0})
        
        for py_file in python_files:
            if '__pycache__' in str(py_file):
                continue
            
            rel_path = py_file.relative_to(project_root)
            module = str(rel_path.parent)
            
            total, code, comments = count_lines(py_file)
            
            stats_by_module[module]["files"] += 1
            stats_by_module[module]["total_lines"] += total
            stats_by_module[module]["code_lines"] += code
            stats_by_module[module]["comment_lines"] += comments
        
        # Print statistics by module
        f.write(f"{'Module':<50} {'Files':<8} {'Total':<8} {'Code':<8} {'Comments':<8}\n")
        f.write("-" * 80 + "\n")
        
        grand_total = {"files": 0, "total_lines": 0, "code_lines": 0, "comment_lines": 0}
        
        for module in sorted(stats_by_module.keys()):
            stats = stats_by_module[module]
            f.write(f"{module:<50} {stats['files']:<8} {stats['total_lines']:<8} "
                   f"{stats['code_lines']:<8} {stats['comment_lines']:<8}\n")
            
            for key in grand_total:
                grand_total[key] += stats[key]
        
        f.write("-" * 80 + "\n")
        f.write(f"{'TOTAL':<50} {grand_total['files']:<8} {grand_total['total_lines']:<8} "
               f"{grand_total['code_lines']:<8} {grand_total['comment_lines']:<8}\n")
        
        f.write("\n\n")
        
        # 6. Dependencies
        f.write("=" * 80 + "\n")
        f.write("6. DEPENDENCIES\n")
        f.write("=" * 80 + "\n\n")
        
        requirements_file = project_root / "requirements.txt"
        if requirements_file.exists():
            f.write("Core Dependencies:\n")
            f.write("-" * 80 + "\n")
            requirements = requirements_file.read_text(encoding='utf-8')
            for line in requirements.split('\n'):
                if line.strip() and not line.startswith('#'):
                    f.write(f"  - {line.strip()}\n")
        
        f.write("\n\n")
        
        # Footer
        f.write("=" * 80 + "\n")
        f.write("END OF ARCHITECTURE DOCUMENTATION\n")
        f.write("=" * 80 + "\n")


def main():
    """Main execution."""
    project_root = Path(__file__).parent
    output_file = project_root / "project_architecture.txt"
    
    print("Generating project architecture documentation...")
    print(f"Output: {output_file}")
    
    generate_architecture_doc(output_file)
    
    file_size = output_file.stat().st_size / 1024
    
    print("\n" + "=" * 60)
    print("SUCCESS!")
    print("=" * 60)
    print(f"Architecture documentation created: {output_file}")
    print(f"File size: {file_size:.2f} KB")
    print("=" * 60)


if __name__ == "__main__":
    main()


================================================================================
FILE: find_bad_imports.py
================================================================================

"""Find files with incorrect imports."""

from pathlib import Path
import re

def check_file(filepath):
    """Check a file for bad import patterns."""
    try:
        content = filepath.read_text(encoding='utf-8')
        lines = content.split('\n')
        bad_imports = []
        
        for i, line in enumerate(lines, 1):
            # Check for "from ai import" (should be "from training_data_bot.ai import")
            if re.match(r'^\s*from\s+ai\s+import', line):
                bad_imports.append((i, line.strip(), "Should be: from training_data_bot.ai import"))
            
            # Check for "from core import" (should be "from training_data_bot.core import")
            if re.match(r'^\s*from\s+core\s+import', line):
                bad_imports.append((i, line.strip(), "Should be: from training_data_bot.core import"))
            
            # Check for other standalone module imports
            for module in ['tasks', 'sources', 'preprocessing', 'evaluation', 'storage']:
                pattern = rf'^\s*from\s+{module}\s+import'
                if re.match(pattern, line):
                    bad_imports.append((i, line.strip(), f"Should be: from training_data_bot.{module} import"))
        
        return bad_imports
    except Exception as e:
        return []

def main():
    """Check all Python files."""
    # Get the current script's directory
    current_dir = Path(__file__).parent
    package_dir = current_dir / "training_data_bot"
    
    if not package_dir.exists():
        print("âŒ training_data_bot directory not found!")
        return
    
    print("ðŸ” Scanning for bad imports...\n")
    
    issues_found = False
    
    for py_file in sorted(package_dir.rglob("*.py")):
        # Skip test files
        if "test_" in py_file.name:
            continue
        
        bad_imports = check_file(py_file)
        
        if bad_imports:
            issues_found = True
            # Use package_dir as the base for relative path
            rel_path = py_file.relative_to(package_dir)
            print(f"âŒ {rel_path}")
            for line_num, line, suggestion in bad_imports:
                print(f"   Line {line_num}: {line}")
                print(f"            {suggestion}")
            print()

            
if __name__ == "__main__":
    main()


================================================================================
FILE: monitoring\__init__.py
================================================================================

"""Monitoring module for Training Data Bot."""

from monitoring.health import HealthCheck
from monitoring.metrics import MetricsCollector, metrics

__all__ = ["HealthCheck", "MetricsCollector", "metrics"]


================================================================================
FILE: monitoring\health.py
================================================================================

"""
Health check system for monitoring application status.
"""

from typing import Dict, List, Optional
from datetime import datetime
from pathlib import Path
import psutil
import os


class HealthCheck:
    """Comprehensive health check system."""
    
    @staticmethod
    def check_disk_space(path: str = ".", threshold_gb: float = 1.0) -> Dict:
        """Check available disk space."""
        try:
            stat = psutil.disk_usage(path)
            available_gb = stat.free / (1024 ** 3)
            
            return {
                "status": "healthy" if available_gb > threshold_gb else "warning",
                "available_gb": round(available_gb, 2),
                "total_gb": round(stat.total / (1024 ** 3), 2),
                "percent_used": stat.percent
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    @staticmethod
    def check_memory(threshold_percent: float = 90.0) -> Dict:
        """Check memory usage."""
        try:
            mem = psutil.virtual_memory()
            
            return {
                "status": "healthy" if mem.percent < threshold_percent else "warning",
                "percent_used": mem.percent,
                "available_gb": round(mem.available / (1024 ** 3), 2),
                "total_gb": round(mem.total / (1024 ** 3), 2)
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    @staticmethod
    def check_cpu(threshold_percent: float = 90.0) -> Dict:
        """Check CPU usage."""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            
            return {
                "status": "healthy" if cpu_percent < threshold_percent else "warning",
                "percent_used": cpu_percent,
                "cpu_count": psutil.cpu_count()
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    @staticmethod
    def check_directories(directories: List[str]) -> Dict:
        """Check if required directories exist and are writable."""
        results = {}
        
        for directory in directories:
            path = Path(directory)
            exists = path.exists()
            writable = os.access(path, os.W_OK) if exists else False
            
            results[directory] = {
                "exists": exists,
                "writable": writable,
                "status": "healthy" if (exists and writable) else "error"
            }
        
        return results
    
    @staticmethod
    def check_environment_variables(required_vars: List[str]) -> Dict:
        """Check if required environment variables are set."""
        results = {}
        
        for var in required_vars:
            value = os.getenv(var)
            results[var] = {
                "set": value is not None,
                "status": "healthy" if value else "error"
            }
        
        return results
    
    @staticmethod
    def get_system_info() -> Dict:
        """Get general system information."""
        return {
            "python_version": os.sys.version,
            "platform": os.sys.platform,
            "hostname": os.uname().nodename if hasattr(os, 'uname') else "unknown",
            "uptime_seconds": round(psutil.boot_time()),
            "timestamp": datetime.utcnow().isoformat()
        }
    
    @staticmethod
    def comprehensive_check() -> Dict:
        """Run all health checks."""
        return {
            "status": "healthy",  # Will be updated based on checks
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {
                "disk": HealthCheck.check_disk_space(),
                "memory": HealthCheck.check_memory(),
                "cpu": HealthCheck.check_cpu(),
                "directories": HealthCheck.check_directories([
                    "output", "logs", "temp"
                ]),
                "environment": HealthCheck.check_environment_variables([
                    "TDB_OPENAI_API_KEY", "API_KEY"
                ])
            },
            "system": HealthCheck.get_system_info()
        }


================================================================================
FILE: monitoring\metrics.py
================================================================================

"""
Metrics collection for monitoring application performance.
"""

from typing import Dict, Optional
from datetime import datetime, timedelta
from collections import defaultdict
import time


class MetricsCollector:
    """Collect and track application metrics."""
    
    def __init__(self):
        self.request_count = 0
        self.request_durations = []
        self.error_count = 0
        self.endpoint_stats = defaultdict(lambda: {
            "count": 0,
            "total_duration": 0,
            "errors": 0
        })
        self.start_time = datetime.utcnow()
    
    def record_request(
        self,
        endpoint: str,
        duration: float,
        status_code: int
    ):
        """Record a request."""
        self.request_count += 1
        self.request_durations.append(duration)
        
        stats = self.endpoint_stats[endpoint]
        stats["count"] += 1
        stats["total_duration"] += duration
        
        if status_code >= 400:
            self.error_count += 1
            stats["errors"] += 1
    
    def get_metrics(self) -> Dict:
        """Get current metrics."""
        uptime = (datetime.utcnow() - self.start_time).total_seconds()
        
        # Calculate averages
        avg_duration = (
            sum(self.request_durations) / len(self.request_durations)
            if self.request_durations else 0
        )
        
        # Endpoint statistics
        endpoint_metrics = {}
        for endpoint, stats in self.endpoint_stats.items():
            endpoint_metrics[endpoint] = {
                "requests": stats["count"],
                "avg_duration": (
                    stats["total_duration"] / stats["count"]
                    if stats["count"] > 0 else 0
                ),
                "errors": stats["errors"],
                "error_rate": (
                    stats["errors"] / stats["count"] * 100
                    if stats["count"] > 0 else 0
                )
            }
        
        return {
            "uptime_seconds": round(uptime, 2),
            "total_requests": self.request_count,
            "total_errors": self.error_count,
            "error_rate_percent": (
                self.error_count / self.request_count * 100
                if self.request_count > 0 else 0
            ),
            "avg_response_time_ms": round(avg_duration * 1000, 2),
            "requests_per_minute": round(
                self.request_count / (uptime / 60) if uptime > 0 else 0,
                2
            ),
            "endpoints": endpoint_metrics
        }
    
    def reset(self):
        """Reset all metrics."""
        self.__init__()


# Global metrics collector
metrics = MetricsCollector()


================================================================================
FILE: project_complete_code.py
================================================================================

"""
Code Aggregator Script

Collects all Python code from the training_data_bot project and writes it
to a single text file with clear section headers.

Usage: python collect_code.py
"""

from pathlib import Path
from typing import List, Tuple


def get_all_python_files(base_path: Path) -> List[Path]:
    """
    Recursively find all Python files in the project.
    
    Args:
        base_path: Root directory to search
        
    Returns:
        Sorted list of Python file paths
    """
    # Patterns to exclude
    exclude_patterns = [
        '__pycache__',
        '.git',
        'venv',
        'env',
        '.venv',
        'logs',
        'output',
        'temp',
        'test_documents',
        '.pytest_cache',
    ]
    
    python_files = []
    
    for py_file in base_path.rglob('*.py'):
        # Skip if in excluded directory
        if any(pattern in str(py_file) for pattern in exclude_patterns):
            continue
        python_files.append(py_file)
    
    return sorted(python_files)


def read_file_content(file_path: Path) -> str:
    """
    Read content from a file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        File content as string
    """
    try:
        return file_path.read_text(encoding='utf-8')
    except Exception as e:
        return f"# Error reading file: {e}\n"


def write_aggregated_code(output_file: Path, files_and_content: List[Tuple[str, str]]):
    """
    Write all code to a single output file.
    
    Args:
        output_file: Path to output file
        files_and_content: List of (relative_path, content) tuples
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # Write header
        f.write("=" * 80 + "\n")
        f.write("TRAINING DATA BOT - COMPLETE PROJECT CODE\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Total files: {len(files_and_content)}\n")
        f.write(f"Generated: {Path.cwd()}\n")
        f.write("=" * 80 + "\n\n\n")
        
        # Write each file
        for relative_path, content in files_and_content:
            # Section header
            f.write("=" * 80 + "\n")
            f.write(f"FILE: {relative_path}\n")
            f.write("=" * 80 + "\n\n")
            
            # File content
            f.write(content)
            
            # Add spacing between files
            f.write("\n\n\n")
        
        # Write footer
        f.write("=" * 80 + "\n")
        f.write("END OF PROJECT CODE\n")
        f.write("=" * 80 + "\n")


def main():
    """Main execution function."""
    # Get project root (assumes script is in project root)
    project_root = Path(__file__).parent
    
    # Output file path
    output_file = project_root / "project_complete_code.txt"
    
    print("Collecting Python files...")
    
    # Get all Python files
    python_files = get_all_python_files(project_root)
    
    print(f"Found {len(python_files)} Python files")
    
    # Read all files
    files_and_content = []
    for py_file in python_files:
        # Get relative path for cleaner output
        try:
            relative_path = py_file.relative_to(project_root)
        except ValueError:
            relative_path = py_file
        
        # Read content
        content = read_file_content(py_file)
        
        # Store as tuple
        files_and_content.append((str(relative_path), content))
        
        print(f"  âœ“ {relative_path}")
    
    # Write aggregated file
    print(f"\nWriting to {output_file}...")
    write_aggregated_code(output_file, files_and_content)
    
    # Print summary
    total_lines = sum(content.count('\n') for _, content in files_and_content)
    total_chars = sum(len(content) for _, content in files_and_content)
    
    print("\n" + "=" * 60)
    print("SUCCESS!")
    print("=" * 60)
    print(f"Output file: {output_file}")
    print(f"Total files: {len(files_and_content)}")
    print(f"Total lines: {total_lines:,}")
    print(f"Total characters: {total_chars:,}")
    print(f"File size: {output_file.stat().st_size / 1024:.2f} KB")
    print("=" * 60)


if __name__ == "__main__":
    main()


================================================================================
FILE: scripts\diagnostics.py
================================================================================

"""
Diagnostic tool for troubleshooting the Training Data Bot.
"""

import asyncio
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from monitoring.health import HealthCheck
from training_data_bot import TrainingDataBot
import os


async def run_diagnostics():
    """Run comprehensive diagnostics."""
    print("="*60)
    print("TRAINING DATA BOT - DIAGNOSTICS")
    print("="*60)
    
    # System Health
    print("\n1. SYSTEM HEALTH")
    print("-"*60)
    health = HealthCheck.comprehensive_check()
    
    print(f"Disk Space: {health['checks']['disk']['status']}")
    print(f"  Available: {health['checks']['disk']['available_gb']} GB")
    
    print(f"Memory: {health['checks']['memory']['status']}")
    print(f"  Used: {health['checks']['memory']['percent_used']}%")
    
    print(f"CPU: {health['checks']['cpu']['status']}")
    print(f"  Used: {health['checks']['cpu']['percent_used']}%")
    
    # Environment Variables
    print("\n2. ENVIRONMENT VARIABLES")
    print("-"*60)
    for var, info in health['checks']['environment'].items():
        status = "âœ“" if info['set'] else "âœ—"
        print(f"{status} {var}: {'Set' if info['set'] else 'Not Set'}")
    
    # Directories
    print("\n3. DIRECTORIES")
    print("-"*60)
    for dir_name, info in health['checks']['directories'].items():
        status = "âœ“" if info['status'] == 'healthy' else "âœ—"
        print(f"{status} {dir_name}: Exists={info['exists']}, Writable={info['writable']}")
    
    # Bot Initialization
    print("\n4. BOT INITIALIZATION")
    print("-"*60)
    try:
        bot = TrainingDataBot()
        print("âœ“ Bot initialized successfully")
        
        # Test AI client
        openai_key = os.getenv("TDB_OPENAI_API_KEY")
        if openai_key:
            bot.set_ai_client(provider="openai", api_key=openai_key)
            print("âœ“ AI client configured")
        else:
            print("âœ— No OpenAI API key found")
        
        # Get statistics
        stats = bot.get_statistics()
        print(f"  Documents loaded: {stats['documents']['total']}")
        print(f"  Datasets created: {stats['datasets']['total']}")
        
        await bot.cleanup()
        
    except Exception as e:
        print(f"âœ— Bot initialization failed: {e}")
    
    print("\n" + "="*60)
    print("DIAGNOSTICS COMPLETE")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(run_diagnostics())


================================================================================
FILE: scripts\production_bot.py
================================================================================

"""
Production-ready script to run the Training Data Bot.

Usage:
    python scripts/production_bot.py --config config/production.yaml
"""

import asyncio
import argparse
from pathlib import Path
import sys
import yaml
from dotenv import load_dotenv
import os

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from training_data_bot import TrainingDataBot
from training_data_bot.core import TaskType, ExportFormat


def load_config(config_path: Path) -> dict:
    """Load configuration from YAML file."""
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


async def main():
    """Main production workflow."""
    # Parse arguments
    parser = argparse.ArgumentParser(description="Training Data Bot - Production Runner")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/production.yaml"),
        help="Path to configuration file"
    )
    parser.add_argument(
        "--input",
        type=Path,
        required=True,
        help="Input directory or file path"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("output/training_data.jsonl"),
        help="Output file path"
    )
    parser.add_argument(
        "--tasks",
        nargs="+",
        default=["qa_generation", "summarization"],
        help="Task types to generate"
    )
    
    args = parser.parse_args()
    
    # Load environment variables
    load_dotenv()
    
    # Load configuration
    print(f"Loading configuration from: {args.config}")
    config = load_config(args.config)
    
    # Verify API key
    api_key = os.getenv("TDB_OPENAI_API_KEY")
    if not api_key:
        print("ERROR: TDB_OPENAI_API_KEY not found in environment")
        sys.exit(1)
    
    print(f"Input: {args.input}")
    print(f"Output: {args.output}")
    print(f"Tasks: {', '.join(args.tasks)}")
    
    try:
        # Initialize bot
        print("\nInitializing Training Data Bot...")
        bot = TrainingDataBot(config=config)
        
        # Configure AI client
        bot.set_ai_client(
            provider=config['ai']['default_provider'],
            api_key=api_key,
            model=config['ai']['openai']['model']
        )
        
        # Load documents
        print(f"\nLoading documents from: {args.input}")
        if args.input.is_dir():
            documents = await bot.load_documents(str(args.input))
        else:
            documents = await bot.load_documents([str(args.input)])
        
        print(f"Loaded {len(documents)} document(s)")
        
        # Process documents
        print("\nProcessing documents (this may take a while)...")
        task_types = [TaskType(task) for task in args.tasks]
        
        dataset = await bot.process_documents(
            documents=documents,
            task_types=task_types,
            quality_filter=True
        )
        
        print(f"Generated {len(dataset.examples)} training examples")
        
        # Evaluate quality
        print("\nEvaluating dataset quality...")
        report = await bot.evaluate_dataset(dataset)
        print(f"Quality Score: {report.overall_score:.2f}")
        print(f"Passed Quality Check: {report.passed}")
        
        # Export dataset
        print(f"\nExporting to: {args.output}")
        await bot.export_dataset(
            dataset=dataset,
            output_path=args.output,
            format=ExportFormat.JSONL,
            split_data=True
        )
        
        # Statistics
        print("\n" + "="*60)
        print("PROCESSING COMPLETE")
        print("="*60)
        stats = bot.get_statistics()
        print(f"Documents Processed: {stats['documents']['total']}")
        print(f"Training Examples: {stats['datasets']['total_examples']}")
        print(f"Output: {args.output}")
        
        # Cleanup
        await bot.cleanup()
        
        print("\nSuccess!")
        
    except Exception as e:
        print(f"\nERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: setup.py
================================================================================

"""
Setup script for Training Data Bot package.
"""

from setuptools import setup, find_packages
from pathlib import Path

# Read README for long description
readme_file = Path(__file__).parent / "README.md"
long_description = readme_file.read_text(encoding="utf-8") if readme_file.exists() else ""

setup(
    name="training-data-bot",
    version="0.1.0",
    description="Enterprise-grade training data curation bot for LLM fine-tuning",
    long_description=long_description,
    long_description_content_type="text/markdown",
    author="Training Data Bot Team",
    author_email="team@company.com",
    url="https://github.com/yourcompany/training-data-bot",
    
    # Package discovery
    packages=find_packages(exclude=["tests*", "examples*", "scripts*"]),
    
    # Python version requirement
    python_requires=">=3.9",
    
    # Core dependencies
    install_requires=[
        "pydantic>=2.0.0",
        "httpx>=0.24.0",
        "beautifulsoup4>=4.12.0",
        "PyMuPDF>=1.22.0",
        "python-docx>=0.8.11",
        "openai>=1.0.0",
        "anthropic>=0.25.0",
        "pandas>=2.0.0",
        "pyarrow>=12.0.0",
        "pyyaml>=6.0",
        "python-dotenv>=1.0.0",
    ],
    
    # Optional dependencies
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "pytest-asyncio>=0.21.0",
            "black>=23.0.0",
            "flake8>=6.0.0",
        ],
        "api": [
            "fastapi>=0.100.0",
            "uvicorn>=0.23.0",
        ],
    },
    
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
)


================================================================================
FILE: streamlit_app.py
================================================================================

"""
Streamlit UI for Training Data Bot.

A visual interface for document upload, processing, and dataset management.
"""

import streamlit as st
import sys
from pathlib import Path
import asyncio

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from training_data_bot import TrainingDataBot
from training_data_bot.core import TaskType, ExportFormat
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Page config
st.set_page_config(
    page_title="Training Data Bot",
    page_icon="ðŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)


# Initialize session state
if 'bot' not in st.session_state:
    st.session_state.bot = None
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'dataset' not in st.session_state:
    st.session_state.dataset = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False


def initialize_bot():
    """Initialize the Training Data Bot."""
    if st.session_state.bot is None:
        with st.spinner("Initializing bot..."):
            bot = TrainingDataBot()
            
            # Configure AI client
            openai_key = os.getenv("TDB_OPENAI_API_KEY")
            if openai_key:
                bot.set_ai_client(
                    provider="openai",
                    api_key=openai_key,
                    model="gpt-3.5-turbo"
                )
            
            st.session_state.bot = bot
    
    return st.session_state.bot


def main():
    """Main application."""
    
    # Header
    st.markdown('<div class="main-header">ðŸ¤– Training Data Bot</div>', unsafe_allow_html=True)
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.title("Navigation")
        page = st.radio(
            "Go to",
            ["ðŸ“¤ Upload Documents", "âš™ï¸ Process Data", "ðŸ“Š View Results", "â„¹ï¸ About"],
            label_visibility="collapsed"
        )
        
        st.markdown("---")
        
        # Status
        st.subheader("Status")
        bot = initialize_bot()
        
        if bot:
            stats = bot.get_statistics()
            st.metric("Documents Loaded", stats['documents']['total'])
            st.metric("Datasets Created", stats['datasets']['total'])
            st.metric("Total Examples", stats['datasets']['total_examples'])
        
        st.markdown("---")
        st.caption("Training Data Bot v0.1.0")
    
    # Main content
    if page == "ðŸ“¤ Upload Documents":
        show_upload_page()
    elif page == "âš™ï¸ Process Data":
        show_process_page()
    elif page == "ðŸ“Š View Results":
        show_results_page()
    elif page == "â„¹ï¸ About":
        show_about_page()


def show_upload_page():
    """Document upload page."""
    st.header("ðŸ“¤ Upload Documents")
    st.write("Upload documents to process into training data.")
    
    bot = initialize_bot()
    
    # File uploader
    uploaded_files = st.file_uploader(
        "Choose files",
        type=['txt', 'pdf', 'docx', 'md', 'html', 'csv', 'json'],
        accept_multiple_files=True,
        help="Upload one or more documents"
    )
    
    if uploaded_files:
        st.write(f"**{len(uploaded_files)} file(s) selected**")
        
        # Show file details
        for file in uploaded_files:
            st.write(f"- {file.name} ({file.size / 1024:.2f} KB)")
        
        if st.button("Load Documents", type="primary"):
            with st.spinner("Loading documents..."):
                try:
                    # Save uploaded files temporarily
                    temp_dir = Path("temp/uploads")
                    temp_dir.mkdir(parents=True, exist_ok=True)
                    
                    file_paths = []
                    for file in uploaded_files:
                        file_path = temp_dir / file.name
                        with open(file_path, "wb") as f:
                            f.write(file.getbuffer())
                        file_paths.append(str(file_path))
                    
                    # Load documents
                    documents = asyncio.run(bot.load_documents(file_paths))
                    st.session_state.documents = documents
                    
                    st.success(f"âœ… Successfully loaded {len(documents)} document(s)!")
                    
                    # Show document details
                    for doc in documents:
                        with st.expander(f"ðŸ“„ {doc.title}"):
                            col1, col2, col3 = st.columns(3)
                            col1.metric("Type", doc.doc_type.value)
                            col2.metric("Words", doc.word_count)
                            col3.metric("Characters", doc.char_count)
                            
                            st.text_area(
                                "Content Preview",
                                doc.content[:500] + "..." if len(doc.content) > 500 else doc.content,
                                height=150
                            )
                
                except Exception as e:
                    st.error(f"âŒ Error loading documents: {e}")
    
    # Show loaded documents
    if st.session_state.documents:
        st.markdown("---")
        st.subheader("Loaded Documents")
        st.write(f"**{len(st.session_state.documents)} document(s) ready for processing**")
        
        for i, doc in enumerate(st.session_state.documents, 1):
            st.write(f"{i}. {doc.title} ({doc.word_count} words)")


def show_process_page():
    """Processing configuration page."""
    st.header("âš™ï¸ Process Data")
    
    if not st.session_state.documents:
        st.warning("âš ï¸ No documents loaded. Please upload documents first.")
        return
    
    bot = initialize_bot()
    
    st.write(f"**Processing {len(st.session_state.documents)} document(s)**")
    
    # Configuration
    st.subheader("Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        task_types = st.multiselect(
            "Task Types",
            ["qa_generation", "summarization", "classification"],
            default=["qa_generation", "summarization"],
            help="Select which types of tasks to generate"
        )
    
    with col2:
        quality_filter = st.checkbox(
            "Enable Quality Filtering",
            value=True,
            help="Filter out low-quality examples"
        )
        
        quality_threshold = st.slider(
            "Quality Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.75,
            step=0.05,
            help="Minimum quality score (0-1)"
        )
    
    # Advanced settings
    with st.expander("Advanced Settings"):
        chunk_size = st.number_input("Chunk Size", value=1000, min_value=100, max_value=5000)
        chunk_overlap = st.number_input("Chunk Overlap", value=200, min_value=0, max_value=500)
    
    # Process button
    if st.button("ðŸš€ Start Processing", type="primary"):
        if not task_types:
            st.error("Please select at least one task type")
            return
        
        # Convert task types
        selected_tasks = [TaskType(t) for t in task_types]
        
        # Processing
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Process documents
            status_text.text("Processing documents...")
            progress_bar.progress(25)
            
            dataset = asyncio.run(
                bot.process_documents(
                    documents=st.session_state.documents,
                    task_types=selected_tasks,
                    quality_filter=quality_filter
                )
            )
            
            progress_bar.progress(50)
            
            # Step 2: Evaluate
            status_text.text("Evaluating quality...")
            report = asyncio.run(bot.evaluate_dataset(dataset))
            
            progress_bar.progress(75)
            
            # Step 3: Complete
            st.session_state.dataset = dataset
            st.session_state.processing_complete = True
            
            progress_bar.progress(100)
            status_text.text("âœ… Processing complete!")
            
            # Show results
            st.markdown('<div class="success-box">', unsafe_allow_html=True)
            st.write("### Processing Complete!")
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Examples Generated", len(dataset.examples))
            col2.metric("Quality Score", f"{report.overall_score:.2f}")
            col3.metric("Passed Quality", "âœ…" if report.passed else "âŒ")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.info("ðŸ’¡ Go to 'View Results' to see and download your dataset")
        
        except Exception as e:
            st.error(f"âŒ Processing failed: {e}")
            import traceback
            st.code(traceback.format_exc())


def show_results_page():
    """Results viewer page."""
    st.header("ðŸ“Š View Results")
    
    if not st.session_state.processing_complete or not st.session_state.dataset:
        st.warning("âš ï¸ No results available. Please process documents first.")
        return
    
    dataset = st.session_state.dataset
    bot = initialize_bot()
    
    # Dataset summary
    st.subheader("Dataset Summary")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total Examples", len(dataset.examples))
    col2.metric("Dataset ID", dataset.id.hex[:8] + "...")
    col3.metric("Created", dataset.created_at.strftime("%Y-%m-%d %H:%M"))
    col4.metric("Name", dataset.name)
    
    st.markdown("---")
    
    # Task distribution
    st.subheader("Task Distribution")
    task_dist = {}
    for example in dataset.examples:
        task_type = example.task_type.value
        task_dist[task_type] = task_dist.get(task_type, 0) + 1
    
    col1, col2, col3 = st.columns(3)
    for i, (task, count) in enumerate(task_dist.items()):
        if i == 0:
            col1.metric(task.replace("_", " ").title(), count)
        elif i == 1:
            col2.metric(task.replace("_", " ").title(), count)
        else:
            col3.metric(task.replace("_", " ").title(), count)
    
    st.markdown("---")
    
    # Example viewer
    st.subheader("Examples")

    # Check if there are examples
    if len(dataset.examples) == 0:
        st.warning("âš ï¸ No examples were generated. This might be because:")
        st.write("- The document was too short")
        st.write("- Quality filtering removed all examples")
        st.write("- The AI client encountered an error")
        return
    
    # Filters
    col1, col2 = st.columns([1, 3])
    with col1:
        filter_task = st.selectbox(
            "Filter by task",
            ["All"] + list(task_dist.keys())
        )

    with col2:
        # Only show slider if there are multiple examples
        if len(dataset.examples) > 1:
            max_examples = min(20, len(dataset.examples))
            default_examples = min(5, len(dataset.examples))
            
            num_examples = st.slider(
                "Number of examples to show",
                min_value=1,
                max_value=max_examples,
                value=default_examples
            )
        else:
            num_examples = 1
            st.info("Showing the only example available")


    
    # Filter examples
    filtered_examples = dataset.examples
    if filter_task != "All":
        filtered_examples = [
            ex for ex in dataset.examples
            if ex.task_type.value == filter_task
        ]
    
    # Display examples
    for i, example in enumerate(filtered_examples[:num_examples], 1):
        with st.expander(f"Example {i} - {example.task_type.value}"):
            st.write("**Input:**")
            st.text_area("", example.input_text, height=100, key=f"input_{i}", label_visibility="collapsed")
            
            st.write("**Output:**")
            st.text_area("", example.output_text, height=150, key=f"output_{i}", label_visibility="collapsed")
            
            if example.quality_scores:
                st.write("**Quality Scores:**")
                score_cols = st.columns(len(example.quality_scores))
                for j, (metric, score) in enumerate(example.quality_scores.items()):
                    score_cols[j].metric(metric, f"{score:.2f}")
    
    st.markdown("---")
    
    # Export section
    st.subheader("Export Dataset")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_format = st.selectbox(
            "Format",
            ["jsonl", "json", "csv"],
            help="Select export format"
        )
    
    with col2:
        split_data = st.checkbox(
            "Split into train/val/test",
            value=True,
            help="Split dataset into training, validation, and test sets"
        )
    
    if st.button("ðŸ“¥ Export Dataset", type="primary"):
        try:
            with st.spinner("Exporting..."):
                # Ensure output directory exists
                output_dir = Path("output")
                output_dir.mkdir(parents=True, exist_ok=True)

                # Create proper file path
                output_path = output_dir / f"{dataset.name}.{export_format}"
                
                result_path = asyncio.run(
                    bot.export_dataset(
                        dataset=dataset,
                        output_path=output_path,
                        format=ExportFormat(export_format),
                        split_data=split_data
                    )
                )
                                
                # Provide download button
                st.success(f"âœ… Dataset exported successfully!")

                # Find and provide download buttons for the actual files
                if split_data:
                    # Look for train/val/test files
                    base_name = dataset.name
                    train_file = output_dir / f"{base_name}_train.{export_format}"
                    val_file = output_dir / f"{base_name}_val.{export_format}"
                    test_file = output_dir / f"{base_name}_test.{export_format}"
                    
                    col1, col2, col3 = st.columns(3)
                    
                    if train_file.exists():
                        with open(train_file, "rb") as f:
                            col1.download_button(
                                "ðŸ“¥ Training Set",
                                f.read(),
                                file_name=train_file.name,
                                mime="application/octet-stream"
                            )
                    
                    if val_file.exists():
                        with open(val_file, "rb") as f:
                            col2.download_button(
                                "ðŸ“¥ Validation Set",
                                f.read(),
                                file_name=val_file.name,
                                mime="application/octet-stream"
                            )
                    
                    if test_file.exists():
                        with open(test_file, "rb") as f:
                            col3.download_button(
                                "ðŸ“¥ Test Set",
                                f.read(),
                                file_name=test_file.name,
                                mime="application/octet-stream"
                            )
                else:
                    # Single file
                    if output_path.exists():
                        with open(output_path, "rb") as f:
                            st.download_button(
                                "ðŸ“¥ Download Dataset",
                                f.read(),
                                file_name=output_path.name,
                                mime="application/octet-stream"
                            )
        
        except Exception as e:
            st.error(f"âŒ Export failed: {e}")


def show_about_page():
    """About page."""
    st.header("â„¹ï¸ About Training Data Bot")
    
    st.write("""
    **Training Data Bot** is an enterprise-grade system for curating high-quality training data for LLM fine-tuning.
    
    ### Features
    - ðŸ“„ Multi-format document loading (PDF, DOCX, TXT, MD, HTML, CSV, JSON)
    - ðŸ¤– AI-powered task generation (Q&A, Classification, Summarization)
    - âœ… Quality evaluation and filtering
    - ðŸ“Š Multiple export formats (JSONL, JSON, CSV)
    - ðŸ”„ Batch processing capabilities
    
    ### Technology Stack
    - **Backend:** FastAPI, Python 3.13
    - **AI:** OpenAI GPT-3.5/4, Anthropic Claude
    - **UI:** Streamlit
    - **Deployment:** Docker, Docker Compose
    
    ### Version
    0.1.0
    
    ### Documentation
    For more information, visit the project repository or check the API documentation at `/docs`.
    """)


if __name__ == "__main__":
    main()


================================================================================
FILE: test_api.py
================================================================================

import requests

API_URL = "http://localhost:8000"
API_KEY = "dev-key-12345"
headers = {"X-API-Key": API_KEY}

print("=" * 60)
print("API Testing")
print("=" * 60)

# Test 1: Health check
print("\n1. Health Check")
response = requests.get(f"{API_URL}/health")
print(f"   Status: {response.status_code}")
print(f"   Result: {response.json()}")

# Test 2: Upload document
print("\n2. Upload Document")
with open("documents/test.txt", "rb") as f:
    files = {"files": ("test.txt", f, "text/plain")}
    response = requests.post(
        f"{API_URL}/api/v1/documents/upload",
        headers=headers,
        files=files
    )
    print(f"   Status: {response.status_code}")
    if response.status_code == 200:
        result = response.json()
        print(f"   Uploaded: {result['files_uploaded']} file(s)")
    else:
        print(f"   Error: {response.text}")

# Test 3: Process uploaded documents
print("\n3. Process Documents")
data = {
    "task_types": ["qa_generation", "summarization"],
    "quality_filter": True
}
response = requests.post(
    f"{API_URL}/api/v1/processing/process-uploaded",
    headers=headers,
    json=data
)
print(f"   Status: {response.status_code}")
if response.status_code == 200:
    result = response.json()
    print(f"   Dataset ID: {result['dataset_id']}")
    print(f"   Examples: {result['examples_generated']}")
    print(f"   Quality: {result['quality_score']:.2f}")
    dataset_id = result['dataset_id']
else:
    print(f"   Error: {response.text}")
    dataset_id = None

# Test 4: List datasets
print("\n4. List Datasets")
response = requests.get(
    f"{API_URL}/api/v1/datasets/list",
    headers=headers
)
print(f"   Status: {response.status_code}")
if response.status_code == 200:
    result = response.json()
    print(f"   Total datasets: {result['total']}")

# Test 5: Export dataset
if dataset_id:
    print("\n5. Export Dataset")
    data = {"format": "jsonl", "split_data": True}
    response = requests.post(
        f"{API_URL}/api/v1/datasets/{dataset_id}/export",
        headers=headers,
        json=data
    )
    print(f"   Status: {response.status_code}")
    if response.status_code == 200:
        result = response.json()
        print(f"   Exported to: {result['export_path']}")

print("\n" + "=" * 60)
print("Tests Complete!")
print("=" * 60)


================================================================================
FILE: test_imports.py
================================================================================

"""Quick import test to verify all imports work."""

import sys
from pathlib import Path

# Add current directory to Python path (since we're in project root)
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


print("Testing imports...")

try:
    from training_data_bot.core import Document, Dataset, TaskType
    print("âœ… Core imports work")
except ImportError as e:
    print(f"âŒ Core imports failed: {e}")

try:
    from training_data_bot.sources import UnifiedLoader
    print("âœ… Sources imports work")
except ImportError as e:
    print(f"âŒ Sources imports failed: {e}")

try:
    from training_data_bot.preprocessing import TextPreprocessor
    print("âœ… Preprocessing imports work")
except ImportError as e:
    print(f"âŒ Preprocessing imports failed: {e}")

try:
    from training_data_bot.ai import AIClient
    print("âœ… AI imports work")
except ImportError as e:
    print(f"âŒ AI imports failed: {e}")

try:
    from training_data_bot.tasks import TaskManager, QAGenerator
    print("âœ… Tasks imports work")
except ImportError as e:
    print(f"âŒ Tasks imports failed: {e}")

try:
    from training_data_bot.evaluation import QualityEvaluator
    print("âœ… Evaluation imports work")
except ImportError as e:
    print(f"âŒ Evaluation imports failed: {e}")

try:
    from training_data_bot.storage import DatasetExporter
    print("âœ… Storage imports work")
except ImportError as e:
    print(f"âŒ Storage imports failed: {e}")

try:
    from training_data_bot.bot import TrainingDataBot
    print("âœ… Bot import works")
except ImportError as e:
    print(f"âŒ Bot import failed: {e}")

try:
    from training_data_bot import TrainingDataBot
    print("âœ… Package-level import works")
except ImportError as e:
    print(f"âŒ Package-level import failed: {e}")

print("\nâœ… All imports successful!")


================================================================================
FILE: test_phase1.py
================================================================================

"""Quick Phase 1 validation script."""
import asyncio
from pathlib import Path

async def main():
    print("Phase 1 Deployment Tests\n" + "="*50)
    
    tests_passed = 0
    tests_failed = 0
    
    # Test 1: Files exist
    print("\n1. Checking required files...")
    required_files = [
        "setup.py",
        "requirements.txt",
        ".env.example",
        ".gitignore",
        "config/production.yaml",
        "scripts/production_bot.py",
        "README.md",
    ]
    
    for file in required_files:
        if Path(file).exists():
            print(f"  âœ“ {file}")
            tests_passed += 1
        else:
            print(f"  âœ— {file} MISSING")
            tests_failed += 1
    
    # Test 2: Documents directory
    print("\n2. Checking documents directory...")
    if Path("documents").exists():
        doc_count = len(list(Path("documents").glob("*.txt")))
        print(f"  âœ“ documents/ exists ({doc_count} files)")
        tests_passed += 1
    else:
        print("  âœ— documents/ missing")
        tests_failed += 1
    
    # Test 3: .env file
    print("\n3. Checking .env file...")
    if Path(".env").exists():
        print("  âœ“ .env exists")
        tests_passed += 1
    else:
        print("  âœ— .env missing")
        tests_failed += 1
    
    # Test 4: Output files
    print("\n4. Checking output files...")
    output_files = list(Path("output").glob("*.jsonl"))
    if len(output_files) > 0:
        print(f"  âœ“ {len(output_files)} output files created")
        tests_passed += 1
    else:
        print("  âœ— No output files found")
        tests_failed += 1
    
    # Test 5: Package installation
    print("\n5. Checking package installation...")
    try:
        import training_data_bot
        print("  âœ“ Package importable")
        tests_passed += 1
    except ImportError:
        print("  âœ— Package not installed")
        tests_failed += 1
    
    # Summary
    print("\n" + "="*50)
    print(f"Tests Passed: {tests_passed}")
    print(f"Tests Failed: {tests_failed}")
    print(f"Success Rate: {tests_passed}/{tests_passed + tests_failed}")
    
    if tests_failed == 0:
        print("\nâœ“ PHASE 1 COMPLETE - Ready for Phase 2!")
    else:
        print(f"\nâœ— Fix {tests_failed} issue(s) before Phase 2")

if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: test_real_world.py
================================================================================

import asyncio
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Load .env from the same directory as this script
env_path = project_root / ".env"
load_dotenv(dotenv_path=env_path)

from training_data_bot.ai import AIClient


load_dotenv()


async def test_openai_connection():
    """Test OpenAI API connection and functionality."""
    print("\n" + "="*60)
    print("Testing OpenAI API Connection")
    print("="*60)
   
    api_key = os.getenv("TDB_OPENAI_API_KEY")
    if not api_key:
        print("âŒ TDB_OPENAI_API_KEY not found in .env")
        return
   
    try:
        # Initialize client
        client = AIClient(
            provider="openai",
            api_key=api_key,
            model="gpt-3.5-turbo"  # Cheapest option
        )
        print(f"âœ“ Client initialized: {client}")
       
        # Test 1: Simple generation
        print("\nTest 1: Simple Generation")
        response = await client.generate(
            prompt="What is 2+2? Answer in one sentence.",
            max_tokens=50
        )
        print(f"âœ“ Response: {response.content}")
        print(f"âœ“ Tokens used: {response.tokens_used}")
        print(f"âœ“ Response time: {response.response_time:.2f}s")
       
        # Test 2: Cost calculation
        print("\nTest 2: Cost Estimation")
        cost = client.estimate_cost(
            response.metadata['prompt_tokens'],
            response.metadata['completion_tokens']
        )
        print(f"âœ“ Prompt tokens: {response.metadata['prompt_tokens']}")
        print(f"âœ“ Completion tokens: {response.metadata['completion_tokens']}")
        print(f"âœ“ Estimated cost: ${cost:.4f}")
       
        # Test 3: With system prompt
        print("\nTest 3: With System Prompt")
        response2 = await client.generate(
            prompt="Explain AI in 10 words.",
            system_prompt="You are a helpful assistant that gives concise answers.",
            max_tokens=30
        )
        print(f"âœ“ Response: {response2.content}")
       
        # Test 4: Token counting
        print("\nTest 4: Token Counting")
        test_text = "The Training Data Bot is an enterprise-grade system."
        tokens = client.count_tokens(test_text)
        print(f"âœ“ Text: '{test_text}'")
        print(f"âœ“ Estimated tokens: {tokens}")
       
        await client.close()
       
        print("\n" + "="*60)
        print("âœ“ ALL OPENAI TESTS PASSED!")
        print("="*60)
       
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_openai_connection())



================================================================================
FILE: training_data_bot\__init__.py
================================================================================

"""
Training Data Curation Bot

A bot that curates training data for fine-tuning large language models (LLMs) 
using AI automation and Python.
"""

__version__ = "0.1.0"
__author__ = "Abhinandan"
__email__ = "abhinandan19909@gmail.com"
__description__ = "A bot that curates training data for fine-tuning large language models (LLMs) using user-provided prompts and responses."

# Core imports for easy access
from .core.config import settings
from .core.logging import get_logger
from .core.exceptions import TrainingDataBotError

# Main bot class
from training_data_bot.bot import TrainingDataBot

# Document loading
from training_data_bot.sources import (
    PDFLoader,           # Worker for reading PDF files
    WebLoader,           # Worker for reading web pages
    DocumentLoader,      # Worker for reading documents
    UnifiedLoader,       # Manager who decides which worker to use
)

# Task generation
from training_data_bot.tasks import (
    QAGenerator,                    # Generates Q&A pairs
    ClassificationGenerator,        # Generates classifications
    SummarizationGenerator,         # Generates summaries
    TaskTemplate,                   # Task template definition
    TaskManager,                    # Task orchestration manager
)

# Support services
from .preprocessing import TextPreprocessor    # Text chunking and preprocessing
from .evaluation import QualityEvaluator       # Quality assessment
from .storage import DatasetExporter           # Dataset export to various formats

# Core models (optional - for advanced users)
from training_data_bot.core import (
    Document,
    TextChunk,
    TrainingExample,
    Dataset,
    TaskType,
    DocumentType,
    ExportFormat,
    QualityReport,
)

__all__ = [
    # Core
    "TrainingDataBot",
    "settings",
    "get_logger",
    "TrainingDataBotError",
    
    # Sources
    "PDFLoader",
    "WebLoader",
    "DocumentLoader",
    "UnifiedLoader",
    
    # Tasks
    "QAGenerator",
    "ClassificationGenerator",
    "SummarizationGenerator",
    "TaskTemplate",
    "TaskManager",
    
    # Services
    "TextPreprocessor",
    "QualityEvaluator",
    "DatasetExporter",
    
    # Models (for advanced usage)
    "Document",
    "TextChunk",
    "TrainingExample",
    "Dataset",
    "TaskType",
    "DocumentType",
    "ExportFormat",
    "QualityReport",
]


================================================================================
FILE: training_data_bot\ai\__init__.py
================================================================================

"""
AI client module.

This module provides AI integration for text generation using
various providers (OpenAI, Anthropic, etc.).
"""

from training_data_bot.ai.client import AIClient
from training_data_bot.ai.providers import (
    BaseAIProvider,
    AIResponse,
    OpenAIProvider,
    AnthropicProvider,
)

__all__ = [
    "AIClient",
    "BaseAIProvider",
    "AIResponse",
    "OpenAIProvider",
    "AnthropicProvider",
]


================================================================================
FILE: training_data_bot\ai\client.py
================================================================================

"""
Main AI client for managing AI providers.

This module provides a unified interface for interacting with different
AI providers (OpenAI, Anthropic, etc.).
"""

from typing import Dict, List, Optional, Union
import asyncio

from training_data_bot.core import (
    AIProviderConfig,
    AIProviderError,
    ConfigurationError,
    get_logger,
    get_performance_logger,
    LogContext,
)
from training_data_bot.ai.providers import (
    BaseAIProvider,
    AIResponse,
    OpenAIProvider,
    AnthropicProvider,
)


class AIClient:
    """
    Main AI client for managing providers and generating responses.
    
    Provides a unified interface for interacting with different AI providers.
    """
    
    # Registry of available providers
    PROVIDERS = {
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
    }
    
    def __init__(
        self,
        provider: str = "openai",
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ):
        """
        Initialize the AI client.
        
        Args:
            provider: Provider name ("openai" or "anthropic")
            api_key: API key for the provider
            model: Model name to use
            **kwargs: Additional provider-specific parameters
        """
        self.logger = get_logger("ai.AIClient")
        self.perf_logger = get_performance_logger()
        
        self.provider_name = provider.lower()
        self.provider_instance: Optional[BaseAIProvider] = None
        
        # Initialize provider
        self._initialize_provider(api_key, model, **kwargs)
    
    def _initialize_provider(
        self,
        api_key: Optional[str],
        model: Optional[str],
        **kwargs
    ):
        """Initialize the AI provider."""
        if self.provider_name not in self.PROVIDERS:
            raise ConfigurationError(
                f"Unknown AI provider: {self.provider_name}. "
                f"Available providers: {', '.join(self.PROVIDERS.keys())}"
            )
        
        provider_class = self.PROVIDERS[self.provider_name]
        
        # Validate API key
        if not api_key:
            raise ConfigurationError(
                f"API key required for {self.provider_name} provider"
            )
        
        # Create provider instance
        try:
            self.provider_instance = provider_class(
                api_key=api_key,
                model=model or self._get_default_model(),
                **kwargs
            )
            
            self.logger.info(
                f"Initialized AI client with {self.provider_name} provider",
                provider=self.provider_name,
                model=self.provider_instance.model
            )
            
        except Exception as e:
            raise AIProviderError(
                f"Failed to initialize {self.provider_name} provider: {e}",
                provider=self.provider_name,
                cause=e
            )
    
    def _get_default_model(self) -> str:
        """Get default model for the provider."""
        defaults = {
            "openai": "gpt-3.5-turbo",
            "anthropic": "claude-3-sonnet-20240229",
        }
        return defaults.get(self.provider_name, "gpt-3.5-turbo")
    
    @classmethod
    def from_config(cls, config: AIProviderConfig) -> "AIClient":
        """
        Create AI client from configuration.
        
        Args:
            config: AI provider configuration
            
        Returns:
            Configured AIClient instance
        """
        return cls(
            provider=config.provider_name,
            api_key=config.api_key,
            model=config.model_name,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            timeout=config.timeout,
        )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> AIResponse:
        """
        Generate a response from the AI model.
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            **kwargs: Additional generation parameters
            
        Returns:
            AIResponse object with generated content
        """
        with LogContext("ai_generate", component="AIClient"):
            self.logger.info("Generating AI response")
            
            try:
                response = await self.provider_instance.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    **kwargs
                )
                
                # Log performance metrics
                self.perf_logger.log_api_call(
                    provider=self.provider_name,
                    model=response.model,
                    tokens_used=response.tokens_used,
                    response_time=response.response_time,
                    success=True
                )
                
                return response
                
            except Exception as e:
                self.logger.error(f"AI generation failed: {e}")
                raise
    
    async def generate_batch(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[AIResponse]:
        """
        Generate responses for multiple prompts.
        
        Args:
            prompts: List of user prompts
            system_prompt: Optional system prompt
            max_concurrent: Maximum concurrent requests
            **kwargs: Additional generation parameters
            
        Returns:
            List of AIResponse objects
        """
        with LogContext("ai_generate_batch", component="AIClient"):
            self.logger.info(
                f"Generating {len(prompts)} AI responses",
                batch_size=len(prompts),
                max_concurrent=max_concurrent
            )
            
            # Create semaphore to limit concurrency
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def generate_with_semaphore(prompt):
                async with semaphore:
                    try:
                        return await self.generate(prompt, system_prompt, **kwargs)
                    except Exception as e:
                        self.logger.error(f"Batch generation failed for prompt: {e}")
                        return None
            
            # Execute all tasks
            tasks = [generate_with_semaphore(p) for p in prompts]
            responses = await asyncio.gather(*tasks)
            
            # Filter out None responses
            valid_responses = [r for r in responses if r is not None]
            
            self.logger.info(
                f"Batch generation complete",
                total_prompts=len(prompts),
                successful=len(valid_responses),
                failed=len(prompts) - len(valid_responses)
            )
            
            return valid_responses
    
    def count_tokens(self, text: str) -> int:
        """
        Count tokens in text.
        
        Args:
            text: Text to count tokens for
            
        Returns:
            Number of tokens
        """
        return self.provider_instance.count_tokens(text)
    
    def estimate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """
        Estimate cost for token usage (rough approximation).
        
        Args:
            input_tokens: Number of input tokens
            output_tokens: Number of output tokens
            
        Returns:
            Estimated cost in USD
        """
        # Rough cost estimates (as of 2024)
        costs = {
            "openai": {
                "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},  # per 1K tokens
                "gpt-4": {"input": 0.03, "output": 0.06},
            },
            "anthropic": {
                "claude-3-sonnet": {"input": 0.003, "output": 0.015},
                "claude-3-opus": {"input": 0.015, "output": 0.075},
            }
        }
        
        # Find matching cost structure
        provider_costs = costs.get(self.provider_name, {})
        model_name = self.provider_instance.model.lower()
        
        # Find matching model
        model_costs = None
        for model_key, cost_data in provider_costs.items():
            if model_key in model_name:
                model_costs = cost_data
                break
        
        if not model_costs:
            # Default rough estimate
            return (input_tokens + output_tokens) * 0.002 / 1000
        
        input_cost = (input_tokens / 1000) * model_costs["input"]
        output_cost = (output_tokens / 1000) * model_costs["output"]
        
        return input_cost + output_cost
    
    def get_provider_info(self) -> Dict[str, any]:
        """
        Get information about the current provider.
        
        Returns:
            Dictionary with provider information
        """
        return {
            "provider": self.provider_name,
            **self.provider_instance.get_model_info()
        }
    
    async def close(self):
        """Close the AI client and cleanup resources."""
        if self.provider_instance:
            await self.provider_instance.close()
            self.logger.info("Closed AI client")
    
    async def __aenter__(self):
        """Context manager entry."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        await self.close()
    
    def __repr__(self) -> str:
        """String representation of the client."""
        return f"AIClient(provider={self.provider_name}, model={self.provider_instance.model})"


================================================================================
FILE: training_data_bot\ai\providers\__init__.py
================================================================================

"""
AI providers module.

This module contains implementations for different AI providers
(OpenAI, Anthropic, etc.).
"""

from training_data_bot.ai.providers.base import BaseAIProvider, AIResponse
from training_data_bot.ai.providers.openai_provider import OpenAIProvider
from training_data_bot.ai.providers.anthropic_provider import AnthropicProvider
__all__ = [
    "BaseAIProvider",
    "AIResponse",
    "OpenAIProvider",
    "AnthropicProvider",
]


================================================================================
FILE: training_data_bot\ai\providers\anthropic_provider.py
================================================================================

"""
Anthropic AI provider implementation.

This module provides integration with Anthropic's Claude API for text generation.
"""

import asyncio
import time
from typing import List, Optional

from training_data_bot.core import (
    AIProviderError,
    RateLimitError,
    AuthenticationError,
    get_logger,
)
from training_data_bot.ai.providers.base import BaseAIProvider, AIResponse


class AnthropicProvider(BaseAIProvider):
    """Anthropic Claude API provider implementation."""
    
    def __init__(
        self,
        api_key: str,
        model: str = "claude-3-sonnet-20240229",
        max_tokens: int = 4000,
        temperature: float = 0.7,
        timeout: float = 30.0,
        **kwargs
    ):
        """Initialize Anthropic provider."""
        super().__init__(api_key, model, max_tokens, temperature, timeout, **kwargs)
        self.logger = get_logger("ai.AnthropicProvider")
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize the Anthropic client."""
        try:
            from anthropic import AsyncAnthropic
            
            self.client = AsyncAnthropic(
                api_key=self.api_key,
                timeout=self.timeout
            )
            
            self.logger.info(
                f"Initialized Anthropic provider with model: {self.model}"
            )
            
        except ImportError:
            raise AIProviderError(
                "Anthropic package not installed. Install with: pip install anthropic",
                provider="anthropic"
            )
        except Exception as e:
            raise AIProviderError(
                f"Failed to initialize Anthropic client: {e}",
                provider="anthropic",
                cause=e
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> AIResponse:
        """Generate a response using Anthropic Claude."""
        start_time = time.time()
        
        try:
            # Get parameters
            max_tokens = kwargs.get('max_tokens', self.max_tokens)
            temperature = kwargs.get('temperature', self.temperature)
            
            # Build request parameters
            request_params = {
                "model": self.model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            # Add system prompt if provided
            if system_prompt:
                request_params["system"] = system_prompt
            
            # Add any extra parameters
            request_params.update({
                k: v for k, v in kwargs.items() 
                if k not in ['max_tokens', 'temperature']
            })
            
            # Make API call
            response = await self.client.messages.create(**request_params)
            
            # Calculate response time
            response_time = time.time() - start_time
            
            # Extract response data
            content = response.content[0].text
            finish_reason = response.stop_reason
            
            # Get token usage
            tokens_used = (
                response.usage.input_tokens + response.usage.output_tokens
                if response.usage else 0
            )
            
            self.logger.debug(
                f"Generated response: {tokens_used} tokens in {response_time:.2f}s"
            )
            
            return AIResponse(
                content=content,
                model=self.model,
                tokens_used=tokens_used,
                finish_reason=finish_reason,
                response_time=response_time,
                metadata={
                    "input_tokens": response.usage.input_tokens if response.usage else 0,
                    "output_tokens": response.usage.output_tokens if response.usage else 0,
                }
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # Handle specific error types
            error_message = str(e).lower()
            
            if "rate_limit" in error_message or "429" in error_message:
                raise RateLimitError(
                    f"Anthropic rate limit exceeded: {e}",
                    provider="anthropic",
                    model=self.model,
                    cause=e
                )
            elif "authentication" in error_message or "401" in error_message:
                raise AuthenticationError(
                    f"Anthropic authentication failed: {e}",
                    provider="anthropic",
                    cause=e
                )
            else:
                raise AIProviderError(
                    f"Anthropic generation failed: {e}",
                    provider="anthropic",
                    model=self.model,
                    cause=e
                )
    
    async def generate_batch(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> List[AIResponse]:
        """Generate responses for multiple prompts."""
        tasks = [
            self.generate(prompt, system_prompt, **kwargs)
            for prompt in prompts
        ]
        
        # Execute all tasks concurrently
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out exceptions and log them
        valid_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                self.logger.error(f"Batch generation failed for prompt {i}: {response}")
            else:
                valid_responses.append(response)
        
        return valid_responses
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using Anthropic's tokenizer."""
        try:
            # Anthropic uses a similar tokenization to GPT
            # Rough approximation: ~4 chars per token
            return len(text) // 4
            
        except Exception as e:
            self.logger.warning(f"Token counting failed: {e}, using rough approximation")
            return len(text.split()) * 4 // 3  # Rough approximation
    
    async def close(self):
        """Close the Anthropic client."""
        if self.client:
            await self.client.close()
            self.logger.info("Closed Anthropic client")


================================================================================
FILE: training_data_bot\ai\providers\base.py
================================================================================

"""
Base AI provider interface.

This module defines the abstract base class that all AI providers must implement.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from dataclasses import dataclass


@dataclass
class AIResponse:
    """Response from an AI provider."""
    content: str
    model: str
    tokens_used: int
    finish_reason: str
    response_time: float
    metadata: Dict[str, Any]


class BaseAIProvider(ABC):
    """
    Abstract base class for AI providers.
    
    All AI providers (OpenAI, Anthropic, etc.) must implement this interface.
    """
    
    def __init__(
        self,
        api_key: str,
        model: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        timeout: float = 30.0,
        **kwargs
    ):
        """
        Initialize the AI provider.
        
        Args:
            api_key: API key for the provider
            model: Model name to use
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature (0.0 to 2.0)
            timeout: Request timeout in seconds
            **kwargs: Additional provider-specific parameters
        """
        self.api_key = api_key
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.extra_params = kwargs
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> AIResponse:
        """
        Generate a response from the AI model.
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            **kwargs: Additional generation parameters
            
        Returns:
            AIResponse object with generated content
        """
        pass
    
    @abstractmethod
    async def generate_batch(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> List[AIResponse]:
        """
        Generate responses for multiple prompts.
        
        Args:
            prompts: List of user prompts
            system_prompt: Optional system prompt
            **kwargs: Additional generation parameters
            
        Returns:
            List of AIResponse objects
        """
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """
        Count tokens in text.
        
        Args:
            text: Text to count tokens for
            
        Returns:
            Number of tokens
        """
        pass
    
    @abstractmethod
    async def close(self):
        """Close any open connections or cleanup resources."""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.
        
        Returns:
            Dictionary with model information
        """
        return {
            "provider": self.__class__.__name__,
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "timeout": self.timeout
        }
    
    def __repr__(self) -> str:
        """String representation of the provider."""
        return f"{self.__class__.__name__}(model={self.model})"


================================================================================
FILE: training_data_bot\ai\providers\openai_provider.py
================================================================================

"""
OpenAI AI provider implementation.

This module provides integration with OpenAI's API for text generation.
"""

import asyncio
import time
from typing import List, Optional

from training_data_bot.core import (
    AIProviderError,
    RateLimitError,
    AuthenticationError,
    get_logger,
)
from training_data_bot.ai.providers.base import BaseAIProvider, AIResponse


class OpenAIProvider(BaseAIProvider):
    """OpenAI API provider implementation."""
    
    def __init__(
        self,
        api_key: str,
        model: str = "gpt-3.5-turbo",
        max_tokens: int = 4000,
        temperature: float = 0.7,
        timeout: float = 30.0,
        **kwargs
    ):
        """Initialize OpenAI provider."""
        super().__init__(api_key, model, max_tokens, temperature, timeout, **kwargs)
        self.logger = get_logger("ai.OpenAIProvider")
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize the OpenAI client."""
        try:
            from openai import AsyncOpenAI
            
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                timeout=self.timeout
            )
            
            self.logger.info(
                f"Initialized OpenAI provider with model: {self.model}"
            )
            
        except ImportError:
            raise AIProviderError(
                "OpenAI package not installed. Install with: pip install openai",
                provider="openai"
            )
        except Exception as e:
            raise AIProviderError(
                f"Failed to initialize OpenAI client: {e}",
                provider="openai",
                cause=e
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> AIResponse:
        """Generate a response using OpenAI."""
        start_time = time.time()
        
        try:
            # Build messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # Get parameters
            max_tokens = kwargs.get('max_tokens', self.max_tokens)
            temperature = kwargs.get('temperature', self.temperature)
            
            # Make API call
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                **{k: v for k, v in kwargs.items() if k not in ['max_tokens', 'temperature']}
            )
            
            # Calculate response time
            response_time = time.time() - start_time
            
            # Extract response data
            choice = response.choices[0]
            content = choice.message.content
            finish_reason = choice.finish_reason
            
            # Get token usage
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            self.logger.debug(
                f"Generated response: {tokens_used} tokens in {response_time:.2f}s"
            )
            
            return AIResponse(
                content=content,
                model=self.model,
                tokens_used=tokens_used,
                finish_reason=finish_reason,
                response_time=response_time,
                metadata={
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                }
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # Handle specific error types
            error_message = str(e).lower()
            
            if "rate_limit" in error_message or "429" in error_message:
                raise RateLimitError(
                    f"OpenAI rate limit exceeded: {e}",
                    provider="openai",
                    model=self.model,
                    cause=e
                )
            elif "authentication" in error_message or "401" in error_message:
                raise AuthenticationError(
                    f"OpenAI authentication failed: {e}",
                    provider="openai",
                    cause=e
                )
            else:
                raise AIProviderError(
                    f"OpenAI generation failed: {e}",
                    provider="openai",
                    model=self.model,
                    cause=e
                )
    
    async def generate_batch(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> List[AIResponse]:
        """Generate responses for multiple prompts."""
        tasks = [
            self.generate(prompt, system_prompt, **kwargs)
            for prompt in prompts
        ]
        
        # Execute all tasks concurrently
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out exceptions and log them
        valid_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                self.logger.error(f"Batch generation failed for prompt {i}: {response}")
            else:
                valid_responses.append(response)
        
        return valid_responses
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using tiktoken."""
        try:
            import tiktoken
            
            # Get encoding for the model
            try:
                encoding = tiktoken.encoding_for_model(self.model)
            except KeyError:
                # Fallback to cl100k_base for newer models
                encoding = tiktoken.get_encoding("cl100k_base")
            
            tokens = encoding.encode(text)
            return len(tokens)
            
        except ImportError:
            # Fallback to rough approximation if tiktoken not available
            self.logger.warning(
                "tiktoken not installed, using rough token approximation"
            )
            return len(text.split()) * 4 // 3  # Rough approximation
    
    async def close(self):
        """Close the OpenAI client."""
        if self.client:
            await self.client.close()
            self.logger.info("Closed OpenAI client")


================================================================================
FILE: training_data_bot\API_key_test.py
================================================================================

# main.py
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Verify API key is loaded
api_key = os.getenv('TDB_OPENAI_API_KEY')
if not api_key:
    raise ValueError("API key not found! Check your .env file")

print("âœ“ Environment loaded successfully")


================================================================================
FILE: training_data_bot\bot.py
================================================================================

"""
Main Training Data Bot class.

This module provides the high-level interface for the entire
training data curation system.
"""

import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from uuid import UUID, uuid4

from training_data_bot.core import (
    Document,
    TextChunk,
    TrainingExample,
    Dataset,
    TaskType,
    DocumentType,
    ExportFormat,
    ProcessingJob,
    ProcessingStatus,
    QualityReport,
    get_logger,
    LogContext,
    TrainingDataBotError,
    ConfigurationError,
)

from training_data_bot.sources import UnifiedLoader
from training_data_bot.preprocessing import TextPreprocessor
from training_data_bot.ai import AIClient
from training_data_bot.tasks import TaskManager
from training_data_bot.evaluation import QualityEvaluator
from training_data_bot.storage import DatasetExporter


class TrainingDataBot:
    """
    Main Training Data Bot class.
    
    This class provides a high-level interface for:
    - Loading documents from various sources
    - Processing text with task templates
    - Quality assessment and filtering
    - Dataset creation and export
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the Training Data Bot.
        
        Args:
            config: Optional configuration overrides
        """
        self.logger = get_logger("training_data_bot")
        self.config = config or {}
        
        # Initialize components
        self._init_components()
        
        # State tracking
        self.documents: Dict[UUID, Document] = {}
        self.datasets: Dict[UUID, Dataset] = {}
        self.jobs: Dict[UUID, ProcessingJob] = {}
        
        self.logger.info("Training Data Bot initialized successfully")
    
    def _init_components(self):
        """Initialize all bot components."""
        try:
            # Document loading
            self.loader = UnifiedLoader()
            
            # Text preprocessing
            chunk_size = self.config.get("chunk_size", 1000)
            chunk_overlap = self.config.get("chunk_overlap", 200)
            self.preprocessor = TextPreprocessor(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            # AI client (will be set by user or from config)
            ai_provider = self.config.get("ai_provider", "openai")
            api_key = self.config.get("api_key")
            
            if api_key:
                self.ai_client = AIClient(provider=ai_provider, api_key=api_key)
            else:
                self.ai_client = None
                self.logger.warning("No AI client configured - set api_key to use task generation")
            
            # Task management
            self.task_manager = TaskManager(ai_client=self.ai_client)
            
            # Quality evaluation
            quality_threshold = self.config.get("quality_threshold", 0.7)
            self.evaluator = QualityEvaluator(quality_threshold=quality_threshold)
            
            # Dataset export
            self.exporter = DatasetExporter()
            
            self.logger.info("All components initialized successfully")
            
        except Exception as e:
            raise ConfigurationError(
                "Failed to initialize bot components",
                details={"error": str(e)},
                cause=e
            )
    
    def set_ai_client(self, provider: str, api_key: str, **kwargs):
        """
        Set or update the AI client.
        
        Args:
            provider: AI provider name (openai, anthropic)
            api_key: API key for the provider
            **kwargs: Additional client parameters
        """
        self.ai_client = AIClient(provider=provider, api_key=api_key, **kwargs)
        self.task_manager.ai_client = self.ai_client
        self.logger.info(f"AI client set to {provider}")
    
    async def load_documents(
        self,
        sources: Union[str, Path, List[Union[str, Path]]],
        doc_types: Optional[List[DocumentType]] = None,
        **kwargs
    ) -> List[Document]:
        """
        Load documents from various sources.
        
        Args:
            sources: File path(s), directory, or URL(s)
            doc_types: Optional filter for document types
            **kwargs: Additional loading parameters
            
        Returns:
            List of loaded Document objects
        """
        with LogContext("load_documents"):
            self.logger.info(f"Loading documents from sources")
            
            # Normalize sources to list
            if isinstance(sources, (str, Path)):
                sources = [sources]
            
            # Load documents
            documents = []
            for source in sources:
                # Handle URLs vs file paths
                if str(source).startswith('http'):
                    # It's a URL
                    doc = await self.loader.load_single(source, **kwargs)
                    documents.append(doc)
                else:
                    # It's a file system path
                    source_path = Path(source)
                    
                    # Check if it exists and is a directory
                    if source_path.exists() and source_path.is_dir():
                        self.logger.info(f"Loading directory: {source_path}")
                        dir_docs = await self.loader.load_directory(source_path, **kwargs)
                        documents.extend(dir_docs)
                    elif source_path.exists():
                        # It's a file
                        doc = await self.loader.load_single(source, **kwargs)
                        documents.append(doc)
                    else:
                        # Path doesn't exist
                        self.logger.error(f"Source not found: {source_path}")
                        raise TrainingDataBotError(f"Source not found: {source_path}")
            
            # Filter by document type if specified
            if doc_types:
                documents = [d for d in documents if d.doc_type in doc_types]
            
            # Store documents
            for doc in documents:
                self.documents[doc.id] = doc
            
            self.logger.info(f"Loaded {len(documents)} documents")
            return documents
            
    async def process_documents(
        self,
        documents: Optional[List[Document]] = None,
        task_types: Optional[List[TaskType]] = None,
        quality_filter: bool = True,
        **kwargs
    ) -> Dataset:
        """
        Process documents into training examples.
        
        Args:
            documents: Documents to process (uses all loaded if None)
            task_types: Types of tasks to generate (default: QA + Summarization)
            quality_filter: Whether to filter by quality threshold
            **kwargs: Additional processing parameters
            
        Returns:
            Dataset with training examples
        """
        with LogContext("process_documents"):
            # Use all documents if none specified
            if documents is None:
                documents = list(self.documents.values())
            
            if not documents:
                raise TrainingDataBotError("No documents to process")
            
            # Default task types
            if task_types is None:
                task_types = [TaskType.QA_GENERATION, TaskType.SUMMARIZATION]
            
            self.logger.info(
                f"Processing {len(documents)} documents",
                task_types=[t.value for t in task_types]
            )
            
            # Step 1: Preprocess into chunks
            all_chunks = []
            for doc in documents:
                chunks = self.preprocessor.process_document(doc)
                all_chunks.extend(chunks)
            
            self.logger.info(f"Created {len(all_chunks)} text chunks")
            
            # Step 2: Generate training examples
            examples = await self.task_manager.create_training_examples(
                chunks=all_chunks,
                task_types=task_types,
                **kwargs
            )
            
            self.logger.info(f"Generated {len(examples)} training examples")
            
            # Step 3: Quality filtering (optional)
            if quality_filter:
                filtered_examples = []
                for example in examples:
                    report = self.evaluator.evaluate_example(example, detailed=False)
                    if report.passed:
                        filtered_examples.append(example)
                
                self.logger.info(
                    f"Quality filtering: {len(filtered_examples)}/{len(examples)} passed"
                )
                examples = filtered_examples
            
            # Step 4: Create dataset
            dataset = Dataset(
                id=uuid4(),
                name=f"Dataset_{len(self.datasets) + 1}",
                description=f"Generated from {len(documents)} documents",
                examples=examples,
                total_examples=len(examples)
            )
            
            # Store dataset
            self.datasets[dataset.id] = dataset
            
            self.logger.info(f"Created dataset with {len(examples)} examples")
            return dataset
    
    async def evaluate_dataset(
        self,
        dataset: Dataset,
        detailed_report: bool = True
    ) -> QualityReport:
        """
        Evaluate dataset quality.
        
        Args:
            dataset: Dataset to evaluate
            detailed_report: Whether to generate detailed report
            
        Returns:
            QualityReport with evaluation results
        """
        with LogContext("evaluate_dataset", dataset_id=str(dataset.id)):
            self.logger.info(f"Evaluating dataset {dataset.name}")
            
            report = self.evaluator.evaluate_dataset(
                dataset=dataset,
                detailed_report=detailed_report
            )
            
            self.logger.info(
                f"Dataset evaluation complete",
                overall_score=report.overall_score,
                passed=report.passed
            )
            
            return report
    
    async def export_dataset(
        self,
        dataset: Dataset,
        output_path: Union[str, Path],
        format: ExportFormat = ExportFormat.JSONL,
        split_data: bool = True,
        **kwargs
    ) -> Path:
        """
        Export dataset to file.
        
        Args:
            dataset: Dataset to export
            output_path: Output file path
            format: Export format (JSONL, JSON, CSV, Parquet)
            split_data: Whether to split into train/val/test
            **kwargs: Additional export options
            
        Returns:
            Path to exported file(s)
        """
        with LogContext("export_dataset", dataset_id=str(dataset.id)):
            self.logger.info(
                f"Exporting dataset to {output_path}",
                format=format.value
            )
            
            result_path = await self.exporter.export_dataset(
                dataset=dataset,
                output_path=output_path,
                format=format,
                split_data=split_data,
                **kwargs
            )
            
            self.logger.info(f"Dataset exported successfully to {result_path}")
            return result_path
    
    async def quick_process(
        self,
        source: Union[str, Path],
        output_path: Union[str, Path],
        task_types: Optional[List[TaskType]] = None,
        export_format: ExportFormat = ExportFormat.JSONL
    ) -> Dataset:
        """
        Quick end-to-end processing: load -> process -> export.
        
        Args:
            source: Document source (file, directory, or URL)
            output_path: Output file path
            task_types: Task types to generate
            export_format: Export format
            
        Returns:
            Created dataset
        """
        with LogContext("quick_process"):
            self.logger.info("Starting quick process workflow")
            
            # Load documents
            documents = await self.load_documents([source])
            
            # Process into dataset
            dataset = await self.process_documents(
                documents=documents,
                task_types=task_types
            )
            
            # Export dataset
            await self.export_dataset(
                dataset=dataset,
                output_path=output_path,
                format=export_format
            )
            
            self.logger.info("Quick process complete")
            return dataset
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive statistics about the bot's state.
        
        Returns:
            Dictionary with statistics
        """
        return {
            "documents": {
                "total": len(self.documents),
                "by_type": self._count_by_type(self.documents.values(), "doc_type"),
                "total_size": sum(
                    len(doc.content) for doc in self.documents.values()
                )
            },
            "datasets": {
                "total": len(self.datasets),
                "total_examples": sum(
                    len(ds.examples) for ds in self.datasets.values()
                ),
                "by_task_type": self._count_examples_by_task_type()
            },
            "jobs": {
                "total": len(self.jobs),
                "by_status": self._count_by_type(self.jobs.values(), "status"),
                "active": len([
                    j for j in self.jobs.values()
                    if j.status == ProcessingStatus.RUNNING
                ])
            },
            "task_manager": self.task_manager.get_statistics() if self.task_manager else {}
        }
    
    def _count_by_type(self, items, attr_name: str) -> Dict[str, int]:
        """Count items by a specific attribute."""
        counts = {}
        for item in items:
            value = getattr(item, attr_name)
            key = value.value if hasattr(value, 'value') else str(value)
            counts[key] = counts.get(key, 0) + 1
        return counts
    
    def _count_examples_by_task_type(self) -> Dict[str, int]:
        """Count examples by task type across all datasets."""
        counts = {}
        for dataset in self.datasets.values():
            for example in dataset.examples:
                task_type = example.task_type.value
                counts[task_type] = counts.get(task_type, 0) + 1
        return counts
    
    async def cleanup(self):
        """Cleanup resources and close connections."""
        try:
            if self.ai_client:
                await self.ai_client.close()
            
            self.logger.info("Bot cleanup completed")
        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")
    
    async def __aenter__(self):
        """Context manager entry."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        await self.cleanup()
    
    def __repr__(self) -> str:
        """String representation of the bot."""
        return (
            f"TrainingDataBot("
            f"documents={len(self.documents)}, "
            f"datasets={len(self.datasets)})"
        )


================================================================================
FILE: training_data_bot\core\__init__.py
================================================================================

"""
Core module for the Training Data Bot.

This module provides the foundational data models, exceptions, and utilities
used throughout the system.
"""

# Data models
from training_data_bot.core.models import (
    # Base classes
    BaseEntity,
    
    # Enums
    DocumentType,
    TaskType,
    QualityMetric,
    ProcessingStatus,
    ExportFormat,
    
    # Document family
    Document,
    TextChunk,
    
    # Task family
    TaskTemplate,
    TaskResult,
    
    # Training family
    TrainingExample,
    Dataset,
    
    # Quality family
    QualityReport,
    
    # Operations family
    ProcessingJob,
    
    # Configuration models
    AIProviderConfig,
    ProcessingConfig,
    
    # Type aliases
    DocumentSource,
    TaskParameters,
    QualityScores,
    MetricScores,
)

# Exceptions
from training_data_bot.core.exceptions import (
    # Base exceptions
    TrainingDataBotError,
    
    # Configuration errors
    ConfigurationError,
    InitializationError,
    
    # Document loading errors
    DocumentLoadError,
    UnsupportedFormatError,
    FileNotFoundError,
    FileCorruptedError,
    WebLoadError,
    
    # AI and task errors
    AIClientError,
    AIProviderError,
    RateLimitError,
    AuthenticationError,
    TaskProcessingError,
    TaskTemplateError,
    TaskTimeoutError,
    
    # Data processing errors
    ValidationError,
    ProcessingError,
    QualityError,
    
    # Storage errors
    StorageError,
    ExportError,
    ImportError,
    DatabaseError,
    
    # Resource errors
    ResourceError,
    MemoryError,
    TimeoutError,
    
    # Utility functions
    handle_exception,
    is_recoverable_error,
    get_retry_delay,
)

# Configuration and logging (Session 2 additions)
from training_data_bot.core.config import (
    Settings,
    LogLevel,
    Environment,
    create_settings,
    get_settings,
    update_settings,
    load_settings_from_env,
    validate_configuration,
    settings,  # Global settings instance
)

from training_data_bot.core.logging import (
    TrainingDataBotLogger,
    LogContext,
    PerformanceLogger,
    setup_logging,
    get_logger,
    get_performance_logger,
    setup_logging_from_settings,
)

# Version info
__version__ = "0.1.0"

# Public API for this module
__all__ = [
    # Models
    "BaseEntity",
    "DocumentType",
    "TaskType", 
    "QualityMetric",
    "ProcessingStatus",
    "ExportFormat",
    "Document",
    "TextChunk",
    "TaskTemplate",
    "TaskResult",
    "TrainingExample",
    "Dataset",
    "QualityReport",
    "ProcessingJob",
    "AIProviderConfig",
    "ProcessingConfig",
    "DocumentSource",
    "TaskParameters",
    "QualityScores",
    "MetricScores",
    
    # Configuration
    "Settings",
    "LogLevel",
    "Environment", 
    "create_settings",
    "get_settings",
    "update_settings",
    "load_settings_from_env",
    "validate_configuration",
    "settings",
    
    # Logging
    "TrainingDataBotLogger",
    "LogContext",
    "PerformanceLogger",
    "setup_logging",
    "get_logger",
    "get_performance_logger",
    "setup_logging_from_settings",
    
    # Exceptions
    "TrainingDataBotError",
    "ConfigurationError",
    "InitializationError",
    "DocumentLoadError",
    "UnsupportedFormatError", 
    "FileNotFoundError",
    "FileCorruptedError",
    "WebLoadError",
    "AIClientError",
    "AIProviderError",
    "RateLimitError",
    "AuthenticationError",
    "TaskProcessingError",
    "TaskTemplateError",
    "TaskTimeoutError",
    "ValidationError",
    "ProcessingError",
    "QualityError",
    "StorageError",
    "ExportError",
    "ImportError",
    "DatabaseError", 
    "ResourceError",
    "MemoryError",
    "TimeoutError",
    "handle_exception",
    "is_recoverable_error",
    "get_retry_delay",
]


================================================================================
FILE: training_data_bot\core\config.py
================================================================================

"""
Configuration management for the Training Data Bot.

This module handles all configuration loading from environment variables,
config files, and provides default settings for the system.
"""

import os
import yaml
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from pydantic import Field, validator
from pydantic_settings import BaseSettings
from enum import Enum

from training_data_bot.core.models import AIProviderConfig, ProcessingConfig


class LogLevel(str, Enum):
    """Available logging levels."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class Environment(str, Enum):
    """Application environments."""
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    TESTING = "testing"


class Settings(BaseSettings):
    """Main application settings."""
    
    # Application info
    app_name: str = "Training Data Bot"
    app_version: str = "0.1.0"
    environment: Environment = Environment.DEVELOPMENT
    debug: bool = Field(default=False)
    
    # Logging configuration
    log_level: LogLevel = LogLevel.INFO
    log_file: Optional[str] = None
    log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    log_max_bytes: int = 10485760  # 10MB
    log_backup_count: int = 5
    enable_structured_logging: bool = True
    
    # File processing settings
    max_file_size_mb: int = Field(default=100, gt=0)
    supported_file_types: List[str] = Field(default=[
        "pdf", "txt", "md", "html", "json", "csv", "docx"
    ])
    default_encoding: str = "utf-8"
    
    # Text processing configuration
    chunk_size: int = Field(default=1000, gt=0)
    chunk_overlap: int = Field(default=200, ge=0)
    min_chunk_size: int = Field(default=100, gt=0)
    max_chunks_per_document: int = Field(default=1000, gt=0)
    
    # Performance settings
    max_workers: int = Field(default=4, gt=0, le=32)
    batch_size: int = Field(default=10, gt=0)
    request_timeout: float = Field(default=30.0, gt=0)
    retry_attempts: int = Field(default=3, ge=0)
    retry_delay: float = Field(default=1.0, gt=0)
    enable_parallel_processing: bool = True
    
    # Quality control
    quality_threshold: float = Field(default=0.7, ge=0.0, le=1.0)
    enable_quality_filtering: bool = True
    min_quality_examples: int = Field(default=10, ge=0)
    
    # AI Provider settings
    default_ai_provider: str = "openai"
    ai_providers: Dict[str, AIProviderConfig] = Field(default_factory=dict)
    
    # OpenAI specific settings
    openai_api_key: Optional[str] = None
    openai_api_base: Optional[str] = None
    openai_model: str = "gpt-3.5-turbo"
    openai_max_tokens: int = 4000
    openai_temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    
    # Anthropic specific settings  
    anthropic_api_key: Optional[str] = None
    anthropic_model: str = "claude-3-sonnet-20240229"
    anthropic_max_tokens: int = 4000
    
    # Web scraping settings
    web_timeout: float = Field(default=30.0, gt=0)
    web_max_redirects: int = Field(default=5, ge=0)
    web_user_agent: str = "TrainingDataBot/0.1.0"
    enable_web_caching: bool = True
    web_cache_ttl: int = Field(default=3600, gt=0)  # 1 hour
    
    # Database settings (optional)
    database_url: Optional[str] = None
    database_echo: bool = False
    database_pool_size: int = Field(default=5, gt=0)
    database_max_overflow: int = Field(default=10, ge=0)
    
    # Storage settings
    output_directory: str = "output"
    temp_directory: str = "temp"
    enable_compression: bool = True
    default_export_format: str = "jsonl"
    
    # Security settings
    max_request_size_mb: int = Field(default=50, gt=0)
    allowed_domains: List[str] = Field(default_factory=list)
    blocked_domains: List[str] = Field(default_factory=list)
    
    # Rate limiting
    requests_per_minute: int = Field(default=60, gt=0)
    requests_per_hour: int = Field(default=3600, gt=0)
    requests_per_day: int = Field(default=86400, gt=0)
    
    @validator('chunk_overlap')
    def validate_chunk_overlap(cls, v, values):
        """Ensure chunk overlap is less than chunk size."""
        if 'chunk_size' in values and v >= values['chunk_size']:
            raise ValueError("chunk_overlap must be less than chunk_size")
        return v
    
    @validator('ai_providers', pre=True)
    def parse_ai_providers(cls, v):
        """Parse AI provider configurations from environment or dict."""
        if isinstance(v, str):
            # If it's a string, try to parse as YAML/JSON
            try:
                import json
                return json.loads(v)
            except json.JSONDecodeError:
                try:
                    return yaml.safe_load(v)
                except yaml.YAMLError:
                    return {}
        return v or {}
    
    @validator('supported_file_types')
    def normalize_file_types(cls, v):
        """Normalize file types to lowercase without dots."""
        return [ext.lower().lstrip('.') for ext in v]
    
    def get_ai_provider_config(self, provider_name: str) -> Optional[AIProviderConfig]:
        """Get configuration for a specific AI provider."""
        if provider_name in self.ai_providers:
            return AIProviderConfig(**self.ai_providers[provider_name])
        
        # Create default configs for known providers
        if provider_name == "openai":
            return AIProviderConfig(
                provider_name="openai",
                api_key=self.openai_api_key,
                api_url=self.openai_api_base,
                model_name=self.openai_model,
                max_tokens=self.openai_max_tokens,
                temperature=self.openai_temperature,
                timeout=self.request_timeout,
                retry_attempts=self.retry_attempts,
            )
        elif provider_name == "anthropic":
            return AIProviderConfig(
                provider_name="anthropic",
                api_key=self.anthropic_api_key,
                model_name=self.anthropic_model,
                max_tokens=self.anthropic_max_tokens,
                temperature=0.7,
                timeout=self.request_timeout,
                retry_attempts=self.retry_attempts,
            )
        
        return None
    
    def get_processing_config(self) -> ProcessingConfig:
        """Get text processing configuration."""
        return ProcessingConfig(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            max_workers=self.max_workers,
            batch_size=self.batch_size,
            quality_threshold=self.quality_threshold,
            enable_parallel_processing=self.enable_parallel_processing,
            max_file_size_mb=self.max_file_size_mb,
        )
    
    def create_directories(self):
        """Create necessary directories if they don't exist."""
        directories = [self.output_directory, self.temp_directory]
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def is_development(self) -> bool:
        """Check if running in development mode."""
        return self.environment == Environment.DEVELOPMENT
    
    def is_production(self) -> bool:
        """Check if running in production mode."""
        return self.environment == Environment.PRODUCTION
    
    class Config:
        """Pydantic settings configuration."""
        env_file = ".env"
        env_file_encoding = "utf-8"
        env_prefix = "TDB_"  # Training Data Bot prefix
        case_sensitive = False
        extra = "allow"
        
        # Field aliases for environment variables
        fields = {
            "openai_api_key": {"env": ["TDB_OPENAI_API_KEY", "OPENAI_API_KEY"]},
            "anthropic_api_key": {"env": ["TDB_ANTHROPIC_API_KEY", "ANTHROPIC_API_KEY"]},
            "database_url": {"env": ["TDB_DATABASE_URL", "DATABASE_URL"]},
            "log_level": {"env": ["TDB_LOG_LEVEL", "LOG_LEVEL"]},
            "environment": {"env": ["TDB_ENVIRONMENT", "ENVIRONMENT"]},
        }


def load_config_from_file(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load configuration from a YAML or JSON file.
    
    Args:
        config_path: Path to the configuration file
        
    Returns:
        Configuration dictionary
    """
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        if config_path.suffix.lower() in ['.yaml', '.yml']:
            return yaml.safe_load(f) or {}
        elif config_path.suffix.lower() == '.json':
            import json
            return json.load(f)
        else:
            raise ValueError(f"Unsupported configuration file format: {config_path.suffix}")


def create_settings(
    config_file: Optional[Union[str, Path]] = None,
    **overrides
) -> Settings:
    """
    Create settings instance with optional configuration file and overrides.
    
    Args:
        config_file: Optional path to configuration file
        **overrides: Additional configuration overrides
        
    Returns:
        Configured Settings instance
    """
    config_data = {}
    
    # Load from file if provided
    if config_file:
        config_data.update(load_config_from_file(config_file))
    
    # Apply overrides
    config_data.update(overrides)
    
    # Create settings instance
    settings = Settings(**config_data)
    
    # Create necessary directories
    settings.create_directories()
    
    return settings


# Global settings instance
settings = create_settings()


def get_settings() -> Settings:
    """Get the global settings instance."""
    return settings


def update_settings(**overrides) -> Settings:
    """Update global settings with new values."""
    global settings
    current_dict = settings.dict()
    current_dict.update(overrides)
    settings = Settings(**current_dict)
    settings.create_directories()
    return settings


def load_settings_from_env() -> Settings:
    """Load settings primarily from environment variables."""
    return Settings()


def validate_configuration(config: Settings) -> List[str]:
    """
    Validate configuration and return list of issues.
    
    Args:
        config: Settings instance to validate
        
    Returns:
        List of validation error messages
    """
    issues = []
    
    # Check for required API keys based on default provider
    if config.default_ai_provider == "openai" and not config.openai_api_key:
        issues.append("OpenAI API key is required when using OpenAI as default provider")
    
    if config.default_ai_provider == "anthropic" and not config.anthropic_api_key:
        issues.append("Anthropic API key is required when using Anthropic as default provider")
    
    # Check chunk configuration
    if config.chunk_overlap >= config.chunk_size:
        issues.append("Chunk overlap must be less than chunk size")
    
    if config.min_chunk_size >= config.chunk_size:
        issues.append("Minimum chunk size must be less than chunk size")
    
    # Check quality threshold
    if not (0.0 <= config.quality_threshold <= 1.0):
        issues.append("Quality threshold must be between 0.0 and 1.0")
    
    # Check worker limits
    if config.max_workers > 32:
        issues.append("Max workers should not exceed 32 for stability")
    
    # Check file size limits
    if config.max_file_size_mb > 1000:  # 1GB
        issues.append("Max file size should not exceed 1GB")
    
    return issues


================================================================================
FILE: training_data_bot\core\exceptions.py
================================================================================

"""
Custom exceptions for the Training Data Bot.

This module defines all custom exception types used throughout the system
for better error handling and debugging.
"""

from typing import Any, Dict, Optional


class TrainingDataBotError(Exception):
    """Base exception for all Training Data Bot errors."""
    
    def __init__(
        self,
        message: str,
        error_code: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None
    ):
        super().__init__(message)
        self.message = message
        self.error_code = error_code or self.__class__.__name__
        self.details = details or {}
        self.cause = cause
    
    def __str__(self) -> str:
        """String representation of the error."""
        parts = [f"{self.error_code}: {self.message}"]
        
        if self.details:
            details_str = ", ".join(f"{k}={v}" for k, v in self.details.items())
            parts.append(f"Details: {details_str}")
        
        if self.cause:
            parts.append(f"Caused by: {self.cause}")
        
        return " | ".join(parts)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert exception to dictionary for logging/serialization."""
        return {
            "error_code": self.error_code,
            "message": self.message,
            "details": self.details,
            "cause": str(self.cause) if self.cause else None,
            "type": self.__class__.__name__
        }


# Configuration and initialization errors
class ConfigurationError(TrainingDataBotError):
    """Raised when there are configuration issues."""
    pass


class InitializationError(TrainingDataBotError):
    """Raised when component initialization fails."""
    pass


# Document loading errors
class DocumentLoadError(TrainingDataBotError):
    """Raised when document loading fails."""
    
    def __init__(
        self,
        message: str,
        file_path: Optional[str] = None,
        file_type: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if file_path:
            details['file_path'] = file_path
        if file_type:
            details['file_type'] = file_type
        
        super().__init__(message, details=details, **kwargs)


class UnsupportedFormatError(DocumentLoadError):
    """Raised when trying to load an unsupported file format."""
    pass


class FileNotFoundError(DocumentLoadError):
    """Raised when a file cannot be found."""
    pass


class FileCorruptedError(DocumentLoadError):
    """Raised when a file is corrupted or unreadable."""
    pass


class WebLoadError(DocumentLoadError):
    """Raised when web content loading fails."""
    
    def __init__(
        self,
        message: str,
        url: Optional[str] = None,
        status_code: Optional[int] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if url:
            details['url'] = url
        if status_code:
            details['status_code'] = status_code
        
        super().__init__(message, details=details, **kwargs)


# AI and task processing errors
class AIClientError(TrainingDataBotError):
    """Base class for AI client errors."""
    pass


class AIProviderError(AIClientError):
    """Raised when AI provider returns an error."""
    
    def __init__(
        self,
        message: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if provider:
            details['provider'] = provider
        if model:
            details['model'] = model
        
        super().__init__(message, details=details, **kwargs)


class RateLimitError(AIProviderError):
    """Raised when API rate limits are exceeded."""
    
    def __init__(
        self,
        message: str,
        retry_after: Optional[int] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if retry_after:
            details['retry_after'] = retry_after
        
        super().__init__(message, details=details, **kwargs)


class AuthenticationError(AIProviderError):
    """Raised when API authentication fails."""
    pass


class TaskProcessingError(TrainingDataBotError):
    """Raised when task processing fails."""
    
    def __init__(
        self,
        message: str,
        task_type: Optional[str] = None,
        chunk_id: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if task_type:
            details['task_type'] = task_type
        if chunk_id:
            details['chunk_id'] = chunk_id
        
        super().__init__(message, details=details, **kwargs)


class TaskTemplateError(TaskProcessingError):
    """Raised when task template is invalid or malformed."""
    pass


class TaskTimeoutError(TaskProcessingError):
    """Raised when task processing times out."""
    pass


# Data processing and validation errors
class ValidationError(TrainingDataBotError):
    """Raised when data validation fails."""
    
    def __init__(
        self,
        message: str,
        field_name: Optional[str] = None,
        invalid_value: Optional[Any] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if field_name:
            details['field_name'] = field_name
        if invalid_value is not None:
            details['invalid_value'] = str(invalid_value)
        
        super().__init__(message, details=details, **kwargs)


class ProcessingError(TrainingDataBotError):
    """Raised when data processing fails."""
    pass


class QualityError(TrainingDataBotError):
    """Raised when quality assessment fails or quality is too low."""
    
    def __init__(
        self,
        message: str,
        quality_score: Optional[float] = None,
        threshold: Optional[float] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if quality_score is not None:
            details['quality_score'] = quality_score
        if threshold is not None:
            details['threshold'] = threshold
        
        super().__init__(message, details=details, **kwargs)


# Storage and export errors
class StorageError(TrainingDataBotError):
    """Base class for storage-related errors."""
    pass


class ExportError(StorageError):
    """Raised when data export fails."""
    
    def __init__(
        self,
        message: str,
        export_format: Optional[str] = None,
        output_path: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if export_format:
            details['export_format'] = export_format
        if output_path:
            details['output_path'] = output_path
        
        super().__init__(message, details=details, **kwargs)


class ImportError(StorageError):
    """Raised when data import fails."""
    pass


class DatabaseError(StorageError):
    """Raised when database operations fail."""
    
    def __init__(
        self,
        message: str,
        operation: Optional[str] = None,
        table_name: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if operation:
            details['operation'] = operation
        if table_name:
            details['table_name'] = table_name
        
        super().__init__(message, details=details, **kwargs)


# Resource and system errors
class ResourceError(TrainingDataBotError):
    """Raised when system resources are insufficient."""
    pass


class MemoryError(ResourceError):
    """Raised when memory limits are exceeded."""
    
    def __init__(
        self,
        message: str,
        required_memory: Optional[int] = None,
        available_memory: Optional[int] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if required_memory:
            details['required_memory'] = required_memory
        if available_memory:
            details['available_memory'] = available_memory
        
        super().__init__(message, details=details, **kwargs)


class TimeoutError(ResourceError):
    """Raised when operations timeout."""
    
    def __init__(
        self,
        message: str,
        timeout_seconds: Optional[float] = None,
        operation: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.get('details', {})
        if timeout_seconds:
            details['timeout_seconds'] = timeout_seconds
        if operation:
            details['operation'] = operation
        
        super().__init__(message, details=details, **kwargs)


# Utility functions for error handling
def handle_exception(
    exception: Exception,
    context: Optional[str] = None,
    reraise_as: Optional[type] = None,
    **details
) -> TrainingDataBotError:
    """
    Handle and wrap exceptions with additional context.
    
    Args:
        exception: The original exception
        context: Additional context about where the error occurred
        reraise_as: Exception class to reraise as
        **details: Additional details to include
    
    Returns:
        TrainingDataBotError or specified exception type
    """
    message = str(exception)
    if context:
        message = f"{context}: {message}"
    
    exception_class = reraise_as or TrainingDataBotError
    
    return exception_class(
        message=message,
        cause=exception,
        details=details
    )


def is_recoverable_error(error: Exception) -> bool:
    """
    Determine if an error is recoverable (should retry).
    
    Args:
        error: The exception to check
        
    Returns:
        True if the error is recoverable, False otherwise
    """
    recoverable_types = (
        RateLimitError,
        TimeoutError,
        WebLoadError,
    )
    
    # Check if it's a recoverable type
    if isinstance(error, recoverable_types):
        return True
    
    # Check if it's a temporary network issue
    if isinstance(error, AIProviderError):
        # 5xx status codes are typically temporary
        status_code = error.details.get('status_code')
        if status_code and 500 <= status_code < 600:
            return True
    
    return False


def get_retry_delay(error: Exception, attempt: int) -> float:
    """
    Calculate retry delay based on error type and attempt number.
    
    Args:
        error: The exception that occurred
        attempt: The current attempt number (1-based)
        
    Returns:
        Delay in seconds before retry
    """
    base_delay = 1.0
    max_delay = 60.0
    
    # Exponential backoff
    delay = min(base_delay * (2 ** (attempt - 1)), max_delay)
    
    # Special handling for rate limit errors
    if isinstance(error, RateLimitError):
        retry_after = error.details.get('retry_after')
        if retry_after:
            return max(delay, retry_after)
    
    return delay


================================================================================
FILE: training_data_bot\core\logging.py
================================================================================

"""
Logging infrastrucutre for the Training Data Bot. 

This module provides structured logging with context management, performance tracking, and configurable output formats. 
"""


import json
import logging
import logging.handlers
import sys
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Union
from uuid import UUID, uuid4


class ContextFilter(logging.Filter):
    """Filter to add context information to log records."""
    
    def __init__(self):
        super().__init__()
        self.context_stack = []
    
    def filter(self, record):
        """Add context information to the log record."""
        # Add context information if available
        if self.context_stack:
            current_context = self.context_stack[-1]
            record.context_id = current_context.get('context_id', 'unknown')
            record.operation = current_context.get('operation', 'unknown')
            record.component = current_context.get('component', 'unknown')
            
            # Add any additional context data
            for key, value in current_context.items():
                if key not in ['context_id', 'operation', 'component']:
                    setattr(record, f"ctx_{key}", value)
        else:
            record.context_id = 'root'
            record.operation = 'system'
            record.component = 'core'
        
        # Add timestamp
        record.timestamp = datetime.utcnow().isoformat()
        
        return True
    
    def push_context(self, context: Dict[str, Any]):
        """Push a new context onto the stack."""
        self.context_stack.append(context)
    
    def pop_context(self):
        """Pop the current context from the stack."""
        if self.context_stack:
            return self.context_stack.pop()
        return None


class StructuredFormatter(logging.Formatter):
    """Formatter that outputs structured JSON logs."""
    
    def format(self, record):
        """Format the log record as structured JSON."""
        log_data = {
            'timestamp': getattr(record, 'timestamp', datetime.utcnow().isoformat()),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'context_id': getattr(record, 'context_id', 'unknown'),
            'operation': getattr(record, 'operation', 'unknown'),
            'component': getattr(record, 'component', 'unknown'),
        }
        
        # Add context data
        for attr_name in dir(record):
            if attr_name.startswith('ctx_'):
                key = attr_name[4:]  # Remove 'ctx_' prefix
                log_data[key] = getattr(record, attr_name)
        
        # Add exception information if present
        if record.exc_info:
            log_data['exception'] = {
                'type': record.exc_info[0].__name__,
                'message': str(record.exc_info[1]),
                'traceback': self.formatException(record.exc_info)
            }
        
        # Add extra fields
        if hasattr(record, 'extra_fields'):
            log_data.update(record.extra_fields)
        
        return json.dumps(log_data, default=str, ensure_ascii=False)


class HumanReadableFormatter(logging.Formatter):
    """Formatter that outputs human-readable logs with context."""
    
    def __init__(self):
        super().__init__(
            fmt='%(timestamp)s [%(levelname)s] %(component)s.%(operation)s (%(context_id)s) - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
    
    def format(self, record):
        """Format the log record in a human-readable format."""
        # Ensure required attributes exist
        if not hasattr(record, 'timestamp'):
            record.timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        if not hasattr(record, 'context_id'):
            record.context_id = 'root'
        if not hasattr(record, 'operation'):
            record.operation = 'system'
        if not hasattr(record, 'component'):
            record.component = 'core'
        
        return super().format(record)


class TrainingDataBotLogger:
    """Main logger class for the Training Data Bot."""
    
    def __init__(
        self,
        name: str,
        level: str = "INFO",
        log_file: Optional[str] = None,
        structured: bool = True,
        max_bytes: int = 10485760,  # 10MB
        backup_count: int = 5
    ):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # Create context filter
        self.context_filter = ContextFilter()
        self.logger.addFilter(self.context_filter)
        
        # Clear existing handlers
        self.logger.handlers.clear()
        
        # Create console handler
        console_handler = logging.StreamHandler(sys.stdout)
        if structured:
            console_handler.setFormatter(StructuredFormatter())
        else:
            console_handler.setFormatter(HumanReadableFormatter())
        self.logger.addHandler(console_handler)
        
        # Create file handler if log file specified
        if log_file:
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.handlers.RotatingFileHandler(
                log_file,
                maxBytes=max_bytes,
                backupCount=backup_count,
                encoding='utf-8'
            )
            file_handler.setFormatter(StructuredFormatter())
            self.logger.addHandler(file_handler)
        
        # Prevent propagation to root logger
        self.logger.propagate = False
    
    def _log_with_extra(self, level: int, message: str, **extra):
        """Log a message with extra context fields."""
        if extra:
            # Create a custom LogRecord with extra fields
            record = self.logger.makeRecord(
                self.logger.name,
                level,
                __file__,
                0,
                message,
                (),
                None
            )
            record.extra_fields = extra
            self.logger.handle(record)
        else:
            self.logger.log(level, message)
    
    def debug(self, message: str, **extra):
        """Log debug message."""
        self._log_with_extra(logging.DEBUG, message, **extra)
    
    def info(self, message: str, **extra):
        """Log info message."""
        self._log_with_extra(logging.INFO, message, **extra)
    
    def warning(self, message: str, **extra):
        """Log warning message."""
        self._log_with_extra(logging.WARNING, message, **extra)
    
    def error(self, message: str, **extra):
        """Log error message."""
        self._log_with_extra(logging.ERROR, message, **extra)
    
    def critical(self, message: str, **extra):
        """Log critical message."""
        self._log_with_extra(logging.CRITICAL, message, **extra)
    
    def exception(self, message: str, **extra):
        """Log exception with traceback."""
        self._log_with_extra(logging.ERROR, message, **extra)
        # Let the logging framework handle the exception info
        self.logger.exception("")
    
    @contextmanager
    def context(
        self,
        operation: str,
        component: Optional[str] = None,
        context_id: Optional[Union[str, UUID]] = None,
        **kwargs
    ):
        """Create a logging context for grouping related operations."""
        context_data = {
            'context_id': str(context_id or uuid4()),
            'operation': operation,
            'component': component or 'unknown',
            **kwargs
        }
        
        self.context_filter.push_context(context_data)
        start_time = time.time()
        
        try:
            self.info(f"Starting operation: {operation}")
            yield context_data
        except Exception as e:
            self.exception(f"Operation failed: {operation}", error=str(e))
            raise
        finally:
            end_time = time.time()
            duration = end_time - start_time
            self.info(
                f"Completed operation: {operation}",
                duration_seconds=duration
            )
            self.context_filter.pop_context()


class LogContext:
    """Context manager for logging operations."""
    
    def __init__(
        self,
        operation: str,
        component: Optional[str] = None,
        logger: Optional[TrainingDataBotLogger] = None,
        **kwargs
    ):
        self.operation = operation
        self.component = component
        self.logger = logger or get_logger()
        self.kwargs = kwargs
        self.context_data = None
    
    def __enter__(self):
        """Enter the logging context."""
        self.context_manager = self.logger.context(
            self.operation,
            self.component,
            **self.kwargs
        )
        self.context_data = self.context_manager.__enter__()
        return self.context_data
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the logging context."""
        return self.context_manager.__exit__(exc_type, exc_val, exc_tb)


class PerformanceLogger:
    """Logger specifically for performance metrics."""
    
    def __init__(self, logger: TrainingDataBotLogger):
        self.logger = logger
    
    def log_processing_stats(
        self,
        operation: str,
        total_items: int,
        processed_items: int,
        failed_items: int,
        duration: float,
        **extra
    ):
        """Log processing performance statistics."""
        success_rate = (processed_items / total_items * 100) if total_items > 0 else 0
        items_per_second = processed_items / duration if duration > 0 else 0
        
        self.logger.info(
            f"Processing complete: {operation}",
            total_items=total_items,
            processed_items=processed_items,
            failed_items=failed_items,
            success_rate_percent=round(success_rate, 2),
            duration_seconds=round(duration, 2),
            items_per_second=round(items_per_second, 2),
            **extra
        )
    
    def log_api_call(
        self,
        provider: str,
        model: str,
        tokens_used: int,
        response_time: float,
        success: bool,
        **extra
    ):
        """Log AI API call metrics."""
        self.logger.info(
            f"AI API call: {provider}/{model}",
            provider=provider,
            model=model,
            tokens_used=tokens_used,
            response_time_seconds=round(response_time, 3),
            success=success,
            **extra
        )
    
    def log_document_processing(
        self,
        document_id: str,
        document_type: str,
        word_count: int,
        chunks_created: int,
        processing_time: float,
        **extra
    ):
        """Log document processing metrics."""
        words_per_second = word_count / processing_time if processing_time > 0 else 0
        
        self.logger.info(
            f"Document processed: {document_type}",
            document_id=document_id,
            document_type=document_type,
            word_count=word_count,
            chunks_created=chunks_created,
            processing_time_seconds=round(processing_time, 2),
            words_per_second=round(words_per_second, 2),
            **extra
        )


# Global logger instance
_global_logger: Optional[TrainingDataBotLogger] = None


def setup_logging(
    level: str = "INFO",
    log_file: Optional[str] = None,
    structured: bool = True,
    **kwargs
) -> TrainingDataBotLogger:
    """
    Setup global logging configuration.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Optional file path for log output
        structured: Whether to use structured JSON logging
        **kwargs: Additional logging configuration
    
    Returns:
        Configured logger instance
    """
    global _global_logger
    _global_logger = TrainingDataBotLogger(
        name="training_data_bot",
        level=level,
        log_file=log_file,
        structured=structured,
        **kwargs
    )
    return _global_logger


def get_logger(name: Optional[str] = None) -> TrainingDataBotLogger:
    """
    Get a logger instance.
    
    Args:
        name: Optional logger name, uses global logger if not specified
    
    Returns:
        Logger instance
    """
    if name and name != "training_data_bot":
        # Create a new logger with the specified name
        return TrainingDataBotLogger(
            name=name,
            level="INFO",
            structured=True
        )
    
    # Return global logger or create default if not initialized
    if _global_logger is None:
        return setup_logging()
    
    return _global_logger


def get_performance_logger() -> PerformanceLogger:
    """Get a performance logger instance."""
    return PerformanceLogger(get_logger())


# Convenience function for quick logging setup from settings
def setup_logging_from_settings(settings):
    """Setup logging from settings configuration."""
    from .config import Settings
    
    if isinstance(settings, Settings):
        return setup_logging(
            level=settings.log_level.value,
            log_file=settings.log_file,
            structured=settings.enable_structured_logging,
            max_bytes=settings.log_max_bytes,
            backup_count=settings.log_backup_count
        )
    else:
        raise ValueError("Invalid settings object provided")


================================================================================
FILE: training_data_bot\core\models.py
================================================================================

"""
Core data models for the Training Data Bot.

This module defines all the data strucutures used throughout the system,
including documents, tasks, training examples, and datasets.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Union
from uuid import UUID, uuid4
import pathlib
from pydantic import BaseModel, Field, field_validator

class BaseEntity(BaseModel):
    """Base class for all entities with common fields."""
    id: UUID = Field(default_factory=uuid4)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = None 
    metadata: Dict[str, Any] = Field(default_factory=dict)

    class Config:
        """Pydantic configuration.""" 
        json_encoders = {
            UUID: str,
            datetime: lambda v: v.isoformat(),
        }

# Enums for categorization
class DocumentType(str, Enum):
    """Supported document types."""
    PDF = "pdf"
    DOCX = "docx"
    TXT = "txt"
    MD = "md"
    HTML = "html"
    JSON = "json"
    CSV = "csv"
    URL = "url"

class TaskType(str, Enum):
    """Types of tasks that can be performed."""
    QA_GENERATION = "qa_generation"
    CLASSIFICATION = "classification"
    SUMMARIZATION = "summarization"
    NER = "named_entity_recognition"
    RED_TEAMING = "red_teaming"
    INSTRUCTION_RESPONSE = "instruction_response"

class QualityMetric(str, Enum):
    """Quality assessment metrics"""
    TOXICITY = "toxicity"
    BIAS = "bias"
    DIVERSITY = "diversity"
    COHERENCE = "coherence"
    RELEVANCE = "relevance"
    ACCURACY = "accuracy"
    COMPLETENESS = "completeness"


class ProcessingStatus(str, Enum):
    """Status of processing jobs."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ExportFormat(str, Enum):
    """Supported export formats."""
    JSONL = "jsonl"
    JSON = "json"
    CSV = "csv"
    PARQUET = "parquet"


# Document family - Input data
class Document(BaseEntity):
    """A source document that contains content to be processed."""
    title: str
    content: str
    source: str
    doc_type: DocumentType
    word_count: int = 0
    char_count: int = 0
    language: Optional[str] = None
    encoding: Optional[str] = None
    
    @field_validator('word_count')
    @classmethod
    def calculate_word_count(cls, v, info):
        if v == 0 and info.data.get('content'):
            return len(info.data['content'].split())
        return v

    @field_validator('char_count') 
    @classmethod
    def calculate_char_count(cls, v, info):
        if v == 0 and info.data.get('content'):
            return len(info.data['content'])
        return v


class TextChunk(BaseEntity):
    """A chunk of text from a document for processing."""
    document_id: UUID
    content: str
    start_index: int
    end_index: int
    chunk_index: int
    token_count: int = 0
    overlap_tokens: int = 0


# Task family - Work instructions
class TaskTemplate(BaseEntity):
    """Template for generating tasks."""
    name: str
    task_type: TaskType
    description: str
    prompt_template: str
    parameters: Dict[str, Any] = Field(default_factory=dict)
    version: str = "1.0"
    enabled: bool = True


class TaskResult(BaseEntity):
    """Result of executing a task on a text chunk."""
    task_id: UUID
    input_chunk_id: UUID
    output: str
    confidence: float = Field(ge=0.0, le=1.0)
    quality_scores: Dict[str, float] = Field(default_factory=dict)
    processing_time: float = 0.0
    model_used: Optional[str] = None
    tokens_used: Optional[int] = None


# Training family - Final products
class TrainingExample(BaseEntity):
    """A single training example for machine learning."""
    input_text: str
    output_text: str
    task_type: TaskType
    source_document_id: UUID
    source_chunk_id: Optional[UUID] = None
    quality_scores: Dict[str, float] = Field(default_factory=dict)
    difficulty: Optional[str] = None
    tags: List[str] = Field(default_factory=list)


class Dataset(BaseEntity):
    """A collection of training examples."""
    name: str
    description: str
    examples: List[Any] = Field(default_factory=list)
    total_examples: int = 0
    train_split: float = Field(default=0.8, ge=0.0, le=1.0)
    validation_split: float = Field(default=0.1, ge=0.0, le=1.0)
    test_split: float = Field(default=0.1, ge=0.0, le=1.0)
    task_distribution: Dict[TaskType, int] = Field(default_factory=dict)
    version: str = "1.0"
    

# Quality family - Inspectors
class QualityReport(BaseEntity):
    """Quality assessment report for data."""
    target_id: UUID
    target_type: str  # "document", "example", "dataset"
    overall_score: float = Field(ge=0.0, le=1.0)
    passed: bool = False
    metric_scores: Dict[QualityMetric, float] = Field(default_factory=dict)
    issues: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    evaluation_model: Optional[str] = None


# Operations family - Factory managers
class ProcessingJob(BaseEntity):
    """A processing job that tracks work progress."""
    name: str
    job_type: str
    status: ProcessingStatus = ProcessingStatus.PENDING
    total_items: int = 0
    processed_items: int = 0
    failed_items: int = 0
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    estimated_completion: Optional[datetime] = None
    error_message: Optional[str] = None
    progress_percentage: float = Field(default=0.0, ge=0.0, le=100.0)
    
    def update_progress(self):
        """Update progress percentage based on processed items."""
        if self.total_items > 0:
            self.progress_percentage = (self.processed_items / self.total_items) * 100.0
        self.updated_at = datetime.utcnow()


# Configuration and settings models
class AIProviderConfig(BaseModel):
    """Configuration for AI providers."""
    provider_name: str
    api_key: Optional[str] = None
    api_url: Optional[str] = None
    model_name: str
    max_tokens: int = 4000
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    timeout: float = 30.0
    retry_attempts: int = 3
    rate_limit_rpm: Optional[int] = None


class ProcessingConfig(BaseModel):
    """Configuration for document processing."""
    chunk_size: int = Field(default=1000, gt=0)
    chunk_overlap: int = Field(default=200, ge=0)
    max_workers: int = Field(default=4, gt=0)
    batch_size: int = Field(default=10, gt=0)
    quality_threshold: float = Field(default=0.7, ge=0.0, le=1.0)
    enable_parallel_processing: bool = True
    max_file_size_mb: int = Field(default=100, gt=0)


# Type aliases for complex types
DocumentSource = Union[str, "pathlib.Path"]
TaskParameters = Dict[str, Any]
QualityScores = Dict[str, float]
MetricScores = Dict[QualityMetric, float]



================================================================================
FILE: training_data_bot\diagnostic.py
================================================================================

# diagnostic.py
import sys
from pathlib import Path

current = Path(__file__).resolve().parent
print(f"Current directory: {current}")
print(f"Parent directory: {current.parent}")

# Check if __init__.py exists
init_file = current / "__init__.py"
print(f"__init__.py exists: {init_file.exists()}")

# Add parent to path
sys.path.insert(0, str(current.parent))

# Try import
try:
    import training_data_bot
    print("âœ“ Import successful!")
except ImportError as e:
    print(f"âœ— Import failed: {e}")


================================================================================
FILE: training_data_bot\evaluation\__init__.py
================================================================================

"""
Evaluation module for quality assessment.

This module provides quality evaluation tools for training data.
"""

from training_data_bot.evaluation.evaluator import QualityEvaluator

__all__ = [
    "QualityEvaluator",
]


================================================================================
FILE: training_data_bot\evaluation\evaluator.py
================================================================================

"""
Quality evaluator for training data.

This module provides comprehensive quality assessment for training examples
and datasets using multiple metrics.
"""

from typing import Dict, List, Optional, Any
from uuid import uuid4

from training_data_bot.core import (
    TrainingExample,
    Dataset,
    QualityReport,
    QualityMetric,
    get_logger,
    LogContext,
)


class QualityEvaluator:
    """
    Quality evaluator for training data.
    
    Assesses the quality of training examples and datasets using
    multiple metrics including relevance, coherence, diversity, etc.
    """
    
    def __init__(
        self,
        quality_threshold: float = 0.7,
        **kwargs
    ):
        """
        Initialize the quality evaluator.
        
        Args:
            quality_threshold: Minimum quality score to pass (0.0-1.0)
            **kwargs: Additional configuration parameters
        """
        self.logger = get_logger("evaluation.QualityEvaluator")
        self.quality_threshold = quality_threshold
        self.config = kwargs
        
        # Metric weights for overall score calculation
        self.metric_weights = {
            QualityMetric.RELEVANCE: 0.25,
            QualityMetric.COHERENCE: 0.20,
            QualityMetric.COMPLETENESS: 0.20,
            QualityMetric.DIVERSITY: 0.15,
            QualityMetric.ACCURACY: 0.10,
            QualityMetric.TOXICITY: 0.05,  # Negative metric (lower is better)
            QualityMetric.BIAS: 0.05,      # Negative metric (lower is better)
        }
        
        self.logger.info(
            "QualityEvaluator initialized",
            threshold=quality_threshold
        )
    
    def evaluate_example(
        self,
        example: TrainingExample,
        detailed: bool = True
    ) -> QualityReport:
        """
        Evaluate a single training example.
        
        Args:
            example: Training example to evaluate
            detailed: Whether to include detailed analysis
            
        Returns:
            QualityReport with evaluation results
        """
        with LogContext("evaluate_example", example_id=str(example.id)):
            self.logger.debug(f"Evaluating example {example.id}")
            
            # Calculate individual metrics
            metric_scores = {}
            
            # Relevance: How well output relates to input
            metric_scores[QualityMetric.RELEVANCE] = self._assess_relevance(
                example.input_text,
                example.output_text
            )
            
            # Coherence: Internal consistency and logical flow
            metric_scores[QualityMetric.COHERENCE] = self._assess_coherence(
                example.output_text
            )
            
            # Completeness: Output is substantial and complete
            metric_scores[QualityMetric.COMPLETENESS] = self._assess_completeness(
                example.output_text
            )
            
            # Accuracy: Based on existing quality scores
            metric_scores[QualityMetric.ACCURACY] = self._assess_accuracy(
                example.quality_scores
            )
            
            # Toxicity: Check for harmful content (lower is better)
            metric_scores[QualityMetric.TOXICITY] = self._assess_toxicity(
                example.output_text
            )
            
            # Bias: Check for biased language (lower is better)
            metric_scores[QualityMetric.BIAS] = self._assess_bias(
                example.output_text
            )
            
            # Calculate overall score
            overall_score = self._calculate_overall_score(metric_scores)
            
            # Determine if passed
            passed = overall_score >= self.quality_threshold
            
            # Generate issues and recommendations
            issues = []
            warnings = []
            recommendations = []
            
            if detailed:
                issues, warnings, recommendations = self._generate_feedback(
                    metric_scores,
                    example
                )
            
            # Create quality report
            report = QualityReport(
                id=uuid4(),
                target_id=example.id,
                target_type="example",
                overall_score=overall_score,
                passed=passed,
                metric_scores=metric_scores,
                issues=issues,
                warnings=warnings,
                recommendations=recommendations
            )
            
            self.logger.debug(
                f"Example evaluation complete",
                overall_score=overall_score,
                passed=passed
            )
            
            return report
    
    def evaluate_dataset(
        self,
        dataset: Dataset,
        detailed_report: bool = True
    ) -> QualityReport:
        """
        Evaluate an entire dataset.
        
        Args:
            dataset: Dataset to evaluate
            detailed_report: Whether to include detailed analysis
            
        Returns:
            QualityReport with dataset evaluation results
        """
        with LogContext("evaluate_dataset", dataset_id=str(dataset.id)):
            self.logger.info(
                f"Evaluating dataset with {len(dataset.examples)} examples"
            )
            
            if not dataset.examples:
                return QualityReport(
                    id=uuid4(),
                    target_id=dataset.id,
                    target_type="dataset",
                    overall_score=0.0,
                    passed=False,
                    metric_scores={},
                    issues=["Dataset is empty"],
                    warnings=[],
                    recommendations=["Add training examples to the dataset"]
                )
            
            # Evaluate individual examples
            example_reports = []
            for example in dataset.examples:
                report = self.evaluate_example(example, detailed=False)
                example_reports.append(report)
            
            # Aggregate metrics
            metric_scores = {}
            for metric in QualityMetric:
                scores = [
                    r.metric_scores.get(metric, 0.0)
                    for r in example_reports
                    if metric in r.metric_scores
                ]
                if scores:
                    metric_scores[metric] = sum(scores) / len(scores)
            
            # Dataset-specific metrics
            metric_scores[QualityMetric.DIVERSITY] = self._assess_dataset_diversity(
                dataset
            )
            
            # Calculate overall score
            overall_score = self._calculate_overall_score(metric_scores)
            
            # Determine if passed
            passed = overall_score >= self.quality_threshold
            
            # Generate feedback
            issues = []
            warnings = []
            recommendations = []
            
            if detailed_report:
                issues, warnings, recommendations = self._generate_dataset_feedback(
                    metric_scores,
                    dataset,
                    example_reports
                )
            
            # Create quality report
            report = QualityReport(
                id=uuid4(),
                target_id=dataset.id,
                target_type="dataset",
                overall_score=overall_score,
                passed=passed,
                metric_scores=metric_scores,
                issues=issues,
                warnings=warnings,
                recommendations=recommendations
            )
            
            self.logger.info(
                f"Dataset evaluation complete",
                overall_score=overall_score,
                passed=passed,
                examples_evaluated=len(example_reports)
            )
            
            return report
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Assess how relevant output is to input."""
        # Simple word overlap heuristic
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if not input_words:
            return 0.0
        
        overlap = len(input_words & output_words)
        relevance = min(overlap / len(input_words), 1.0)
        
        return relevance
    
    def _assess_coherence(self, text: str) -> float:
        """Assess internal coherence of text."""
        # Simple heuristic based on sentence structure
        sentences = text.split('.')
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return 0.0
        
        # Check for reasonable sentence lengths
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        # Ideal sentence length: 10-25 words
        if 10 <= avg_sentence_length <= 25:
            coherence = 1.0
        elif 5 <= avg_sentence_length <= 35:
            coherence = 0.8
        else:
            coherence = 0.6
        
        return coherence
    
    def _assess_completeness(self, text: str) -> float:
        """Assess if text is complete and substantial."""
        word_count = len(text.split())
        
        if word_count < 10:
            return 0.3
        elif word_count < 20:
            return 0.6
        elif word_count < 50:
            return 0.8
        else:
            return 1.0
    
    def _assess_accuracy(self, quality_scores: Dict[str, float]) -> float:
        """Assess accuracy based on existing quality scores."""
        if not quality_scores:
            return 0.5  # Neutral if no scores
        
        # Average existing quality scores
        return sum(quality_scores.values()) / len(quality_scores)
    
    def _assess_toxicity(self, text: str) -> float:
        """Assess toxicity (simple keyword-based check)."""
        # Simple heuristic - in production, use proper toxicity detection
        toxic_keywords = [
            'hate', 'kill', 'stupid', 'idiot', 'damn', 'hell',
            'offensive', 'attack', 'violent'
        ]
        
        text_lower = text.lower()
        toxic_count = sum(1 for word in toxic_keywords if word in text_lower)
        
        # Lower toxicity score is better (inverted)
        if toxic_count == 0:
            return 0.0  # No toxicity (best)
        elif toxic_count == 1:
            return 0.3
        elif toxic_count == 2:
            return 0.6
        else:
            return 1.0  # High toxicity (worst)
    
    def _assess_bias(self, text: str) -> float:
        """Assess potential bias (simple keyword-based check)."""
        # Simple heuristic - in production, use proper bias detection
        biased_patterns = [
            'always', 'never', 'all', 'none', 'everyone', 'no one',
            'must', 'obviously', 'clearly'
        ]
        
        text_lower = text.lower()
        bias_count = sum(1 for pattern in biased_patterns if pattern in text_lower)
        
        # Lower bias score is better (inverted)
        if bias_count == 0:
            return 0.0  # No bias (best)
        elif bias_count <= 2:
            return 0.3
        elif bias_count <= 4:
            return 0.6
        else:
            return 0.9  # High bias (worst)
    
    def _assess_dataset_diversity(self, dataset: Dataset) -> float:
        """Assess diversity of examples in dataset."""
        if len(dataset.examples) < 2:
            return 0.5
        
        # Check diversity based on unique words
        all_words = set()
        example_word_sets = []
        
        for example in dataset.examples:
            words = set(example.output_text.lower().split())
            example_word_sets.append(words)
            all_words.update(words)
        
        # Calculate average uniqueness
        unique_ratios = []
        for words in example_word_sets:
            if all_words:
                ratio = len(words) / len(all_words)
                unique_ratios.append(ratio)
        
        diversity = sum(unique_ratios) / len(unique_ratios) if unique_ratios else 0.5
        return diversity
    
    def _calculate_overall_score(self, metric_scores: Dict[QualityMetric, float]) -> float:
        """Calculate weighted overall quality score."""
        weighted_sum = 0.0
        total_weight = 0.0
        
        for metric, weight in self.metric_weights.items():
            if metric in metric_scores:
                score = metric_scores[metric]
                
                # Invert negative metrics (toxicity, bias)
                if metric in [QualityMetric.TOXICITY, QualityMetric.BIAS]:
                    score = 1.0 - score
                
                weighted_sum += score * weight
                total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        return weighted_sum / total_weight
    
    def _generate_feedback(
        self,
        metric_scores: Dict[QualityMetric, float],
        example: TrainingExample
    ) -> tuple:
        """Generate issues, warnings, and recommendations."""
        issues = []
        warnings = []
        recommendations = []
        
        # Check each metric
        for metric, score in metric_scores.items():
            if metric == QualityMetric.RELEVANCE and score < 0.5:
                issues.append(f"Low relevance score ({score:.2f})")
                recommendations.append("Ensure output directly addresses the input")
            
            if metric == QualityMetric.COHERENCE and score < 0.6:
                warnings.append(f"Coherence could be improved ({score:.2f})")
                recommendations.append("Review text for logical flow and structure")
            
            if metric == QualityMetric.COMPLETENESS and score < 0.5:
                issues.append(f"Output may be incomplete ({score:.2f})")
                recommendations.append("Provide more substantial and complete responses")
            
            if metric == QualityMetric.TOXICITY and score > 0.5:
                issues.append(f"Potential toxic content detected ({score:.2f})")
                recommendations.append("Review and remove any harmful language")
            
            if metric == QualityMetric.BIAS and score > 0.5:
                warnings.append(f"Potential biased language detected ({score:.2f})")
                recommendations.append("Use more balanced and objective language")
        
        return issues, warnings, recommendations
    
    def _generate_dataset_feedback(
        self,
        metric_scores: Dict[QualityMetric, float],
        dataset: Dataset,
        example_reports: List[QualityReport]
    ) -> tuple:
        """Generate dataset-level feedback."""
        issues = []
        warnings = []
        recommendations = []
        
        # Check dataset size
        if len(dataset.examples) < 10:
            warnings.append(f"Small dataset size ({len(dataset.examples)} examples)")
            recommendations.append("Consider generating more training examples")
        
        # Check failure rate
        failed_count = sum(1 for r in example_reports if not r.passed)
        failure_rate = failed_count / len(example_reports) if example_reports else 0
        
        if failure_rate > 0.3:
            issues.append(f"High failure rate ({failure_rate:.1%})")
            recommendations.append("Review and improve example quality")
        elif failure_rate > 0.1:
            warnings.append(f"Moderate failure rate ({failure_rate:.1%})")
        
        # Check diversity
        if QualityMetric.DIVERSITY in metric_scores:
            diversity = metric_scores[QualityMetric.DIVERSITY]
            if diversity < 0.3:
                issues.append(f"Low diversity score ({diversity:.2f})")
                recommendations.append("Generate more varied training examples")
        
        return issues, warnings, recommendations


================================================================================
FILE: training_data_bot\example_session7_usage.py
================================================================================

"""
Example usage of Session 7: Task Generation System

Demonstrates how to use the task generators to create training data.
"""

import asyncio
import sys
from pathlib import Path
from uuid import uuid4

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Core imports
from training_data_bot.core import Document, TextChunk, TaskType, DocumentType

# AI imports
from training_data_bot.ai import AIClient

# Task imports
from training_data_bot.tasks import (
    TaskManager,
    QAGenerator,
    ClassificationGenerator,
    SummarizationGenerator,
)


async def example_1_basic_qa_generation():
    """Example 1: Basic Q&A generation."""
    print("\n" + "="*60)
    print("EXAMPLE 1: Basic Q&A Generation")
    print("="*60)
    
    # Setup (in production, use real API keys)
    # ai_client = AIClient(provider="openai", api_key="your-key")
    
    # For demo, we'll show the structure
    print("\n# Step 1: Initialize AI Client")
    print("ai_client = AIClient(provider='openai', api_key='your-key')")
    
    print("\n# Step 2: Create Q&A Generator")
    print("qa_gen = QAGenerator(ai_client=ai_client, num_questions=5)")
    
    print("\n# Step 3: Prepare Text Chunk")
    print("""chunk = TextChunk(
    id=uuid4(),
    document_id=uuid4(),
    content="Machine learning is transforming...",
    start_index=0,
    end_index=200,
    chunk_index=0,
    token_count=50
)""")
    
    print("\n# Step 4: Generate Q&A")
    print("result = await qa_gen.generate_single(chunk)")
    
    print("\n# Step 5: Create Training Example")
    print("example = qa_gen.create_training_example(result, chunk)")
    
    print("\nâœ“ Result: Training example ready for model fine-tuning!")


async def example_2_multiple_task_types():
    """Example 2: Generate multiple task types."""
    print("\n" + "="*60)
    print("EXAMPLE 2: Multiple Task Types")
    print("="*60)
    
    print("\n# Step 1: Initialize Task Manager")
    print("ai_client = AIClient(provider='anthropic', api_key='your-key')")
    print("manager = TaskManager(ai_client=ai_client)")
    
    print("\n# Step 2: Prepare Multiple Chunks")
    print("""chunks = [
    TextChunk(...),  # Chapter 1
    TextChunk(...),  # Chapter 2
    TextChunk(...),  # Chapter 3
]""")
    
    print("\n# Step 3: Define Task Types")
    print("""task_types = [
    TaskType.QA_GENERATION,
    TaskType.CLASSIFICATION,
    TaskType.SUMMARIZATION
]""")
    
    print("\n# Step 4: Generate All Tasks")
    print("results = await manager.generate_tasks(chunks, task_types)")
    
    print("\n# Step 5: Create Training Examples")
    print("examples = await manager.create_training_examples(chunks, task_types)")
    
    print(f"\nâœ“ Result: {len([1,2,3]) * 3} training examples created!")
    print("  - 3 Q&A pairs")
    print("  - 3 Classifications")
    print("  - 3 Summaries")


async def example_3_custom_categories():
    """Example 3: Custom classification categories."""
    print("\n" + "="*60)
    print("EXAMPLE 3: Custom Classification Categories")
    print("="*60)
    
    print("\n# Step 1: Define Custom Categories")
    print("""categories = [
    "technical_documentation",
    "marketing_content",
    "customer_support",
    "product_description",
    "blog_post"
]""")
    
    print("\n# Step 2: Create Classifier")
    print("classifier = ClassificationGenerator(")
    print("    ai_client=ai_client,")
    print("    categories=categories,")
    print("    include_reasoning=True")
    print(")")
    
    print("\n# Step 3: Classify Text")
    print("result = await classifier.generate_single(chunk)")
    
    print("\n# Example Output:")
    print("""Category: technical_documentation
Reasoning: The text contains technical terminology, code examples, 
and follows a structured documentation format with clear sections 
and API references.""")
    
    print("\nâœ“ Perfect for training domain-specific classifiers!")


async def example_4_batch_processing():
    """Example 4: Batch processing with progress."""
    print("\n" + "="*60)
    print("EXAMPLE 4: Batch Processing")
    print("="*60)
    
    print("\n# Step 1: Load Large Document")
    print("""from training_data_bot.sources import UnifiedLoader
from training_data_bot.preprocessing import TextPreprocessor

loader = UnifiedLoader()
preprocessor = TextPreprocessor(chunk_size=1000, chunk_overlap=200)

# Load and chunk document
document = await loader.load_single('large_textbook.pdf')
chunks = preprocessor.process_document(document)

print(f"Created {len(chunks)} chunks from document")""")
    
    print("\n# Step 2: Setup Batch Processing")
    print("""manager = TaskManager(ai_client=ai_client, max_concurrent=10)

# Create processing job
job = ProcessingJob(
    id=uuid4(),
    name="Textbook Q&A Generation",
    job_type="qa_generation"
)""")
    
    print("\n# Step 3: Process with Progress Tracking")
    print("""examples = await manager.process_job(
    job=job,
    chunks=chunks,
    task_types=[TaskType.QA_GENERATION]
)

print(f"Progress: {job.progress_percentage}%")
print(f"Created {len(examples)} examples")""")
    
    print("\nâœ“ Efficient batch processing with progress monitoring!")


async def example_5_custom_templates():
    """Example 5: Using custom task templates."""
    print("\n" + "="*60)
    print("EXAMPLE 5: Custom Task Templates")
    print("="*60)
    
    print("\n# Step 1: Create Custom Template")
    print("""from core import TaskTemplate

custom_template = TaskTemplate(
    id=uuid4(),
    name="Advanced Q&A with Difficulty Levels",
    task_type=TaskType.QA_GENERATION,
    description="Generate Q&A pairs with difficulty ratings",
    prompt_template=\"\"\"
Read this text and generate 5 questions with answers.
Rate each question's difficulty (Easy/Medium/Hard).

Text: {text}

Format:
Q: [question]
Difficulty: [Easy/Medium/Hard]
A: [answer]
\"\"\",
    parameters={
        "num_questions": 5,
        "temperature": 0.8,
        "max_tokens": 1500
    }
)""")
    
    print("\n# Step 2: Use Custom Template")
    print("""result = await qa_gen.generate_single(
    chunk=chunk,
    template=custom_template
)""")
    
    print("\nâœ“ Full control over generation format and style!")


async def example_6_quality_filtering():
    """Example 6: Quality filtering and assessment."""
    print("\n" + "="*60)
    print("EXAMPLE 6: Quality Filtering")
    print("="*60)
    
    print("\n# Step 1: Generate with Quality Scores")
    print("""results = await manager.generate_tasks(chunks, task_types)

# Each result has quality scores
for result in results:
    print(f"Confidence: {result.confidence}")
    print(f"Quality Scores: {result.quality_scores}")""")
    
    print("\n# Step 2: Filter by Quality")
    print("""quality_threshold = 0.7

high_quality_results = [
    r for r in results 
    if r.confidence >= quality_threshold
]

print(f"High quality: {len(high_quality_results)}/{len(results)}")""")
    
    print("\n# Step 3: Create Examples from Best Results")
    print("""examples = [
    generator.create_training_example(result, chunk)
    for result, chunk in zip(high_quality_results, chunks)
]""")
    
    print("\nâœ“ Ensure only high-quality data for training!")


async def example_7_statistics_monitoring():
    """Example 7: Monitor generation statistics."""
    print("\n" + "="*60)
    print("EXAMPLE 7: Statistics Monitoring")
    print("="*60)
    
    print("\n# Step 1: Generate Training Data")
    print("""manager = TaskManager(ai_client=ai_client)

# Generate lots of examples
examples = await manager.create_training_examples(
    chunks=all_chunks,
    task_types=[
        TaskType.QA_GENERATION,
        TaskType.CLASSIFICATION,
        TaskType.SUMMARIZATION
    ]
)""")
    
    print("\n# Step 2: Check Statistics")
    print("""stats = manager.get_statistics()

for task_type, metrics in stats.items():
    print(f"{task_type}:")
    print(f"  Generated: {metrics['total_generated']}")
    print(f"  Failed: {metrics['total_failed']}")
    print(f"  Success Rate: {metrics['success_rate']:.1%}")
    print(f"  Tokens Used: {metrics['total_tokens_used']}")
    print(f"  Avg Time: {metrics['avg_time_per_task']:.2f}s")
    print(f"  Total Cost: ${metrics['estimated_cost']:.2f}")""")
    
    print("\nâœ“ Track performance and costs!")


async def example_8_real_world_pipeline():
    """Example 8: Complete real-world pipeline."""
    print("\n" + "="*60)
    print("EXAMPLE 8: Complete Training Data Pipeline")
    print("="*60)
    
    print("\n# Complete Pipeline:")
    print("""
from training_data_bot.sources import UnifiedLoader
from training_data_bot.preprocessing import TextPreprocessor
from training_data_bot.tasks import TaskManager
from training_data_bot.storage import DatasetExporter

async def create_training_dataset(source_files):
    # Step 1: Load documents
    loader = UnifiedLoader()
    documents = await loader.load_multiple(source_files)
    print(f"Loaded {len(documents)} documents")
    
    # Step 2: Preprocess into chunks
    preprocessor = TextPreprocessor(chunk_size=800, chunk_overlap=150)
    all_chunks = []
    for doc in documents:
        chunks = preprocessor.process_document(doc)
        all_chunks.extend(chunks)
    print(f"Created {len(all_chunks)} chunks")
    
    # Step 3: Generate training data
    manager = TaskManager(ai_client=ai_client)
    examples = await manager.create_training_examples(
        chunks=all_chunks,
        task_types=[
            TaskType.QA_GENERATION,
            TaskType.SUMMARIZATION
        ]
    )
    print(f"Generated {len(examples)} training examples")
    
    # Step 4: Filter by quality
    quality_threshold = 0.75
    high_quality = [
        ex for ex in examples
        if all(score >= quality_threshold 
               for score in ex.quality_scores.values())
    ]
    print(f"High quality examples: {len(high_quality)}")
    
    # Step 5: Export dataset
    exporter = DatasetExporter()
    dataset_path = await exporter.export(
        examples=high_quality,
        output_path="training_data.jsonl",
        format="jsonl"
    )
    print(f"Dataset exported to: {dataset_path}")
    
    return dataset_path

# Run the pipeline
dataset = await create_training_dataset([
    "textbook_chapter1.pdf",
    "textbook_chapter2.pdf",
    "documentation.md"
])
""")
    
    print("\nâœ“ Complete automated training data generation!")


async def main():
    """Run all examples."""
    print("\n" + "="*60)
    print("SESSION 7 EXAMPLES: TASK GENERATION SYSTEM")
    print("="*60)
    print("\nThese examples demonstrate the task generation capabilities.")
    print("In production, replace mock clients with real AI API keys.\n")
    
    examples = [
        example_1_basic_qa_generation,
        example_2_multiple_task_types,
        example_3_custom_categories,
        example_4_batch_processing,
        example_5_custom_templates,
        example_6_quality_filtering,
        example_7_statistics_monitoring,
        example_8_real_world_pipeline,
    ]
    
    for example in examples:
        await example()
        await asyncio.sleep(0.1)  # Small delay for readability
    
    print("\n" + "="*60)
    print("ðŸŽ‰ ALL EXAMPLES COMPLETE!")
    print("="*60)
    print("\nNext Steps:")
    print("1. Set up AI API keys (OpenAI or Anthropic)")
    print("2. Load your documents")
    print("3. Generate training data")
    print("4. Export to JSONL for model fine-tuning")
    print("\nReady for Session 8: Evaluation & Storage!")


if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: training_data_bot\example_usage.py
================================================================================

"""
Example usage of the UnifiedLoader.

This script demonstrates various ways to use the unified loader
for loading different types of documents.
"""


import asyncio
import sys
from pathlib import Path

# ADD this instead:
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from training_data_bot.sources import UnifiedLoader
from training_data_bot.core import setup_logging


async def example_1_single_file():
    """Example 1: Load a single file."""
    print("\n" + "="*60)
    print("Example 1: Loading a Single File")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Load a single text file (type is automatically detected)
    doc = await loader.load_single("test_documents/sample.txt")
    
    print(f"Document Title: {doc.title}")
    print(f"Document Type: {doc.doc_type.value}")
    print(f"Word Count: {doc.word_count}")
    print(f"Content Preview: {doc.content[:100]}...")


async def example_2_multiple_files():
    """Example 2: Load multiple files at once."""
    print("\n" + "="*60)
    print("Example 2: Loading Multiple Files")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Load multiple files of different types
    sources = [
        "test_documents/sample.txt",
        "test_documents/sample.json",
        "test_documents/sample.csv",
    ]
    
    documents = await loader.load_multiple(sources, max_workers=4)
    
    print(f"Loaded {len(documents)} documents:")
    for doc in documents:
        print(f"  - {doc.title} ({doc.doc_type.value}): {doc.word_count} words")


async def example_3_directory():
    """Example 3: Load all files from a directory."""
    print("\n" + "="*60)
    print("Example 3: Loading Entire Directory")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Load all supported files from a directory
    documents = await loader.load_directory("test_documents")
    
    print(f"Loaded {len(documents)} documents from directory")
    
    # Group by type
    by_type = {}
    for doc in documents:
        doc_type = doc.doc_type.value
        by_type[doc_type] = by_type.get(doc_type, 0) + 1
    
    print("\nDocuments by type:")
    for doc_type, count in sorted(by_type.items()):
        print(f"  - {doc_type}: {count}")


async def example_4_web_content():
    """Example 4: Load content from a web URL."""
    print("\n" + "="*60)
    print("Example 4: Loading Web Content")
    print("="*60)
    
    loader = UnifiedLoader()
    
    try:
        # Load content from a URL (automatically detected as web content)
        doc = await loader.load_single("https://example.com")
        
        print(f"Web Page Title: {doc.title}")
        print(f"Word Count: {doc.word_count}")
        print(f"Content Preview: {doc.content[:150]}...")
    except Exception as e:
        print(f"Could not load web content: {e}")


async def example_5_mixed_sources():
    """Example 5: Load from mixed sources (files + URLs)."""
    print("\n" + "="*60)
    print("Example 5: Loading Mixed Sources")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Mix of local files and URLs
    sources = [
        "test_documents/sample.txt",
        "test_documents/sample.json",
        "https://example.com",
    ]
    
    documents = await loader.load_multiple(sources, max_workers=2, skip_errors=True)
    
    print(f"Loaded {len(documents)} documents from mixed sources:")
    for doc in documents:
        source_type = "Web" if doc.doc_type.value == "url" else "Local"
        print(f"  - {doc.title} ({source_type}, {doc.doc_type.value})")


async def example_6_grouped_by_type():
    """Example 6: Load and group documents by type."""
    print("\n" + "="*60)
    print("Example 6: Loading with Grouping by Type")
    print("="*60)
    
    loader = UnifiedLoader()
    
    sources = [
        "test_documents/sample.txt",
        "test_documents/sample.md",
        "test_documents/sample.json",
        "test_documents/sample.csv",
    ]
    
    # Load and automatically group by document type
    grouped = await loader.load_mixed_batch(
        sources,
        max_workers=4,
        group_by_type=True
    )
    
    print("Documents grouped by type:")
    for doc_type, docs in grouped.items():
        print(f"\n{doc_type.value.upper()} ({len(docs)} documents):")
        for doc in docs:
            print(f"  - {doc.title}: {doc.word_count} words")


async def example_7_error_handling():
    """Example 7: Error handling with skip_errors."""
    print("\n" + "="*60)
    print("Example 7: Error Handling")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Some sources that might fail
    sources = [
        "test_documents/sample.txt",  # Valid
        "nonexistent_file.txt",        # Will fail
        "test_documents/sample.json",  # Valid
        "another_missing.pdf",         # Will fail
    ]
    
    # With skip_errors=True, failed loads won't crash the whole batch
    documents = await loader.load_multiple(sources, skip_errors=True)
    
    print(f"Attempted to load {len(sources)} sources")
    print(f"Successfully loaded {len(documents)} documents")
    print(f"Failed: {len(sources) - len(documents)}")


async def example_8_getting_info():
    """Example 8: Getting loader information."""
    print("\n" + "="*60)
    print("Example 8: Loader Information")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Get information about the loader
    print(f"Loader: {loader}")
    print(f"\nSupported formats: {', '.join(loader.get_supported_formats())}")
    
    print("\nRegistered specialized loaders:")
    loader_info = loader.get_loader_info()
    for loader_name, formats in loader_info.items():
        print(f"  - {loader_name}: {', '.join(formats)}")


async def run_all_examples():
    """Run all examples."""
    # Setup logging
    setup_logging(level="WARNING", structured=False)  # Reduce log noise
    
    print("\n" + "="*80)
    print(" "*25 + "UNIFIED LOADER EXAMPLES")
    print("="*80)
    
    try:
        await example_1_single_file()
        await example_2_multiple_files()
        await example_3_directory()
        await example_4_web_content()
        await example_5_mixed_sources()
        await example_6_grouped_by_type()
        await example_7_error_handling()
        await example_8_getting_info()
        
        print("\n" + "="*80)
        print(" "*28 + "EXAMPLES COMPLETE")
        print("="*80)
        
    except Exception as e:
        print(f"\nExample failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(run_all_examples())


================================================================================
FILE: training_data_bot\preprocessing\__init__.py
================================================================================

"""
Text preprocessing module. 

This module provides text preprocessing and hunking functionality for preparing documents for AI processing. 

"""

from training_data_bot.preprocessing.processor import TextPreprocessor

__all__ = [
    "TextProcessor",
]


================================================================================
FILE: training_data_bot\preprocessing\processor.py
================================================================================

"""
Text preprocessing and chunking for document processing. 

This module handles splitting documents into manageable chunks for AI processing, with configurable size, overlap and metadata 
chunking
"""


import re
from typing import List, Optional
from uuid import uuid4

from training_data_bot.core import (
    Document,
    TextChunk,
    ProcessingConfig,
    ProcessingError,
    get_logger,
    LogContext,
)


class TextPreprocessor:
    """
    Text preprocessor for chunking documents.
    
    Splits documents into overlapping chunks suitable for AI processing
    while preserving context and tracking metadata.
    """
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        min_chunk_size: Optional[int] = None,
    ):
        """
        Initialize the text preprocessor.
        
        Args:
            chunk_size: Target number of tokens per chunk
            chunk_overlap: Number of tokens to overlap between chunks
            min_chunk_size: Minimum chunk size to keep
        """
        self.logger = get_logger("preprocessor.TextPreprocessor")
        
        # Validate parameters
        if chunk_overlap >= chunk_size:
            raise ProcessingError(
                "Chunk overlap must be less than chunk size",
                details={
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap
                }
            )
        
        # Set default min_chunk_size if not provided
        if min_chunk_size is None:
            min_chunk_size = min(100, chunk_size // 10)
        
        if min_chunk_size > chunk_size:
            raise ProcessingError(
                "Minimum chunk size cannot exceed chunk size",
                details={
                    "chunk_size": chunk_size,
                    "min_chunk_size": min_chunk_size
                }
            )
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        
        self.logger.info(
            "TextPreprocessor initialized",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_chunk_size=min_chunk_size
        )
    
    @classmethod
    def from_config(cls, config: ProcessingConfig) -> "TextPreprocessor":
        """
        Create preprocessor from configuration.
        
        Args:
            config: Processing configuration object
            
        Returns:
            Configured TextPreprocessor instance
        """
        return cls(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            min_chunk_size=getattr(config, 'min_chunk_size', 100)
        )
    
    def process_document(self, document: Document) -> List[TextChunk]:
        """
        Process a document into chunks.
        
        Args:
            document: Document to process
            
        Returns:
            List of TextChunk objects
        """
        with LogContext("process_document", component="TextPreprocessor"):
            self.logger.info(
                f"Processing document: {document.title}",
                document_id=str(document.id),
                content_length=len(document.content),
                word_count=document.word_count
            )
            
            # Clean the text
            cleaned_text = self._clean_text(document.content)
            
            # Split into chunks
            chunks = self._create_chunks(document, cleaned_text)
            
            self.logger.info(
                f"Created {len(chunks)} chunks",
                document_id=str(document.id),
                chunk_count=len(chunks),
                avg_chunk_size=sum(c.token_count for c in chunks) / len(chunks) if chunks else 0
            )
            
            return chunks
    
    def process_documents(self, documents: List[Document]) -> List[TextChunk]:
        """
        Process multiple documents into chunks.
        
        Args:
            documents: List of documents to process
            
        Returns:
            List of all TextChunk objects from all documents
        """
        all_chunks = []
        
        for document in documents:
            chunks = self.process_document(document)
            all_chunks.extend(chunks)
        
        self.logger.info(
            f"Processed {len(documents)} documents into {len(all_chunks)} chunks"
        )
        
        return all_chunks
    
    def _clean_text(self, text: str) -> str:
        """
        Clean and normalize text.
        
        Args:
            text: Raw text to clean
            
        Returns:
            Cleaned text
        """
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove leading/trailing whitespace
        text = text.strip()
        
        # Normalize quotes
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(''', "'").replace(''', "'")
        
        # Remove control characters except newlines
        text = ''.join(char for char in text if char.isprintable() or char == '\n')
        
        return text
    
    def _create_chunks(self, document: Document, text: str) -> List[TextChunk]:
        """
        Create overlapping chunks from text.
        
        Args:
            document: Source document
            text: Cleaned text to chunk
            
        Returns:
            List of TextChunk objects
        """
        # Split text into words (simple tokenization)
        words = text.split()
        
        if not words:
            self.logger.warning(
                f"Document has no words after cleaning: {document.title}"
            )
            return []
        
        chunks = []
        chunk_index = 0
        start_word_idx = 0
        
        while start_word_idx < len(words):
            # Calculate end index for this chunk
            end_word_idx = min(start_word_idx + self.chunk_size, len(words))
            
            # Extract chunk words
            chunk_words = words[start_word_idx:end_word_idx]
            chunk_text = ' '.join(chunk_words)
            
            # Calculate character positions in original text
            # This is approximate since we've normalized the text
            chars_before = len(' '.join(words[:start_word_idx]))
            start_char = chars_before + (1 if start_word_idx > 0 else 0)
            end_char = start_char + len(chunk_text)
            
            # Only keep chunks that meet minimum size
            if len(chunk_words) >= self.min_chunk_size or end_word_idx >= len(words):
                chunk = TextChunk(
                    id=uuid4(),
                    document_id=document.id,
                    content=chunk_text,
                    start_index=start_char,
                    end_index=end_char,
                    chunk_index=chunk_index,
                    token_count=len(chunk_words),
                    overlap_tokens=self.chunk_overlap if chunk_index > 0 else 0
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # Move to next chunk with overlap
            if end_word_idx >= len(words):
                break
            
            start_word_idx = end_word_idx - self.chunk_overlap
            
            # Prevent infinite loop
            if start_word_idx <= 0:
                start_word_idx = end_word_idx
        
        return chunks
    
    def get_chunk_context(
        self,
        chunks: List[TextChunk],
        chunk_index: int,
        context_chunks: int = 1
    ) -> str:
        """
        Get context around a specific chunk.
        
        Args:
            chunks: List of chunks from same document
            chunk_index: Index of target chunk
            context_chunks: Number of chunks before/after to include
            
        Returns:
            Combined text with context
        """
        if not chunks or chunk_index < 0 or chunk_index >= len(chunks):
            return ""
        
        start_idx = max(0, chunk_index - context_chunks)
        end_idx = min(len(chunks), chunk_index + context_chunks + 1)
        
        context_texts = [chunks[i].content for i in range(start_idx, end_idx)]
        return ' '.join(context_texts)
    
    def merge_small_chunks(
        self,
        chunks: List[TextChunk],
        merge_threshold: Optional[int] = None
    ) -> List[TextChunk]:
        """
        Merge chunks that are too small.
        
        Args:
            chunks: List of chunks to process
            merge_threshold: Size threshold for merging (default: min_chunk_size)
            
        Returns:
            List of chunks with small ones merged
        """
        if not chunks:
            return []
        
        threshold = merge_threshold or self.min_chunk_size
        merged_chunks = []
        current_merge = None
        
        for chunk in chunks:
            if chunk.token_count < threshold:
                # This chunk is too small
                if current_merge is None:
                    current_merge = chunk
                else:
                    # Merge with previous small chunk
                    current_merge = TextChunk(
                        id=uuid4(),
                        document_id=chunk.document_id,
                        content=current_merge.content + ' ' + chunk.content,
                        start_index=current_merge.start_index,
                        end_index=chunk.end_index,
                        chunk_index=current_merge.chunk_index,
                        token_count=current_merge.token_count + chunk.token_count,
                        overlap_tokens=0
                    )
            else:
                # This chunk is large enough
                if current_merge is not None:
                    # Save the merged chunk
                    merged_chunks.append(current_merge)
                    current_merge = None
                merged_chunks.append(chunk)
        
        # Don't forget the last merge if any
        if current_merge is not None:
            merged_chunks.append(current_merge)
        
        return merged_chunks
    
    def split_by_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences.
        
        Args:
            text: Text to split
            
        Returns:
            List of sentences
        """
        # Simple sentence splitting (can be enhanced with NLTK)
        sentence_endings = r'[.!?]+\s+'
        sentences = re.split(sentence_endings, text)
        
        # Clean and filter empty sentences
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def create_sentence_chunks(
        self,
        document: Document,
        sentences_per_chunk: int = 5
    ) -> List[TextChunk]:
        """
        Create chunks based on sentences rather than word count.
        
        Args:
            document: Document to process
            sentences_per_chunk: Number of sentences per chunk
            
        Returns:
            List of sentence-based chunks
        """
        cleaned_text = self._clean_text(document.content)
        sentences = self.split_by_sentences(cleaned_text)
        
        if not sentences:
            return []
        
        chunks = []
        chunk_index = 0
        
        for i in range(0, len(sentences), sentences_per_chunk):
            chunk_sentences = sentences[i:i + sentences_per_chunk]
            chunk_text = ' '.join(chunk_sentences)
            word_count = len(chunk_text.split())
            
            chunk = TextChunk(
                id=uuid4(),
                document_id=document.id,
                content=chunk_text,
                start_index=0,  # Approximate
                end_index=len(chunk_text),
                chunk_index=chunk_index,
                token_count=word_count,
                overlap_tokens=0
            )
            chunks.append(chunk)
            chunk_index += 1
        
        return chunks
    
    def get_statistics(self, chunks: List[TextChunk]) -> dict:
        """
        Get statistics about chunks.
        
        Args:
            chunks: List of chunks to analyze
            
        Returns:
            Dictionary with statistics
        """
        if not chunks:
            return {
                "total_chunks": 0,
                "total_tokens": 0,
                "avg_tokens_per_chunk": 0,
                "min_tokens": 0,
                "max_tokens": 0,
            }
        
        token_counts = [chunk.token_count for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_tokens": sum(token_counts),
            "avg_tokens_per_chunk": sum(token_counts) / len(token_counts),
            "min_tokens": min(token_counts),
            "max_tokens": max(token_counts),
            "documents_processed": len(set(chunk.document_id for chunk in chunks))
        }
    
    def __repr__(self) -> str:
        """String representation of the preprocessor."""
        return (
            f"TextPreprocessor(chunk_size={self.chunk_size}, "
            f"overlap={self.chunk_overlap}, "
            f"min_size={self.min_chunk_size})"
        )


================================================================================
FILE: training_data_bot\production_bot.py
================================================================================

"""
Production Bot Configuration Loader
Loads production configuration from YAML file and creates production-ready bot instance.
"""

import yaml
import os
from pathlib import Path
from typing import Dict, Any
from dotenv import load_dotenv

from training_data_bot import TrainingDataBot

def load_production_config(config_path: str = "config/production.yaml") -> Dict[str, Any]:
    """
    Load production configuration from YAML file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Configuration dictionary
        
    Raises:
        FileNotFoundError: If config file doesn't exist
        yaml.YAMLError: If config file is invalid
    """
    config_file = Path(config_path)
    
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    return config

def create_production_bot(config_path: str = "config/production.yaml") -> TrainingDataBot:
    """
    Create production-ready bot with configuration.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        Configured TrainingDataBot instance
        
    Raises:
        ValueError: If required environment variables are missing
    """
    # Load environment variables
    load_dotenv()
    
    # Load configuration
    config = load_production_config(config_path)
    
    # Extract bot configuration
    bot_config = {
        "chunk_size": config['processing']['chunk_size'],
        "chunk_overlap": config['processing']['chunk_overlap'],
        "quality_threshold": config['quality']['threshold'],
        "max_workers": config['processing']['max_workers'],
    }
    
    # Create bot instance
    bot = TrainingDataBot(config=bot_config)
    
    # Get AI provider settings
    ai_provider = config['ai_provider']['default']
    
    # Get API key from environment
    if ai_provider == "openai":
        api_key = os.getenv('TDB_OPENAI_API_KEY')
        if not api_key:
            raise ValueError("TDB_OPENAI_API_KEY environment variable not set!")
        
        model = config['ai_provider']['openai']['model']
        temperature = config['ai_provider']['openai']['temperature']
        max_tokens = config['ai_provider']['openai']['max_tokens']
        
    elif ai_provider == "anthropic":
        api_key = os.getenv('TDB_ANTHROPIC_API_KEY')
        if not api_key:
            raise ValueError("TDB_ANTHROPIC_API_KEY environment variable not set!")
        
        model = config['ai_provider']['anthropic']['model']
        temperature = config['ai_provider']['anthropic']['temperature']
        max_tokens = config['ai_provider']['anthropic']['max_tokens']
    else:
        raise ValueError(f"Unknown AI provider: {ai_provider}")
    
    # Set AI client
    bot.set_ai_client(
        provider=ai_provider,
        api_key=api_key,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    return bot

def validate_production_config(config_path: str = "config/production.yaml") -> bool:
    """
    Validate production configuration file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        True if configuration is valid
        
    Raises:
        ValueError: If configuration is invalid
    """
    config = load_production_config(config_path)
    
    # Required sections
    required_sections = [
        'application', 'ai_provider', 'processing', 
        'quality', 'logging', 'storage'
    ]
    
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required section: {section}")
    
    # Validate processing config
    processing = config['processing']
    if processing['chunk_overlap'] >= processing['chunk_size']:
        raise ValueError("chunk_overlap must be less than chunk_size")
    
    # Validate quality config
    quality = config['quality']
    if not 0.0 <= quality['threshold'] <= 1.0:
        raise ValueError("quality threshold must be between 0.0 and 1.0")
    
    # Validate AI provider
    default_provider = config['ai_provider']['default']
    if default_provider not in config['ai_provider']:
        raise ValueError(f"Default provider '{default_provider}' not configured")
    
    return True

def print_production_config(config_path: str = "config/production.yaml"):
    """
    Print production configuration in a readable format.
    
    Args:
        config_path: Path to configuration file
    """
    config = load_production_config(config_path)
    
    print("\n" + "="*60)
    print("PRODUCTION CONFIGURATION")
    print("="*60)
    
    print(f"\nApplication:")
    print(f"  Name: {config['application']['name']}")
    print(f"  Version: {config['application']['version']}")
    print(f"  Environment: {config['application']['environment']}")
    
    print(f"\nAI Provider:")
    print(f"  Default: {config['ai_provider']['default']}")
    print(f"  Model: {config['ai_provider'][config['ai_provider']['default']]['model']}")
    
    print(f"\nProcessing:")
    print(f"  Chunk Size: {config['processing']['chunk_size']}")
    print(f"  Chunk Overlap: {config['processing']['chunk_overlap']}")
    print(f"  Max Workers: {config['processing']['max_workers']}")
    print(f"  Max Concurrent: {config['processing']['max_concurrent']}")
    
    print(f"\nQuality:")
    print(f"  Threshold: {config['quality']['threshold']}")
    print(f"  Filtering: {config['quality']['enable_filtering']}")
    
    print(f"\nLogging:")
    print(f"  Level: {config['logging']['level']}")
    print(f"  File: {config['logging']['file']}")
    
    print(f"\nStorage:")
    print(f"  Output Dir: {config['storage']['output_dir']}")
    print(f"  Default Format: {config['storage']['default_format']}")
    
    print("="*60 + "\n")

if __name__ == "__main__":
    """Test the production configuration."""
    import asyncio
    
    async def test_production_bot():
        print("Testing Production Bot Configuration...")
        
        # Validate config
        try:
            validate_production_config()
            print("âœ“ Configuration is valid")
        except Exception as e:
            print(f"âŒ Configuration error: {e}")
            return
        
        # Print config
        print_production_config()
        
        # Create bot
        try:
            bot = create_production_bot()
            print("âœ“ Production bot created successfully")
            
            # Get bot info
            info = bot.get_provider_info() if hasattr(bot, 'get_provider_info') else {}
            if info:
                print(f"âœ“ AI Provider: {info.get('provider', 'unknown')}")
                print(f"âœ“ Model: {info.get('model', 'unknown')}")
            
            # Cleanup
            await bot.cleanup()
            print("âœ“ Bot cleanup completed")
            
        except ValueError as e:
            print(f"âŒ Bot creation failed: {e}")
            print("\nMake sure to set the required environment variables:")
            print("  - TDB_OPENAI_API_KEY (for OpenAI)")
            print("  - TDB_ANTHROPIC_API_KEY (for Anthropic)")
        except Exception as e:
            print(f"âŒ Unexpected error: {e}")
    
    asyncio.run(test_production_bot())  


================================================================================
FILE: training_data_bot\sources\__init__.py
================================================================================

"""
Document loading sources module.

This module provides loaders for various document types including
text files, PDFs, web content, and more.
"""

from training_data_bot.sources.base import BaseLoader
from training_data_bot.sources.document_loader import DocumentLoader
from training_data_bot.sources.pdf_loader import PDFLoader
from training_data_bot.sources.web_loader import WebLoader
from training_data_bot.sources.unified import UnifiedLoader

__all__ = [
    "BaseLoader",
    "DocumentLoader",
    "PDFLoader",
    "WebLoader",
    "UnifiedLoader",
]


================================================================================
FILE: training_data_bot\sources\base.py
================================================================================

"""
Base loader class for document loading. 

This module provides the abstract base class that all document loaders must inherit from, ensuring consistent interface behavior. 
"""

import asyncio
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Optional, Union
from uuid import uuid4

from training_data_bot.core import (
    Document,
    DocumentType,
    DocumentLoadError,
    get_logger,
    LogContext,
)


class BaseLoader(ABC):
    """
    Abstract base class for all document loaders.
    
    All loaders must implement the load_single method and define
    their supported formats.
    """
    
    def __init__(self):
        """Initialize the base loader."""
        self.logger = get_logger(f"loader.{self.__class__.__name__}")
        self.supported_formats: List[DocumentType] = []
    
    @abstractmethod
    async def load_single(
        self,
        source: Union[str, Path],
        **kwargs
    ) -> Document:
        """
        Load a single document from a source.
        
        Args:
            source: Path or URL to the document
            **kwargs: Additional loader-specific parameters
            
        Returns:
            Document object with loaded content
            
        Raises:
            DocumentLoadError: If loading fails
        """
        pass
    
    async def load_multiple(
        self,
        sources: List[Union[str, Path]],
        max_workers: int = 4,
        **kwargs
    ) -> List[Document]:
        """
        Load multiple documents in parallel.
        
        Args:
            sources: List of paths or URLs to documents
            max_workers: Maximum number of parallel loading operations
            **kwargs: Additional loader-specific parameters
            
        Returns:
            List of successfully loaded Document objects
        """
        with LogContext("load_multiple", component=self.__class__.__name__):
            self.logger.info(
                f"Loading {len(sources)} documents",
                total_sources=len(sources),
                max_workers=max_workers
            )
            
            # Create semaphore to limit concurrent operations
            semaphore = asyncio.Semaphore(max_workers)
            
            async def load_with_semaphore(source):
                """Load a single document with semaphore control."""
                async with semaphore:
                    try:
                        return await self.load_single(source, **kwargs)
                    except Exception as e:
                        self.logger.error(
                            f"Failed to load document: {source}",
                            source=str(source),
                            error=str(e)
                        )
                        return None
            
            # Create tasks for all sources
            tasks = [load_with_semaphore(source) for source in sources]
            
            # Execute all tasks and gather results
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out failed loads
            documents = []
            failed_count = 0
            
            for result in results:
                if isinstance(result, Document):
                    documents.append(result)
                elif isinstance(result, Exception):
                    failed_count += 1
                    self.logger.warning(f"Document load failed: {result}")
                elif result is None:
                    failed_count += 1
            
            self.logger.info(
                "Batch loading complete",
                total_sources=len(sources),
                successful_loads=len(documents),
                failed_loads=failed_count
            )
            
            return documents
    
    def get_document_type(self, source: Union[str, Path]) -> DocumentType:
        """
        Determine document type from source.
        
        Args:
            source: Path or URL to the document
            
        Returns:
            DocumentType enum value
            
        Raises:
            DocumentLoadError: If type cannot be determined
        """
        # Check if it's a URL
        if isinstance(source, str) and source.startswith(('http://', 'https://')):
            return DocumentType.URL
        
        # Convert to Path and get extension
        source_path = Path(source)
        suffix = source_path.suffix.lower().lstrip('.')
        
        # Try to match to DocumentType
        try:
            return DocumentType(suffix)
        except ValueError:
            raise DocumentLoadError(
                f"Unsupported file type: {suffix}",
                file_path=str(source),
                file_type=suffix
            )
    
    def validate_source(self, source: Union[str, Path]) -> bool:
        """
        Validate that a source can be loaded.
        
        Args:
            source: Path or URL to validate
            
        Returns:
            True if source is valid and can be loaded
        """
        # Check if it's a URL
        if isinstance(source, str) and source.startswith(('http://', 'https://')):
            return DocumentType.URL in self.supported_formats
        
        # Check file existence
        source_path = Path(source)
        if not source_path.exists():
            return False
        
        # Check if file type is supported
        try:
            doc_type = self.get_document_type(source)
            return doc_type in self.supported_formats
        except DocumentLoadError:
            return False
    
    def create_document(
        self,
        title: str,
        content: str,
        source: Union[str, Path],
        doc_type: DocumentType,
        **kwargs
    ) -> Document:
        """
        Create a Document object with standard fields.
        
        Args:
            title: Document title
            content: Document text content
            source: Source path or URL
            doc_type: Type of document
            **kwargs: Additional document metadata
            
        Returns:
            Document object
        """
        # Calculate word and character counts
        word_count = len(content.split())
        char_count = len(content)
        
        # Create document
        document = Document(
            id=uuid4(),
            title=title,
            content=content,
            source=str(source),
            doc_type=doc_type,
            word_count=word_count,
            char_count=char_count,
            **kwargs
        )
        
        self.logger.debug(
            f"Created document: {title}",
            document_id=str(document.id),
            doc_type=doc_type.value,
            word_count=word_count,
            char_count=char_count
        )
        
        return document
    
    async def load_directory(
        self,
        directory: Union[str, Path],
        recursive: bool = True,
        max_workers: int = 4,
        **kwargs
    ) -> List[Document]:
        """
        Load all supported documents from a directory.
        
        Args:
            directory: Path to directory
            recursive: Whether to search subdirectories
            max_workers: Maximum parallel loading operations
            **kwargs: Additional loader-specific parameters
            
        Returns:
            List of loaded Document objects
        """
        directory_path = Path(directory)
        
        if not directory_path.exists() or not directory_path.is_dir():
            raise DocumentLoadError(
                f"Directory not found or not a directory: {directory}",
                file_path=str(directory)
            )
        
        with LogContext("load_directory", component=self.__class__.__name__):
            self.logger.info(
                f"Scanning directory: {directory}",
                directory=str(directory),
                recursive=recursive
            )
            
            # Find all supported files
            sources = self._find_supported_files(directory_path, recursive)
            
            self.logger.info(
                f"Found {len(sources)} supported files",
                file_count=len(sources)
            )
            
            # Load all files
            return await self.load_multiple(sources, max_workers, **kwargs)
    
    def _find_supported_files(
        self,
        directory: Path,
        recursive: bool
    ) -> List[Path]:
        """
        Find all supported files in a directory.
        
        Args:
            directory: Directory to search
            recursive: Whether to search subdirectories
            
        Returns:
            List of file paths
        """
        files = []
        
        # Create patterns for supported formats
        patterns = [f"*.{fmt.value}" for fmt in self.supported_formats]
        
        for pattern in patterns:
            if recursive:
                files.extend(directory.rglob(pattern))
            else:
                files.extend(directory.glob(pattern))
        
        # Sort files for consistent ordering
        return sorted(set(files))
    
    def __repr__(self) -> str:
        """String representation of the loader."""
        formats = ", ".join(fmt.value for fmt in self.supported_formats)
        return f"{self.__class__.__name__}(formats=[{formats}])"


================================================================================
FILE: training_data_bot\sources\document_loader.py
================================================================================

"""
Document loader for text-based formats.

Handles TXT, MD, HTML, JSON, CSV, and DOCX files with appropriate
text extraction for each format.
"""

import asyncio
import csv
import json
from pathlib import Path
from typing import Optional, Union

from training_data_bot.core import (
    Document,
    DocumentType,
    DocumentLoadError,
    LogContext,
)
from training_data_bot.sources.base import BaseLoader


class DocumentLoader(BaseLoader):
    """
    Loader for various text-based document formats.
    
    Supports: TXT, MD, HTML, JSON, CSV, DOCX
    """
    
    def __init__(self):
        """Initialize the document loader."""
        super().__init__()
        self.supported_formats = [
            DocumentType.TXT,
            DocumentType.MD,
            DocumentType.HTML,
            DocumentType.JSON,
            DocumentType.CSV,
            DocumentType.DOCX,
        ]
    
    async def load_single(
        self,
        source: Union[str, Path],
        encoding: str = "utf-8",
        **kwargs
    ) -> Document:
        """
        Load a single text-based document.
        
        Args:
            source: Path to the document file
            encoding: Text encoding (default: utf-8)
            **kwargs: Additional parameters
            
        Returns:
            Document object with loaded content
            
        Raises:
            DocumentLoadError: If loading fails
        """
        source_path = Path(source)
        
        with LogContext("load_single", component="DocumentLoader"):
            self.logger.info(f"Loading document: {source_path.name}")
            
            # Validate source
            if not source_path.exists():
                raise DocumentLoadError(
                    f"File not found: {source}",
                    file_path=str(source)
                )
            
            # Determine document type
            doc_type = self.get_document_type(source)
            
            # Load content based on type
            try:
                if doc_type == DocumentType.TXT:
                    content = await self._load_text(source_path, encoding)
                elif doc_type == DocumentType.MD:
                    content = await self._load_markdown(source_path, encoding)
                elif doc_type == DocumentType.HTML:
                    content = await self._load_html(source_path, encoding)
                elif doc_type == DocumentType.JSON:
                    content = await self._load_json(source_path, encoding)
                elif doc_type == DocumentType.CSV:
                    content = await self._load_csv(source_path, encoding)
                elif doc_type == DocumentType.DOCX:
                    content = await self._load_docx(source_path)
                else:
                    raise DocumentLoadError(
                        f"Unsupported document type: {doc_type}",
                        file_type=doc_type.value
                    )
                
                # Create document object
                document = self.create_document(
                    title=source_path.stem,
                    content=content,
                    source=source_path,
                    doc_type=doc_type,
                    encoding=encoding,
                    file_size=source_path.stat().st_size,
                )
                
                self.logger.info(
                    f"Successfully loaded: {source_path.name}",
                    word_count=document.word_count,
                    char_count=document.char_count
                )
                
                return document
                
            except Exception as e:
                if isinstance(e, DocumentLoadError):
                    raise
                raise DocumentLoadError(
                    f"Failed to load document: {source_path.name}",
                    file_path=str(source),
                    file_type=doc_type.value,
                    cause=e
                )
    
    async def _load_text(self, path: Path, encoding: str) -> str:
        """
        Load plain text file.
        
        Args:
            path: File path
            encoding: Text encoding
            
        Returns:
            File content as string
        """
        def _read():
            return path.read_text(encoding=encoding)
        
        return await asyncio.to_thread(_read)
    
    async def _load_markdown(self, path: Path, encoding: str) -> str:
        """
        Load Markdown file.
        
        Args:
            path: File path
            encoding: Text encoding
            
        Returns:
            Markdown content as string (preserves formatting)
        """
        # For now, treat as plain text
        # Could add Markdown parsing in the future
        return await self._load_text(path, encoding)
    
    async def _load_html(self, path: Path, encoding: str) -> str:
        """
        Load HTML file and extract text content.
        
        Args:
            path: File path
            encoding: Text encoding
            
        Returns:
            Extracted text content
        """
        def _extract():
            try:
                from bs4 import BeautifulSoup
                
                with open(path, 'r', encoding=encoding) as f:
                    html_content = f.read()
                
                # Parse HTML
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Remove script and style elements
                for script in soup(['script', 'style']):
                    script.decompose()
                
                # Get text
                text = soup.get_text()
                
                # Clean up whitespace
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text
                
            except ImportError:
                self.logger.warning(
                    "BeautifulSoup not installed, reading HTML as plain text"
                )
                return path.read_text(encoding=encoding)
        
        return await asyncio.to_thread(_extract)
    
    async def _load_json(self, path: Path, encoding: str) -> str:
        """
        Load JSON file and convert to text representation.
        
        Args:
            path: File path
            encoding: Text encoding
            
        Returns:
            JSON content as formatted text
        """
        def _extract():
            with open(path, 'r', encoding=encoding) as f:
                data = json.load(f)
            
            # Convert JSON to readable text format
            if isinstance(data, dict):
                lines = [f"{key}: {value}" for key, value in data.items()]
                return "\n".join(lines)
            elif isinstance(data, list):
                lines = [f"Item {i+1}: {item}" for i, item in enumerate(data)]
                return "\n".join(lines)
            else:
                return str(data)
        
        return await asyncio.to_thread(_extract)
    
    async def _load_csv(self, path: Path, encoding: str) -> str:
        """
        Load CSV file and convert to text representation.
        
        Args:
            path: File path
            encoding: Text encoding
            
        Returns:
            CSV content as formatted text
        """
        def _extract():
            lines = []
            
            with open(path, 'r', encoding=encoding, newline='') as f:
                reader = csv.reader(f)
                
                # Get headers
                headers = next(reader, None)
                if headers:
                    # Strip whitespace from headers
                    headers = [h.strip() for h in headers]
                    lines.append("Headers: " + ", ".join(headers))
                    lines.append("")
                
                # Process rows
                for row_num, row in enumerate(reader, 1):
                    if headers and len(row) == len(headers):
                        # Create key-value pairs
                        row_data = [
                            f"{header}: {value.strip()}"
                            for header, value in zip(headers, row)
                        ]
                        lines.append(f"Row {row_num}: {' | '.join(row_data)}")
                    else:
                        # Just list values if no headers or mismatch
                        lines.append(f"Row {row_num}: {', '.join(row)}")
                    
                    # Limit number of rows to prevent huge files
                    if row_num > 1000:
                        lines.append("... (truncated, too many rows)")
                        break
            
            return "\n".join(lines)
        
        return await asyncio.to_thread(_extract)
    
    async def _load_docx(self, path: Path) -> str:
        """
        Load Microsoft Word document and extract text.
        
        Args:
            path: File path
            
        Returns:
            Extracted text content
            
        Raises:
            DocumentLoadError: If python-docx is not installed
        """
        def _extract():
            try:
                from docx import Document as DocxDocument
                
                doc = DocxDocument(path)
                
                # Extract text from all paragraphs
                text_parts = [
                    paragraph.text
                    for paragraph in doc.paragraphs
                    if paragraph.text.strip()
                ]
                
                return "\n".join(text_parts)
                
            except ImportError:
                raise DocumentLoadError(
                    "python-docx package required for DOCX files. "
                    "Install with: pip install python-docx",
                    file_path=str(path),
                    file_type="docx"
                )
        
        return await asyncio.to_thread(_extract)


================================================================================
FILE: training_data_bot\sources\pdf_loader.py
================================================================================

"""
PDF document loader.

Handles PDF file loading and text extraction using PyMuPDF (fitz).
"""

import asyncio
from pathlib import Path
from typing import Union

from training_data_bot.core import Document, DocumentType, DocumentLoadError, LogContext
from training_data_bot.sources.base import BaseLoader


class PDFLoader(BaseLoader):
    """
    Loader for PDF documents.
    
    Uses PyMuPDF (fitz) for robust PDF text extraction.
    """
    
    def __init__(self):
        """Initialize the PDF loader."""
        super().__init__()
        self.supported_formats = [DocumentType.PDF]
    
    async def load_single(
        self,
        source: Union[str, Path],
        **kwargs
    ) -> Document:
        """
        Load a single PDF document.
        
        Args:
            source: Path to the PDF file
            **kwargs: Additional parameters
            
        Returns:
            Document object with extracted text
            
        Raises:
            DocumentLoadError: If loading fails
        """
        source_path = Path(source)
        
        with LogContext("load_single", component="PDFLoader"):
            self.logger.info(f"Loading PDF: {source_path.name}")
            
            # Validate source
            if not source_path.exists():
                raise DocumentLoadError(
                    f"PDF file not found: {source}",
                    file_path=str(source),
                    file_type="pdf"
                )
            
            # Extract text from PDF
            try:
                content = await self._extract_pdf_text(source_path)
                
                if not content or not content.strip():
                    self.logger.warning(
                        f"No text extracted from PDF: {source_path.name}"
                    )
                    content = "[Empty PDF or no extractable text]"
                
                # Create document
                document = self.create_document(
                    title=source_path.stem,
                    content=content,
                    source=source_path,
                    doc_type=DocumentType.PDF,
                    extraction_method="PyMuPDF",
                    file_size=source_path.stat().st_size,
                )
                
                self.logger.info(
                    f"Successfully loaded PDF: {source_path.name}",
                    word_count=document.word_count,
                    char_count=document.char_count
                )
                
                return document
                
            except Exception as e:
                if isinstance(e, DocumentLoadError):
                    raise
                raise DocumentLoadError(
                    f"Failed to load PDF: {source_path.name}",
                    file_path=str(source),
                    file_type="pdf",
                    cause=e
                )
    
    async def _extract_pdf_text(self, path: Path) -> str:
        """
        Extract text from PDF file.
        
        Args:
            path: Path to PDF file
            
        Returns:
            Extracted text content
            
        Raises:
            DocumentLoadError: If PyMuPDF is not installed or extraction fails
        """
        def _extract():
            try:
                import fitz  # PyMuPDF
                
                text_parts = []
                
                # Open PDF
                doc = fitz.open(path)
                
                try:
                    # Extract text from each page
                    for page_num in range(doc.page_count):
                        page = doc[page_num]
                        text = page.get_text()
                        
                        if text.strip():
                            # Add page marker
                            text_parts.append(f"--- Page {page_num + 1} ---")
                            text_parts.append(text)
                    
                    return "\n\n".join(text_parts)
                    
                finally:
                    # Always close the document
                    doc.close()
                    
            except ImportError:
                raise DocumentLoadError(
                    "PyMuPDF package required for PDF files. "
                    "Install with: pip install PyMuPDF",
                    file_path=str(path),
                    file_type="pdf"
                )
            except Exception as e:
                raise DocumentLoadError(
                    f"Failed to extract text from PDF: {e}",
                    file_path=str(path),
                    file_type="pdf",
                    cause=e
                )
        
        return await asyncio.to_thread(_extract)


================================================================================
FILE: training_data_bot\sources\unified.py
================================================================================

"""
Unified document loader.

This module provides a single interface for loading any supported document
type by automatically detecting the format and routing to the appropriate
specialized loader.
"""

import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Union

from training_data_bot.core import (
    Document,
    DocumentType,
    DocumentLoadError,
    UnsupportedFormatError,
    get_logger,
    LogContext,
)
from training_data_bot.sources.base import BaseLoader
from training_data_bot.sources.document_loader import DocumentLoader
from training_data_bot.sources.pdf_loader import PDFLoader
from training_data_bot.sources.web_loader import WebLoader


class UnifiedLoader(BaseLoader):
    """
    Unified loader that automatically detects document types and routes
    to the appropriate specialized loader.
    
    This is the main entry point for document loading in the system.
    """
    
    def __init__(self):
        """Initialize the unified loader with all specialized loaders."""
        super().__init__()
        
        # Initialize specialized loaders
        self.document_loader = DocumentLoader()
        self.pdf_loader = PDFLoader()
        self.web_loader = WebLoader()
        
        # Build loader registry
        self._loader_registry: Dict[DocumentType, BaseLoader] = {}
        self._register_loaders()
        
        # All supported formats
        self.supported_formats = list(DocumentType)
        
        self.logger.info("UnifiedLoader initialized with all specialized loaders")
    
    def _register_loaders(self):
        """Register all specialized loaders for their supported formats."""
        # Register document loader for text-based formats
        for doc_type in self.document_loader.supported_formats:
            self._loader_registry[doc_type] = self.document_loader
        
        # Register PDF loader
        for doc_type in self.pdf_loader.supported_formats:
            self._loader_registry[doc_type] = self.pdf_loader
        
        # Register web loader
        for doc_type in self.web_loader.supported_formats:
            self._loader_registry[doc_type] = self.web_loader
        
        self.logger.debug(
            f"Registered loaders for {len(self._loader_registry)} document types"
        )
    
    def _get_loader_for_source(self, source: Union[str, Path]) -> BaseLoader:
        """
        Determine the appropriate loader for a source.
        
        Args:
            source: Path or URL to the document
            
        Returns:
            Specialized loader for the document type
            
        Raises:
            UnsupportedFormatError: If no loader supports the format
        """
        # Detect document type
        try:
            doc_type = self.get_document_type(source)
        except DocumentLoadError as e:
            raise UnsupportedFormatError(
                f"Cannot determine document type for: {source}",
                file_path=str(source),
                cause=e
            )
        
        # Get appropriate loader
        loader = self._loader_registry.get(doc_type)
        
        if loader is None:
            raise UnsupportedFormatError(
                f"No loader available for document type: {doc_type.value}",
                file_type=doc_type.value,
                file_path=str(source)
            )
        
        return loader
    
    async def load_single(
        self,
        source: Union[str, Path],
        **kwargs
    ) -> Document:
        """
        Load a single document, automatically detecting type and using
        the appropriate loader.
        
        Args:
            source: Path or URL to the document
            **kwargs: Additional parameters passed to specialized loader
            
        Returns:
            Document object with loaded content
            
        Raises:
            DocumentLoadError: If loading fails
            UnsupportedFormatError: If format is not supported
        """
        with LogContext("unified_load_single", component="UnifiedLoader"):
            self.logger.info(f"Loading document: {source}")
            
            # Get appropriate loader
            loader = self._get_loader_for_source(source)
            
            self.logger.debug(
                f"Routing to {loader.__class__.__name__} for {source}"
            )
            
            # Load using specialized loader
            try:
                document = await loader.load_single(source, **kwargs)
                
                self.logger.info(
                    f"Successfully loaded document: {document.title}",
                    document_id=str(document.id),
                    doc_type=document.doc_type.value,
                    word_count=document.word_count
                )
                
                return document
                
            except Exception as e:
                self.logger.error(
                    f"Failed to load document: {source}",
                    error=str(e)
                )
                raise
    
    async def load_multiple(
        self,
        sources: List[Union[str, Path]],
        max_workers: int = 4,
        skip_errors: bool = True,
        **kwargs
    ) -> List[Document]:
        """
        Load multiple documents with automatic type detection.
        
        Args:
            sources: List of paths or URLs to documents
            max_workers: Maximum number of parallel loading operations
            skip_errors: If True, continue loading other documents if one fails
            **kwargs: Additional parameters passed to specialized loaders
            
        Returns:
            List of successfully loaded Document objects
        """
        with LogContext("unified_load_multiple", component="UnifiedLoader"):
            self.logger.info(
                f"Loading {len(sources)} documents",
                total_sources=len(sources),
                max_workers=max_workers,
                skip_errors=skip_errors
            )
            
            # Create semaphore to limit concurrent operations
            semaphore = asyncio.Semaphore(max_workers)
            
            async def load_with_semaphore(source):
                """Load a single document with semaphore control."""
                async with semaphore:
                    try:
                        return await self.load_single(source, **kwargs)
                    except Exception as e:
                        if skip_errors:
                            self.logger.warning(
                                f"Skipping failed document: {source}",
                                source=str(source),
                                error=str(e)
                            )
                            return None
                        else:
                            raise
            
            # Create tasks for all sources
            tasks = [load_with_semaphore(source) for source in sources]
            
            # Execute all tasks
            results = await asyncio.gather(*tasks, return_exceptions=not skip_errors)
            
            # Filter successful loads
            documents = [doc for doc in results if isinstance(doc, Document)]
            failed_count = len(sources) - len(documents)
            
            self.logger.info(
                "Batch loading complete",
                total_sources=len(sources),
                successful_loads=len(documents),
                failed_loads=failed_count,
                success_rate=f"{(len(documents)/len(sources)*100):.1f}%"
            )
            
            return documents
    
    async def load_directory(
        self,
        directory: Union[str, Path],
        recursive: bool = True,
        max_workers: int = 4,
        file_patterns: Optional[List[str]] = None,
        **kwargs
    ) -> List[Document]:
        """
        Load all supported documents from a directory.
        
        Args:
            directory: Path to directory
            recursive: Whether to search subdirectories
            max_workers: Maximum parallel loading operations
            file_patterns: Optional list of glob patterns to filter files
            **kwargs: Additional parameters passed to specialized loaders
            
        Returns:
            List of loaded Document objects
        """
        directory_path = Path(directory)
        
        if not directory_path.exists() or not directory_path.is_dir():
            raise DocumentLoadError(
                f"Directory not found or not a directory: {directory}",
                file_path=str(directory)
            )
        
        with LogContext("unified_load_directory", component="UnifiedLoader"):
            self.logger.info(
                f"Scanning directory: {directory}",
                directory=str(directory),
                recursive=recursive
            )
            
            # Find all supported files
            sources = self._find_all_supported_files(
                directory_path,
                recursive,
                file_patterns
            )
            
            self.logger.info(
                f"Found {len(sources)} supported files",
                file_count=len(sources)
            )
            
            # Load all files
            return await self.load_multiple(sources, max_workers, **kwargs)
    
    def _find_all_supported_files(
        self,
        directory: Path,
        recursive: bool,
        file_patterns: Optional[List[str]] = None
    ) -> List[Path]:
        """
        Find all supported files in a directory.
        
        Args:
            directory: Directory to search
            recursive: Whether to search subdirectories
            file_patterns: Optional list of glob patterns
            
        Returns:
            List of file paths
        """
        files = []
        
        if file_patterns:
            # Use custom patterns
            patterns = file_patterns
        else:
            # Use patterns for all supported formats
            patterns = [f"*.{fmt.value}" for fmt in self.supported_formats if fmt != DocumentType.URL]
        
        for pattern in patterns:
            if recursive:
                files.extend(directory.rglob(pattern))
            else:
                files.extend(directory.glob(pattern))
        
        # Sort files for consistent ordering
        return sorted(set(files))
    
    async def load_from_urls(
        self,
        urls: List[str],
        max_workers: int = 4,
        **kwargs
    ) -> List[Document]:
        """
        Load content from multiple URLs.
        
        Args:
            urls: List of URLs to load
            max_workers: Maximum parallel loading operations
            **kwargs: Additional parameters passed to web loader
            
        Returns:
            List of loaded Document objects
        """
        with LogContext("load_from_urls", component="UnifiedLoader"):
            self.logger.info(f"Loading {len(urls)} URLs")
            return await self.load_multiple(urls, max_workers, **kwargs)
    
    def get_supported_formats(self) -> List[str]:
        """
        Get list of all supported document formats.
        
        Returns:
            List of format extensions (e.g., ['txt', 'pdf', 'html'])
        """
        return [fmt.value for fmt in self.supported_formats if fmt != DocumentType.URL]
    
    def get_loader_info(self) -> Dict[str, List[str]]:
        """
        Get information about registered loaders and their formats.
        
        Returns:
            Dictionary mapping loader names to their supported formats
        """
        info = {}
        
        for loader in [self.document_loader, self.pdf_loader, self.web_loader]:
            loader_name = loader.__class__.__name__
            formats = [fmt.value for fmt in loader.supported_formats]
            info[loader_name] = formats
        
        return info
    
    async def load_mixed_batch(
        self,
        sources: List[Union[str, Path]],
        max_workers: int = 4,
        group_by_type: bool = False,
        **kwargs
    ) -> Union[List[Document], Dict[DocumentType, List[Document]]]:
        """
        Load a mixed batch of different document types.
        
        Args:
            sources: List of paths and URLs
            max_workers: Maximum parallel loading operations
            group_by_type: If True, return documents grouped by type
            **kwargs: Additional parameters passed to loaders
            
        Returns:
            Either a flat list of documents or a dictionary grouped by type
        """
        documents = await self.load_multiple(sources, max_workers, **kwargs)
        
        if not group_by_type:
            return documents
        
        # Group documents by type
        grouped: Dict[DocumentType, List[Document]] = {}
        for doc in documents:
            if doc.doc_type not in grouped:
                grouped[doc.doc_type] = []
            grouped[doc.doc_type].append(doc)
        
        self.logger.info(
            "Documents grouped by type",
            type_counts={
                doc_type.value: len(docs)
                for doc_type, docs in grouped.items()
            }
        )
        
        return grouped
    
    def __repr__(self) -> str:
        """String representation of the unified loader."""
        return f"UnifiedLoader(formats={len(self.supported_formats)}, loaders={len(set(self._loader_registry.values()))})"


================================================================================
FILE: training_data_bot\sources\web_loader.py
================================================================================

"""
Web content loader.

Handles loading and extracting content from web pages using HTTP requests.
"""

from pathlib import Path
from typing import Union
from urllib.parse import urlparse

from training_data_bot.core import Document, DocumentType, WebLoadError, LogContext
from training_data_bot.sources.base import BaseLoader


class WebLoader(BaseLoader):
    """
    Loader for web content.
    
    Fetches and extracts text content from web pages.
    """
    
    def __init__(self, timeout: float = 30.0, user_agent: str = "TrainingDataBot/0.1.0"):
        """
        Initialize the web loader.
        
        Args:
            timeout: Request timeout in seconds
            user_agent: User agent string for requests
        """
        super().__init__()
        self.supported_formats = [DocumentType.URL]
        self.timeout = timeout
        self.user_agent = user_agent
    
    async def load_single(
        self,
        source: Union[str, Path],
        **kwargs
    ) -> Document:
        """
        Load content from a web URL.
        
        Args:
            source: URL to load
            **kwargs: Additional parameters
            
        Returns:
            Document object with extracted content
            
        Raises:
            WebLoadError: If loading fails
        """
        url = str(source)
        
        with LogContext("load_single", component="WebLoader"):
            self.logger.info(f"Loading URL: {url}")
            
            # Validate URL
            if not url.startswith(('http://', 'https://')):
                raise WebLoadError(
                    f"Invalid URL (must start with http:// or https://): {url}",
                    url=url
                )
            
            # Fetch content
            try:
                content = await self._fetch_url_content(url)
                title = self._extract_title(url, content)
                
                # Create document
                document = self.create_document(
                    title=title,
                    content=content,
                    source=url,
                    doc_type=DocumentType.URL,
                    extraction_method="httpx",
                )
                
                self.logger.info(
                    f"Successfully loaded URL: {url}",
                    word_count=document.word_count,
                    char_count=document.char_count
                )
                
                return document
                
            except Exception as e:
                if isinstance(e, WebLoadError):
                    raise
                raise WebLoadError(
                    f"Failed to load URL: {url}",
                    url=url,
                    cause=e
                )
    
    async def _fetch_url_content(self, url: str) -> str:
        """
        Fetch content from URL.
        
        Args:
            url: URL to fetch
            
        Returns:
            Extracted text content
            
        Raises:
            WebLoadError: If request fails
        """
        try:
            import httpx
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Make request with custom headers
                headers = {
                    'User-Agent': self.user_agent,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                }
                
                response = await client.get(url, headers=headers, follow_redirects=True)
                
                # Check for HTTP errors
                if response.status_code != 200:
                    raise WebLoadError(
                        f"HTTP {response.status_code} error",
                        url=url,
                        status_code=response.status_code
                    )
                
                # Get content type
                content_type = response.headers.get('content-type', '').lower()
                
                # Extract text based on content type
                if 'text/html' in content_type:
                    return self._extract_html_text(response.text)
                else:
                    # For plain text or other types
                    return response.text
                    
        except ImportError:
            raise WebLoadError(
                "httpx package required for web loading. "
                "Install with: pip install httpx",
                url=url
            )
        except httpx.TimeoutException:
            raise WebLoadError(
                f"Request timeout after {self.timeout} seconds",
                url=url
            )
        except httpx.HTTPError as e:
            raise WebLoadError(
                f"HTTP error: {e}",
                url=url,
                cause=e
            )
    
    def _extract_html_text(self, html: str) -> str:
        """
        Extract text content from HTML.
        
        Args:
            html: HTML string
            
        Returns:
            Extracted text
        """
        try:
            from bs4 import BeautifulSoup
            
            # Parse HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # Remove script and style elements
            for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                script.decompose()
            
            # Get text
            text = soup.get_text()
            
            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except ImportError:
            self.logger.warning(
                "BeautifulSoup not installed, returning raw HTML"
            )
            return html
    
    def _extract_title(self, url: str, content: str) -> str:
        """
        Extract title from URL or content.
        
        Args:
            url: Source URL
            content: Page content (may be HTML)
            
        Returns:
            Extracted or generated title
        """
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # Try to find title tag
            title_tag = soup.find('title')
            if title_tag and title_tag.text.strip():
                return title_tag.text.strip()
                
        except ImportError:
            pass
        
        # Fallback: use URL path
        parsed = urlparse(url)
        path_title = parsed.netloc + parsed.path
        
        # Clean up the title
        path_title = path_title.rstrip('/')
        if not path_title:
            path_title = url
        
        return path_title


================================================================================
FILE: training_data_bot\storage\__init__.py
================================================================================

"""
Storage module for data persistence and export.

This module provides dataset export and optional database storage.
"""

from training_data_bot.storage.exporter import DatasetExporter

__all__ = [
    "DatasetExporter",
]


================================================================================
FILE: training_data_bot\storage\exporter.py
================================================================================

"""
Dataset exporter for multiple formats.

This module handles exporting training datasets to various formats
including JSONL, JSON, CSV, and Parquet.
"""

import json
import csv
from pathlib import Path
from typing import List, Optional, Union, Dict, Any
from datetime import datetime

from training_data_bot.core import (
    TrainingExample,
    Dataset,
    ExportFormat,
    get_logger,
    LogContext,
    ExportError,
)


class DatasetExporter:
    """
    Dataset exporter supporting multiple formats.
    
    Exports training examples and datasets to JSONL, JSON, CSV,
    and Parquet formats suitable for ML training pipelines.
    """
    
    def __init__(self, **kwargs):
        """
        Initialize the dataset exporter.
        
        Args:
            **kwargs: Additional configuration parameters
        """
        self.logger = get_logger("storage.DatasetExporter")
        self.config = kwargs
        
        self.logger.info("DatasetExporter initialized")
    
    async def export_dataset(
        self,
        dataset: Dataset,
        output_path: Union[str, Path],
        format: ExportFormat = ExportFormat.JSONL,
        split_data: bool = False,
        **kwargs
    ) -> Path:
        """
        Export a dataset to file.
        
        Args:
            dataset: Dataset to export
            output_path: Output file path
            format: Export format (JSONL, JSON, CSV, Parquet)
            split_data: Whether to split into train/val/test files
            **kwargs: Additional export options
            
        Returns:
            Path to exported file(s)
        """
        with LogContext("export_dataset", dataset_id=str(dataset.id)):
            output_path = Path(output_path)
            
            self.logger.info(
                f"Exporting dataset",
                format=format.value,
                examples=len(dataset.examples),
                split_data=split_data
            )
            
            try:
                if split_data:
                    return await self._export_split_dataset(
                        dataset, output_path, format, **kwargs
                    )
                else:
                    return await self._export_single_file(
                        dataset.examples, output_path, format, **kwargs
                    )
            except Exception as e:
                raise ExportError(
                    f"Failed to export dataset: {e}",
                    export_format=format.value,
                    output_path=str(output_path),
                    cause=e
                )
    
    async def export_examples(
        self,
        examples: List[TrainingExample],
        output_path: Union[str, Path],
        format: ExportFormat = ExportFormat.JSONL,
        **kwargs
    ) -> Path:
        """
        Export a list of training examples.
        
        Args:
            examples: List of training examples
            output_path: Output file path
            format: Export format
            **kwargs: Additional export options
            
        Returns:
            Path to exported file
        """
        with LogContext("export_examples"):
            self.logger.info(
                f"Exporting {len(examples)} examples",
                format=format.value
            )
            
            return await self._export_single_file(
                examples, Path(output_path), format, **kwargs
            )
    
    async def _export_single_file(
        self,
        examples: List[TrainingExample],
        output_path: Path,
        format: ExportFormat,
        **kwargs
    ) -> Path:
        """Export examples to a single file."""
        # Ensure output directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Export based on format
        if format == ExportFormat.JSONL:
            return await self._export_jsonl(examples, output_path, **kwargs)
        elif format == ExportFormat.JSON:
            return await self._export_json(examples, output_path, **kwargs)
        elif format == ExportFormat.CSV:
            return await self._export_csv(examples, output_path, **kwargs)
        elif format == ExportFormat.PARQUET:
            return await self._export_parquet(examples, output_path, **kwargs)
        else:
            raise ExportError(f"Unsupported export format: {format}")
    
    async def _export_split_dataset(
        self,
        dataset: Dataset,
        output_path: Path,
        format: ExportFormat,
        **kwargs
    ) -> Path:
        """Export dataset split into train/val/test files."""
        # Calculate split sizes
        total = len(dataset.examples)
        train_size = int(total * dataset.train_split)
        val_size = int(total * dataset.validation_split)
        
        # Split examples
        train_examples = dataset.examples[:train_size]
        val_examples = dataset.examples[train_size:train_size + val_size]
        test_examples = dataset.examples[train_size + val_size:]
        
        # Generate file paths
        base_path = output_path.parent / output_path.stem
        extension = self._get_extension(format)
        
        train_path = Path(f"{base_path}_train{extension}")
        val_path = Path(f"{base_path}_val{extension}")
        test_path = Path(f"{base_path}_test{extension}")
        
        # Export each split
        await self._export_single_file(train_examples, train_path, format, **kwargs)
        await self._export_single_file(val_examples, val_path, format, **kwargs)
        await self._export_single_file(test_examples, test_path, format, **kwargs)
        
        self.logger.info(
            f"Dataset split exported",
            train_size=len(train_examples),
            val_size=len(val_examples),
            test_size=len(test_examples)
        )
        
        return base_path.parent
    
    async def _export_jsonl(
        self,
        examples: List[TrainingExample],
        output_path: Path,
        **kwargs
    ) -> Path:
        """Export to JSONL format (one JSON object per line)."""
        include_metadata = kwargs.get("include_metadata", True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for example in examples:
                data = self._example_to_dict(example, include_metadata)
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        self.logger.debug(f"Exported {len(examples)} examples to JSONL: {output_path}")
        return output_path
    
    async def _export_json(
        self,
        examples: List[TrainingExample],
        output_path: Path,
        **kwargs
    ) -> Path:
        """Export to JSON format (single array)."""
        include_metadata = kwargs.get("include_metadata", True)
        pretty_print = kwargs.get("pretty_print", True)
        
        data = [self._example_to_dict(ex, include_metadata) for ex in examples]
        
        with open(output_path, 'w', encoding='utf-8') as f:
            if pretty_print:
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                json.dump(data, f, ensure_ascii=False)
        
        self.logger.debug(f"Exported {len(examples)} examples to JSON: {output_path}")
        return output_path
    
    async def _export_csv(
        self,
        examples: List[TrainingExample],
        output_path: Path,
        **kwargs
    ) -> Path:
        """Export to CSV format."""
        include_metadata = kwargs.get("include_metadata", False)
        
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            # Determine fieldnames
            if include_metadata:
                fieldnames = [
                    'id', 'input_text', 'output_text', 'task_type',
                    'source_document_id', 'quality_scores', 'metadata'
                ]
            else:
                fieldnames = ['input_text', 'output_text', 'task_type']
            
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for example in examples:
                row = {
                    'input_text': example.input_text,
                    'output_text': example.output_text,
                    'task_type': example.task_type.value,
                }
                
                if include_metadata:
                    row.update({
                        'id': str(example.id),
                        'source_document_id': str(example.source_document_id),
                        'quality_scores': json.dumps(example.quality_scores),
                        'metadata': json.dumps(example.metadata),
                    })
                
                writer.writerow(row)
        
        self.logger.debug(f"Exported {len(examples)} examples to CSV: {output_path}")
        return output_path
    
    async def _export_parquet(
        self,
        examples: List[TrainingExample],
        output_path: Path,
        **kwargs
    ) -> Path:
        """Export to Parquet format."""
        try:
            import pandas as pd
        except ImportError:
            raise ExportError(
                "pandas is required for Parquet export. Install with: pip install pandas pyarrow"
            )
        
        include_metadata = kwargs.get("include_metadata", True)
        
        # Convert examples to DataFrame
        data = [self._example_to_dict(ex, include_metadata) for ex in examples]
        df = pd.DataFrame(data)
        
        # Write to Parquet
        df.to_parquet(output_path, index=False)
        
        self.logger.debug(f"Exported {len(examples)} examples to Parquet: {output_path}")
        return output_path
    
    def _example_to_dict(
        self,
        example: TrainingExample,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """Convert training example to dictionary."""
        data = {
            'input': example.input_text,
            'output': example.output_text,
            'task_type': example.task_type.value,
        }
        
        if include_metadata:
            data.update({
                'id': str(example.id),
                'source_document_id': str(example.source_document_id),
                'source_chunk_id': str(example.source_chunk_id) if example.source_chunk_id else None,
                'quality_scores': example.quality_scores,
                'difficulty': example.difficulty,
                'tags': example.tags,
                'metadata': example.metadata,
                'created_at': example.created_at.isoformat() if example.created_at else None,
            })
        
        return data
    
    def _get_extension(self, format: ExportFormat) -> str:
        """Get file extension for format."""
        extensions = {
            ExportFormat.JSONL: '.jsonl',
            ExportFormat.JSON: '.json',
            ExportFormat.CSV: '.csv',
            ExportFormat.PARQUET: '.parquet',
        }
        return extensions.get(format, '.txt')
    
    def get_export_info(self, output_path: Path) -> Dict[str, Any]:
        """
        Get information about an exported file.
        
        Args:
            output_path: Path to exported file
            
        Returns:
            Dictionary with file information
        """
        if not output_path.exists():
            return {"exists": False}
        
        return {
            "exists": True,
            "path": str(output_path),
            "size_bytes": output_path.stat().st_size,
            "size_mb": output_path.stat().st_size / (1024 * 1024),
            "modified": datetime.fromtimestamp(output_path.stat().st_mtime).isoformat(),
        }


================================================================================
FILE: training_data_bot\tasks\__init__.py
================================================================================

"""
Tasks module for training data generation.

This module provides task generators for creating different types
of training data from text chunks.
"""

from training_data_bot.tasks.base import BaseTaskGenerator
from training_data_bot.tasks.manager import TaskManager
from training_data_bot.tasks.generators import (
    QAGenerator,
    ClassificationGenerator,
    SummarizationGenerator,
)

from ..core import TaskTemplate

__all__ = [
    # Base
    "BaseTaskGenerator",
    
    # Manager
    "TaskManager",
    
    # Generators
    "QAGenerator",
    "ClassificationGenerator",
    "SummarizationGenerator",

    # Template
    "TaskTemplate",
]


================================================================================
FILE: training_data_bot\tasks\base.py
================================================================================

"""
Base task generator interface.

This module defines the abstract base class that all task generators must implement.
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from uuid import uuid4

from training_data_bot.core import (
    TextChunk,
    TaskTemplate,
    TaskResult,
    TrainingExample,
    TaskType,
    get_logger,
    LogContext,
)
from training_data_bot.ai import AIClient, AIResponse


class BaseTaskGenerator(ABC):
    """
    Abstract base class for task generators.
    
    All task generators (QA, Classification, Summarization, etc.) 
    must implement this interface.
    """
    
    def __init__(
        self,
        ai_client: AIClient,
        task_type: TaskType,
        **kwargs
    ):
        """
        Initialize the task generator.
        
        Args:
            ai_client: AI client for generation
            task_type: Type of task this generator handles
            **kwargs: Additional generator-specific parameters
        """
        self.ai_client = ai_client
        self.task_type = task_type
        self.logger = get_logger(f"tasks.{self.__class__.__name__}")
        self.extra_params = kwargs
        
        # Statistics tracking
        self.stats = {
            "total_generated": 0,
            "total_failed": 0,
            "total_tokens_used": 0,
            "total_time": 0.0,
        }
    
    @abstractmethod
    def get_default_template(self) -> TaskTemplate:
        """
        Get the default task template for this generator.
        
        Returns:
            TaskTemplate object with default prompt and parameters
        """
        pass
    
    @abstractmethod
    async def generate_single(
        self,
        chunk: TextChunk,
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> TaskResult:
        """
        Generate a single task result from a text chunk.
        
        Args:
            chunk: Text chunk to process
            template: Optional custom template to use
            **kwargs: Additional generation parameters
            
        Returns:
            TaskResult object with generated output
        """
        pass
    
    async def generate_batch(
        self,
        chunks: List[TextChunk],
        template: Optional[TaskTemplate] = None,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[TaskResult]:
        """
        Generate task results for multiple chunks.
        
        Args:
            chunks: List of text chunks to process
            template: Optional custom template to use
            max_concurrent: Maximum concurrent requests
            **kwargs: Additional generation parameters
            
        Returns:
            List of TaskResult objects
        """
        with LogContext("generate_batch", component=self.__class__.__name__):
            self.logger.info(
                f"Generating batch of {len(chunks)} tasks",
                batch_size=len(chunks),
                task_type=self.task_type.value
            )
            
            import asyncio
            
            # Create semaphore to limit concurrency
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def generate_with_semaphore(chunk):
                async with semaphore:
                    try:
                        return await self.generate_single(chunk, template, **kwargs)
                    except Exception as e:
                        self.logger.error(
                            f"Failed to generate task for chunk {chunk.id}: {e}"
                        )
                        self.stats["total_failed"] += 1
                        return None
            
            # Execute all tasks
            tasks = [generate_with_semaphore(chunk) for chunk in chunks]
            results = await asyncio.gather(*tasks)
            
            # Filter out None results
            valid_results = [r for r in results if r is not None]
            
            self.logger.info(
                f"Batch generation complete",
                total_chunks=len(chunks),
                successful=len(valid_results),
                failed=len(chunks) - len(valid_results)
            )
            
            return valid_results
    
    def create_training_example(
        self,
        task_result: TaskResult,
        chunk: TextChunk,
        **kwargs
    ) -> TrainingExample:
        """
        Convert a task result into a training example.
        
        Args:
            task_result: The task result to convert
            chunk: The source text chunk
            **kwargs: Additional metadata
            
        Returns:
            TrainingExample object
        """
        return TrainingExample(
            id=uuid4(),
            input_text=self._format_input(chunk),
            output_text=task_result.output,
            task_type=self.task_type,
            source_document_id=chunk.document_id,
            source_chunk_id=chunk.id,
            quality_scores=task_result.quality_scores,
            metadata={
                "confidence": task_result.confidence,
                "model_used": task_result.model_used,
                "tokens_used": task_result.tokens_used,
                **kwargs
            }
        )
    
    def _format_input(self, chunk: TextChunk) -> str:
        """
        Format the chunk content for input.
        
        Args:
            chunk: Text chunk to format
            
        Returns:
            Formatted input text
        """
        # Default implementation - can be overridden
        return chunk.content
    
    def _build_prompt(
        self,
        chunk: TextChunk,
        template: TaskTemplate,
        **kwargs
    ) -> str:
        """
        Build the prompt from template and chunk.
        
        Args:
            chunk: Text chunk to process
            template: Task template with prompt template
            **kwargs: Additional template variables
            
        Returns:
            Formatted prompt string
        """
        # Replace template variables
        prompt = template.prompt_template
        
        # Common variables
        variables = {
            "text": chunk.content,
            "chunk_index": chunk.chunk_index,
            "token_count": chunk.token_count,
            **kwargs
        }
        
        # Replace all variables in template
        for key, value in variables.items():
            prompt = prompt.replace(f"{{{key}}}", str(value))
        
        return prompt
    
    def _calculate_confidence(
        self,
        response: AIResponse,
        chunk: TextChunk
    ) -> float:
        """
        Calculate confidence score for the generated output.
        
        Args:
            response: AI response object
            chunk: Source text chunk
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        # Base confidence from finish reason
        confidence = 1.0 if response.finish_reason == "stop" else 0.5
        
        # Adjust based on output length
        output_length = len(response.content.split())
        if output_length < 5:
            confidence *= 0.5
        elif output_length > 500:
            confidence *= 0.8
        
        return min(max(confidence, 0.0), 1.0)
    
    def _assess_quality(
        self,
        output: str,
        chunk: TextChunk
    ) -> Dict[str, float]:
        """
        Assess the quality of generated output.
        
        Args:
            output: Generated output text
            chunk: Source text chunk
            
        Returns:
            Dictionary of quality metric scores
        """
        scores = {}
        
        # Length check
        output_words = len(output.split())
        if output_words > 0:
            scores["length_score"] = min(output_words / 100, 1.0)
        else:
            scores["length_score"] = 0.0
        
        # Non-empty check
        scores["completeness"] = 1.0 if output.strip() else 0.0
        
        # Relevance (simple heuristic - can be enhanced)
        chunk_words = set(chunk.content.lower().split())
        output_words_set = set(output.lower().split())
        overlap = len(chunk_words & output_words_set)
        scores["relevance"] = min(overlap / len(chunk_words) if chunk_words else 0, 1.0)
        
        return scores
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get generation statistics.
        
        Returns:
            Dictionary with statistics
        """
        return {
            "task_type": self.task_type.value,
            "total_generated": self.stats["total_generated"],
            "total_failed": self.stats["total_failed"],
            "success_rate": (
                self.stats["total_generated"] / 
                (self.stats["total_generated"] + self.stats["total_failed"])
                if (self.stats["total_generated"] + self.stats["total_failed"]) > 0
                else 0.0
            ),
            "total_tokens_used": self.stats["total_tokens_used"],
            "total_time": self.stats["total_time"],
            "avg_time_per_task": (
                self.stats["total_time"] / self.stats["total_generated"]
                if self.stats["total_generated"] > 0
                else 0.0
            )
        }
    
    def reset_statistics(self):
        """Reset all statistics counters."""
        self.stats = {
            "total_generated": 0,
            "total_failed": 0,
            "total_tokens_used": 0,
            "total_time": 0.0,
        }
    
    def __repr__(self) -> str:
        """String representation of the generator."""
        return f"{self.__class__.__name__}(task_type={self.task_type.value})"


================================================================================
FILE: training_data_bot\tasks\generators\__init__.py
================================================================================

"""
Task generators module.

Exports all available task generators.
"""

from training_data_bot.tasks.generators.qa_generator import QAGenerator
from training_data_bot.tasks.generators.classification_generator import ClassificationGenerator
from training_data_bot.tasks.generators.summarization_generator import SummarizationGenerator


__all__ = [
    "QAGenerator",
    "ClassificationGenerator",
    "SummarizationGenerator",
]


================================================================================
FILE: training_data_bot\tasks\generators\classification_generator.py
================================================================================

"""
Classification task generator.

Generates text classification training examples from text chunks.
"""

import time
from typing import Optional, List
from uuid import uuid4

from training_data_bot.core import (
    TextChunk,
    TaskTemplate,
    TaskResult,
    TaskType,
    LogContext,
)
from training_data_bot.tasks.base import BaseTaskGenerator


class ClassificationGenerator(BaseTaskGenerator):
    """
    Text classification task generator.
    
    Generates classification examples with labels and reasoning
    suitable for training classification models.
    """
    
    def __init__(self, ai_client, **kwargs):
        """
        Initialize the classification generator.
        
        Args:
            ai_client: AI client for generation
            **kwargs: Additional parameters including:
                - categories: List of classification categories
                - include_reasoning: Whether to include reasoning
        """
        super().__init__(
            ai_client=ai_client,
            task_type=TaskType.CLASSIFICATION,
            **kwargs
        )
        
        self.categories = kwargs.get("categories", [
            "informative", "opinion", "narrative", "instructional", "analytical"
        ])
        self.include_reasoning = kwargs.get("include_reasoning", True)
    
    def get_default_template(self) -> TaskTemplate:
        """Get the default classification template."""
        return TaskTemplate(
            id=uuid4(),
            name="Default Text Classification",
            task_type=TaskType.CLASSIFICATION,
            description="Classify text into predefined categories",
            prompt_template="""Classify the following text into one of these categories: {categories}

Text:
{text}

Instructions:
- Choose the most appropriate category
- Provide a brief reasoning for your classification
- Be objective and analytical

Format your response as:
Category: [chosen category]
Reasoning: [brief explanation]

Classification:""",
            parameters={
                "categories": ", ".join(self.categories),
                "temperature": 0.3,
                "max_tokens": 300,
            },
            version="1.0"
        )
    
    async def generate_single(
        self,
        chunk: TextChunk,
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> TaskResult:
        """
        Generate classification for a single text chunk.
        
        Args:
            chunk: Text chunk to classify
            template: Optional custom template
            **kwargs: Additional generation parameters
            
        Returns:
            TaskResult with classification and reasoning
        """
        with LogContext("classification_generate_single", chunk_id=str(chunk.id)):
            # Use provided template or default
            template = template or self.get_default_template()
            
            # Get categories from kwargs or use defaults
            categories = kwargs.get("categories", self.categories)
            
            # Build the prompt
            prompt = self._build_prompt(
                chunk=chunk,
                template=template,
                categories=", ".join(categories)
            )
            
            # Track start time
            start_time = time.time()
            
            try:
                # Generate with AI
                response = await self.ai_client.generate(
                    prompt=prompt,
                    system_prompt="You are an expert at text classification and analysis.",
                    temperature=template.parameters.get("temperature", 0.3),
                    max_tokens=template.parameters.get("max_tokens", 300),
                )
                
                # Calculate metrics
                processing_time = time.time() - start_time
                confidence = self._calculate_confidence(response, chunk)
                quality_scores = self._assess_quality(response.content, chunk, categories)
                
                # Update statistics
                self.stats["total_generated"] += 1
                self.stats["total_tokens_used"] += response.tokens_used
                self.stats["total_time"] += processing_time
                
                # Create task result
                result = TaskResult(
                    id=uuid4(),
                    task_id=template.id,
                    input_chunk_id=chunk.id,
                    output=response.content,
                    confidence=confidence,
                    quality_scores=quality_scores,
                    processing_time=processing_time,
                    model_used=response.model,
                    tokens_used=response.tokens_used
                )
                
                self.logger.debug(
                    f"Generated classification",
                    chunk_id=str(chunk.id),
                    confidence=confidence,
                    tokens_used=response.tokens_used
                )
                
                return result
                
            except Exception as e:
                self.logger.error(f"Classification generation failed: {e}")
                self.stats["total_failed"] += 1
                raise
    
    def _assess_quality(
        self,
        output: str,
        chunk: TextChunk,
        categories: List[str]
    ) -> dict:
        """
        Assess quality of generated classification.
        
        Args:
            output: Generated classification text
            chunk: Source text chunk
            categories: Valid categories
            
        Returns:
            Dictionary of quality scores
        """
        scores = super()._assess_quality(output, chunk)
        
        # Parse classification result
        parsed = self._parse_classification(output)
        
        # Check if category is valid
        if parsed["category"]:
            # Normalize and check
            category_lower = parsed["category"].lower()
            valid_categories = [c.lower() for c in categories]
            scores["valid_category"] = 1.0 if category_lower in valid_categories else 0.0
        else:
            scores["valid_category"] = 0.0
        
        # Check if reasoning is provided
        if parsed["reasoning"]:
            reasoning_length = len(parsed["reasoning"].split())
            # Good reasoning is 10-50 words
            scores["reasoning_quality"] = 1.0 if 10 <= reasoning_length <= 50 else 0.7
        else:
            scores["reasoning_quality"] = 0.0 if self.include_reasoning else 1.0
        
        # Check structure
        scores["structure_score"] = 1.0 if parsed["category"] else 0.0
        
        return scores
    
    def _parse_classification(self, output: str) -> dict:
        """
        Parse classification result from output.
        
        Args:
            output: Generated text with classification
            
        Returns:
            Dictionary with category and reasoning
        """
        result = {
            "category": None,
            "reasoning": None
        }
        
        lines = output.split('\n')
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('Category:'):
                result["category"] = line.split(':', 1)[1].strip()
            elif line.startswith('Reasoning:'):
                result["reasoning"] = line.split(':', 1)[1].strip()
            elif result["reasoning"] and line and not line.startswith('Category:'):
                # Continue reasoning
                result["reasoning"] += ' ' + line
        
        return result
    
    def _format_input(self, chunk: TextChunk) -> str:
        """
        Format chunk for classification input.
        
        Args:
            chunk: Text chunk
            
        Returns:
            Formatted input text with instruction
        """
        return f"Classify this text into one of the predefined categories:\n{chunk.content}"
    
    def set_categories(self, categories: List[str]):
        """
        Update the classification categories.
        
        Args:
            categories: New list of categories
        """
        self.categories = categories
        self.logger.info(f"Updated categories to: {categories}")


================================================================================
FILE: training_data_bot\tasks\generators\qa_generator.py
================================================================================

"""
Q&A task generator.

Generates question-answer pairs from text chunks for training data.
"""

import time
from typing import Optional
from uuid import uuid4

from training_data_bot.core import (
    TextChunk,
    TaskTemplate,
    TaskResult,
    TaskType,
    LogContext,
)
from training_data_bot.ai import AIResponse
from training_data_bot.tasks.base import BaseTaskGenerator


class QAGenerator(BaseTaskGenerator):
    """
    Question-Answer pair generator.
    
    Generates high-quality Q&A pairs from text chunks suitable for
    training question-answering models.
    """
    
    def __init__(self, ai_client, **kwargs):
        """
        Initialize the Q&A generator.
        
        Args:
            ai_client: AI client for generation
            **kwargs: Additional parameters
        """
        super().__init__(
            ai_client=ai_client,
            task_type=TaskType.QA_GENERATION,
            **kwargs
        )
        
        self.num_questions = kwargs.get("num_questions", 3)
        self.question_types = kwargs.get("question_types", [
            "factual", "analytical", "application"
        ])
    
    def get_default_template(self) -> TaskTemplate:
        """Get the default Q&A generation template."""
        return TaskTemplate(
            id=uuid4(),
            name="Default Q&A Generation",
            task_type=TaskType.QA_GENERATION,
            description="Generate question-answer pairs from text",
            prompt_template="""Read the following text carefully and generate {num_questions} high-quality question-answer pairs.

Text:
{text}

Requirements:
- Questions should be clear and specific
- Answers should be accurate and found in the text
- Cover different aspects of the text
- Use varied question types: factual, analytical, and application-based

Format each Q&A pair as:
Q: [question]
A: [answer]

Generate {num_questions} Q&A pairs:""",
            parameters={
                "num_questions": self.num_questions,
                "temperature": 0.7,
                "max_tokens": 1000,
            },
            version="1.0"
        )
    
    async def generate_single(
        self,
        chunk: TextChunk,
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> TaskResult:
        """
        Generate Q&A pairs from a single text chunk.
        
        Args:
            chunk: Text chunk to process
            template: Optional custom template
            **kwargs: Additional generation parameters
            
        Returns:
            TaskResult with generated Q&A pairs
        """
        with LogContext("qa_generate_single", chunk_id=str(chunk.id)):
            # Use provided template or default
            template = template or self.get_default_template()
            
            # Build the prompt
            prompt = self._build_prompt(
                chunk=chunk,
                template=template,
                num_questions=kwargs.get("num_questions", self.num_questions)
            )
            
            # Track start time
            start_time = time.time()
            
            try:
                # Generate with AI
                response = await self.ai_client.generate(
                    prompt=prompt,
                    system_prompt="You are an expert at creating educational question-answer pairs.",
                    temperature=template.parameters.get("temperature", 0.7),
                    max_tokens=template.parameters.get("max_tokens", 1000),
                )
                
                # Calculate metrics
                processing_time = time.time() - start_time
                confidence = self._calculate_confidence(response, chunk)
                quality_scores = self._assess_quality(response.content, chunk)
                
                # Update statistics
                self.stats["total_generated"] += 1
                self.stats["total_tokens_used"] += response.tokens_used
                self.stats["total_time"] += processing_time
                
                # Create task result
                result = TaskResult(
                    id=uuid4(),
                    task_id=template.id,
                    input_chunk_id=chunk.id,
                    output=response.content,
                    confidence=confidence,
                    quality_scores=quality_scores,
                    processing_time=processing_time,
                    model_used=response.model,
                    tokens_used=response.tokens_used
                )
                
                self.logger.debug(
                    f"Generated Q&A pairs",
                    chunk_id=str(chunk.id),
                    confidence=confidence,
                    tokens_used=response.tokens_used
                )
                
                return result
                
            except Exception as e:
                self.logger.error(f"Q&A generation failed: {e}")
                self.stats["total_failed"] += 1
                raise
    
    def _assess_quality(self, output: str, chunk: TextChunk) -> dict:
        """
        Assess quality of generated Q&A pairs.
        
        Args:
            output: Generated Q&A text
            chunk: Source text chunk
            
        Returns:
            Dictionary of quality scores
        """
        scores = super()._assess_quality(output, chunk)
        
        # Q&A specific quality checks
        qa_pairs = self._parse_qa_pairs(output)
        
        # Check if we got the expected number of pairs
        expected_count = self.num_questions
        actual_count = len(qa_pairs)
        scores["count_accuracy"] = min(actual_count / expected_count, 1.0) if expected_count > 0 else 0.0
        
        # Check Q&A structure
        scores["structure_score"] = 1.0 if actual_count > 0 else 0.0
        
        # Check average lengths
        if qa_pairs:
            avg_q_length = sum(len(q.split()) for q, _ in qa_pairs) / len(qa_pairs)
            avg_a_length = sum(len(a.split()) for _, a in qa_pairs) / len(qa_pairs)
            
            # Good questions are 5-20 words
            scores["question_length"] = 1.0 if 5 <= avg_q_length <= 20 else 0.5
            
            # Good answers are 10-100 words
            scores["answer_length"] = 1.0 if 10 <= avg_a_length <= 100 else 0.7
        
        return scores
    
    def _parse_qa_pairs(self, output: str) -> list:
        """
        Parse Q&A pairs from output text.
        
        Args:
            output: Generated text with Q&A pairs
            
        Returns:
            List of (question, answer) tuples
        """
        pairs = []
        lines = output.split('\n')
        
        current_question = None
        current_answer = None
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('Q:') or line.startswith('Question:'):
                # Save previous pair if exists
                if current_question and current_answer:
                    pairs.append((current_question, current_answer))
                
                # Start new question
                current_question = line.split(':', 1)[1].strip() if ':' in line else line
                current_answer = None
                
            elif line.startswith('A:') or line.startswith('Answer:'):
                # Start answer
                current_answer = line.split(':', 1)[1].strip() if ':' in line else line
            
            elif current_answer is not None and line:
                # Continue answer
                current_answer += ' ' + line
        
        # Add last pair
        if current_question and current_answer:
            pairs.append((current_question, current_answer))
        
        return pairs
    
    def _format_input(self, chunk: TextChunk) -> str:
        """
        Format chunk for Q&A input.
        
        Args:
            chunk: Text chunk
            
        Returns:
            Formatted input text
        """
        return f"Text for Q&A generation:\n{chunk.content}"


================================================================================
FILE: training_data_bot\tasks\generators\summarization_generator.py
================================================================================

"""
Summarization task generator.

Generates text summaries from text chunks for training data.
"""

import time
from typing import Optional
from uuid import uuid4

from training_data_bot.core import (
    TextChunk,
    TaskTemplate,
    TaskResult,
    TaskType,
    LogContext,
)
from training_data_bot.tasks.base import BaseTaskGenerator


class SummarizationGenerator(BaseTaskGenerator):
    """
    Text summarization generator.
    
    Generates concise summaries from text chunks suitable for
    training summarization models.
    """
    
    def __init__(self, ai_client, **kwargs):
        """
        Initialize the summarization generator.
        
        Args:
            ai_client: AI client for generation
            **kwargs: Additional parameters including:
                - summary_style: Style of summary (concise, detailed, bullet)
                - max_summary_length: Maximum words in summary
        """
        super().__init__(
            ai_client=ai_client,
            task_type=TaskType.SUMMARIZATION,
            **kwargs
        )
        
        self.summary_style = kwargs.get("summary_style", "concise")
        self.max_summary_length = kwargs.get("max_summary_length", 100)
    
    def get_default_template(self) -> TaskTemplate:
        """Get the default summarization template."""
        style_instructions = {
            "concise": "Create a brief, one-paragraph summary",
            "detailed": "Create a comprehensive summary with key details",
            "bullet": "Create a bullet-point summary of main points"
        }
        
        instruction = style_instructions.get(
            self.summary_style,
            "Create a concise summary"
        )
        
        return TaskTemplate(
            id=uuid4(),
            name="Default Summarization",
            task_type=TaskType.SUMMARIZATION,
            description="Generate summaries from text",
            prompt_template="""Read the following text and create a summary.

Text:
{text}

Instructions:
- {instruction}
- Keep it under {max_length} words
- Capture the main ideas and key points
- Use clear, concise language
- Maintain factual accuracy

Summary:""",
            parameters={
                "instruction": instruction,
                "max_length": self.max_summary_length,
                "temperature": 0.5,
                "max_tokens": 500,
            },
            version="1.0"
        )
    
    async def generate_single(
        self,
        chunk: TextChunk,
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> TaskResult:
        """
        Generate summary for a single text chunk.
        
        Args:
            chunk: Text chunk to summarize
            template: Optional custom template
            **kwargs: Additional generation parameters
            
        Returns:
            TaskResult with generated summary
        """
        with LogContext("summarization_generate_single", chunk_id=str(chunk.id)):
            # Use provided template or default
            template = template or self.get_default_template()
            
            # Get style instructions
            style_instructions = {
                "concise": "Create a brief, one-paragraph summary",
                "detailed": "Create a comprehensive summary with key details",
                "bullet": "Create a bullet-point summary of main points"
            }
            
            style = kwargs.get("summary_style", self.summary_style)
            instruction = style_instructions.get(style, "Create a concise summary")
            
            # Build the prompt
            prompt = self._build_prompt(
                chunk=chunk,
                template=template,
                instruction=instruction,
                max_length=kwargs.get("max_summary_length", self.max_summary_length)
            )
            
            # Track start time
            start_time = time.time()
            
            try:
                # Generate with AI
                response = await self.ai_client.generate(
                    prompt=prompt,
                    system_prompt="You are an expert at creating clear, concise summaries that capture key information.",
                    temperature=template.parameters.get("temperature", 0.5),
                    max_tokens=template.parameters.get("max_tokens", 500),
                )
                
                # Calculate metrics
                processing_time = time.time() - start_time
                confidence = self._calculate_confidence(response, chunk)
                quality_scores = self._assess_quality(response.content, chunk)
                
                # Update statistics
                self.stats["total_generated"] += 1
                self.stats["total_tokens_used"] += response.tokens_used
                self.stats["total_time"] += processing_time
                
                # Create task result
                result = TaskResult(
                    id=uuid4(),
                    task_id=template.id,
                    input_chunk_id=chunk.id,
                    output=response.content,
                    confidence=confidence,
                    quality_scores=quality_scores,
                    processing_time=processing_time,
                    model_used=response.model,
                    tokens_used=response.tokens_used
                )
                
                self.logger.debug(
                    f"Generated summary",
                    chunk_id=str(chunk.id),
                    confidence=confidence,
                    tokens_used=response.tokens_used
                )
                
                return result
                
            except Exception as e:
                self.logger.error(f"Summarization generation failed: {e}")
                self.stats["total_failed"] += 1
                raise
    
    def _assess_quality(self, output: str, chunk: TextChunk) -> dict:
        """
        Assess quality of generated summary.
        
        Args:
            output: Generated summary text
            chunk: Source text chunk
            
        Returns:
            Dictionary of quality scores
        """
        scores = super()._assess_quality(output, chunk)
        
        # Summary-specific quality checks
        summary_length = len(output.split())
        source_length = len(chunk.content.split())
        
        # Check if summary is shorter than source (compression ratio)
        if source_length > 0:
            compression_ratio = summary_length / source_length
            # Good summaries are 10-50% of original length
            if 0.1 <= compression_ratio <= 0.5:
                scores["compression_score"] = 1.0
            elif compression_ratio < 0.1:
                scores["compression_score"] = 0.7  # Too short
            else:
                scores["compression_score"] = 0.5  # Not compressed enough
        else:
            scores["compression_score"] = 0.0
        
        # Check length against target
        target_length = self.max_summary_length
        if summary_length <= target_length:
            scores["length_compliance"] = 1.0
        else:
            # Penalize for exceeding target
            overage = (summary_length - target_length) / target_length
            scores["length_compliance"] = max(1.0 - overage, 0.0)
        
        # Check for completeness (summary should be substantial)
        if summary_length >= 20:
            scores["completeness"] = 1.0
        elif summary_length >= 10:
            scores["completeness"] = 0.7
        else:
            scores["completeness"] = 0.3
        
        return scores
    
    def _format_input(self, chunk: TextChunk) -> str:
        """
        Format chunk for summarization input.
        
        Args:
            chunk: Text chunk
            
        Returns:
            Formatted input text with instruction
        """
        return f"Summarize the following text:\n{chunk.content}"
    
    def set_summary_style(self, style: str):
        """
        Update the summary style.
        
        Args:
            style: New style (concise, detailed, or bullet)
        """
        valid_styles = ["concise", "detailed", "bullet"]
        if style in valid_styles:
            self.summary_style = style
            self.logger.info(f"Updated summary style to: {style}")
        else:
            raise ValueError(f"Invalid style. Choose from: {valid_styles}")
    
    def set_max_length(self, max_length: int):
        """
        Update the maximum summary length.
        
        Args:
            max_length: Maximum words in summary
        """
        if max_length > 0:
            self.max_summary_length = max_length
            self.logger.info(f"Updated max summary length to: {max_length}")
        else:
            raise ValueError("max_length must be positive")


================================================================================
FILE: training_data_bot\tasks\manager.py
================================================================================

"""
Task manager for orchestrating task generation.

This module manages the creation and execution of tasks across
different generators, providing a unified interface.
"""

import asyncio
from typing import Dict, List, Optional, Union
from uuid import UUID

from training_data_bot.core import (
    TextChunk,
    TaskTemplate,
    TaskResult,
    TrainingExample,
    TaskType,
    ProcessingJob,
    ProcessingStatus,
    get_logger,
    LogContext,
    TaskProcessingError,
)
from training_data_bot.ai import AIClient
from training_data_bot.tasks.base import BaseTaskGenerator
from training_data_bot.tasks.generators import (
    QAGenerator,
    ClassificationGenerator,
    SummarizationGenerator,
)


class TaskManager:
    """
    Task manager for orchestrating task generation.
    
    Manages multiple task generators and coordinates the creation
    of training examples from text chunks.
    """
    
    # Registry of available generators
    GENERATOR_REGISTRY = {
        TaskType.QA_GENERATION: QAGenerator,
        TaskType.CLASSIFICATION: ClassificationGenerator,
        TaskType.SUMMARIZATION: SummarizationGenerator,
    }
    
    def __init__(
        self,
        ai_client: Optional[AIClient] = None,
        **kwargs
    ):
        """
        Initialize the task manager.
        
        Args:
            ai_client: AI client for generation (creates default if None)
            **kwargs: Additional configuration parameters
        """
        self.logger = get_logger("tasks.TaskManager")
        
        # Initialize AI client
        if ai_client is None:
            # Create default client - should be provided in production
            self.logger.warning("No AI client provided, task generation will fail")
            self.ai_client = None
        else:
            self.ai_client = ai_client
        
        # Initialize generators dictionary
        self.generators: Dict[TaskType, BaseTaskGenerator] = {}
        
        # Task templates
        self.templates: Dict[UUID, TaskTemplate] = {}
        
        # Active jobs tracking
        self.jobs: Dict[UUID, ProcessingJob] = {}
        
        # Configuration
        self.max_concurrent = kwargs.get("max_concurrent", 5)
        
        self.logger.info(
            "TaskManager initialized",
            max_concurrent=self.max_concurrent
        )
    
    def register_generator(
        self,
        task_type: TaskType,
        generator: Optional[BaseTaskGenerator] = None,
        **kwargs
    ):
        """
        Register a task generator for a specific task type.
        
        Args:
            task_type: Type of task
            generator: Pre-initialized generator (creates default if None)
            **kwargs: Parameters for generator initialization
        """
        if generator is None:
            # Create default generator for this task type
            if task_type not in self.GENERATOR_REGISTRY:
                raise TaskProcessingError(
                    f"No default generator available for task type: {task_type}",
                    task_type=task_type.value
                )
            
            generator_class = self.GENERATOR_REGISTRY[task_type]
            generator = generator_class(self.ai_client, **kwargs)
        
        self.generators[task_type] = generator
        self.logger.info(f"Registered generator for {task_type.value}")
    
    def get_generator(self, task_type: TaskType) -> BaseTaskGenerator:
        """
        Get the generator for a specific task type.
        
        Args:
            task_type: Type of task
            
        Returns:
            Task generator instance
        """
        if task_type not in self.generators:
            # Auto-register default generator
            self.register_generator(task_type)
        
        return self.generators[task_type]
    
    async def generate_task(
        self,
        chunk: TextChunk,
        task_type: TaskType,
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> TaskResult:
        """
        Generate a single task result.
        
        Args:
            chunk: Text chunk to process
            task_type: Type of task to generate
            template: Optional custom template
            **kwargs: Additional generation parameters
            
        Returns:
            TaskResult object
        """
        with LogContext("generate_task", task_type=task_type.value):
            generator = self.get_generator(task_type)
            
            result = await generator.generate_single(
                chunk=chunk,
                template=template,
                **kwargs
            )
            
            return result
    
    async def generate_tasks(
        self,
        chunks: List[TextChunk],
        task_types: List[TaskType],
        template: Optional[TaskTemplate] = None,
        **kwargs
    ) -> List[TaskResult]:
        """
        Generate multiple task results for chunks and task types.
        
        Args:
            chunks: List of text chunks to process
            task_types: List of task types to generate
            template: Optional custom template
            **kwargs: Additional generation parameters
            
        Returns:
            List of TaskResult objects
        """
        with LogContext("generate_tasks", 
                       chunk_count=len(chunks),
                       task_type_count=len(task_types)):
            
            self.logger.info(
                f"Generating tasks",
                chunks=len(chunks),
                task_types=[t.value for t in task_types]
            )
            
            # Create all task combinations
            tasks = []
            for chunk in chunks:
                for task_type in task_types:
                    tasks.append(
                        self.generate_task(
                            chunk=chunk,
                            task_type=task_type,
                            template=template,
                            **kwargs
                        )
                    )
            
            # Execute with limited concurrency
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            async def generate_with_semaphore(task_coro):
                async with semaphore:
                    try:
                        return await task_coro
                    except Exception as e:
                        self.logger.error(f"Task generation failed: {e}")
                        return None
            
            results = await asyncio.gather(
                *[generate_with_semaphore(t) for t in tasks]
            )
            
            # Filter out None results
            valid_results = [r for r in results if r is not None]
            
            self.logger.info(
                f"Task generation complete",
                total_tasks=len(tasks),
                successful=len(valid_results),
                failed=len(tasks) - len(valid_results)
            )
            
            return valid_results
    
    async def create_training_examples(
        self,
        chunks: List[TextChunk],
        task_types: List[TaskType],
        **kwargs
    ) -> List[TrainingExample]:
        """
        Create training examples from chunks.
        
        Args:
            chunks: List of text chunks
            task_types: List of task types to generate
            **kwargs: Additional parameters
            
        Returns:
            List of TrainingExample objects
        """
        with LogContext("create_training_examples",
                       chunk_count=len(chunks)):
            
            # Generate all task results
            task_results = await self.generate_tasks(
                chunks=chunks,
                task_types=task_types,
                **kwargs
            )
            
            # Convert to training examples
            examples = []
            chunk_map = {chunk.id: chunk for chunk in chunks}
            
            for result in task_results:
                chunk = chunk_map.get(result.input_chunk_id)
                if chunk:
                    generator = self.get_generator(
                        self._get_task_type_from_result(result)
                    )
                    example = generator.create_training_example(
                        task_result=result,
                        chunk=chunk
                    )
                    examples.append(example)
            
            self.logger.info(
                f"Created {len(examples)} training examples"
            )
            
            return examples
    
    def _get_task_type_from_result(self, result: TaskResult) -> TaskType:
        """
        Determine task type from result.
        
        Args:
            result: Task result
            
        Returns:
            TaskType enum value
        """
        # Check which generator produced this result
        for task_type, generator in self.generators.items():
            if generator.task_type == task_type:
                # Simple heuristic - could be improved
                return task_type
        
        # Default fallback
        return TaskType.QA_GENERATION
    
    async def process_job(
        self,
        job: ProcessingJob,
        chunks: List[TextChunk],
        task_types: List[TaskType],
        **kwargs
    ) -> List[TrainingExample]:
        """
        Process a job with progress tracking.
        
        Args:
            job: Processing job to track
            chunks: Text chunks to process
            task_types: Task types to generate
            **kwargs: Additional parameters
            
        Returns:
            List of TrainingExample objects
        """
        with LogContext("process_job", job_id=str(job.id)):
            try:
                # Update job status
                job.status = ProcessingStatus.RUNNING
                job.total_items = len(chunks) * len(task_types)
                job.processed_items = 0
                
                import datetime
                job.started_at = datetime.datetime.utcnow()
                
                # Store job
                self.jobs[job.id] = job
                
                # Process in batches for progress tracking
                batch_size = max(1, len(chunks) // 10)  # 10% batches
                examples = []
                
                for i in range(0, len(chunks), batch_size):
                    batch_chunks = chunks[i:i + batch_size]
                    
                    batch_examples = await self.create_training_examples(
                        chunks=batch_chunks,
                        task_types=task_types,
                        **kwargs
                    )
                    
                    examples.extend(batch_examples)
                    
                    # Update progress
                    job.processed_items += len(batch_chunks) * len(task_types)
                    job.update_progress()
                
                # Job complete
                job.status = ProcessingStatus.COMPLETED
                job.completed_at = datetime.datetime.utcnow()
                
                self.logger.info(
                    f"Job {job.id} completed",
                    examples_created=len(examples)
                )
                
                return examples
                
            except Exception as e:
                job.status = ProcessingStatus.FAILED
                job.error_message = str(e)
                self.logger.error(f"Job {job.id} failed: {e}")
                raise
    
    def get_statistics(self) -> Dict[str, any]:
        """
        Get statistics from all generators.
        
        Returns:
            Dictionary with statistics per task type
        """
        stats = {}
        
        for task_type, generator in self.generators.items():
            stats[task_type.value] = generator.get_statistics()
        
        return stats
    
    def reset_statistics(self):
        """Reset statistics for all generators."""
        for generator in self.generators.values():
            generator.reset_statistics()
    
    def __repr__(self) -> str:
        """String representation of the manager."""
        return f"TaskManager(generators={len(self.generators)})"


================================================================================
FILE: training_data_bot\tasks.py
================================================================================




================================================================================
FILE: training_data_bot\test_foundation.py
================================================================================

from core.models import Document, DocumentType, TrainingExample, TaskType
from core.exceptions import DocumentLoadError

# Test data model creation
doc = Document(
    title="Test Document",
    content="This is a test document content.",
    source="/path/to/test.txt",
    doc_type=DocumentType.TXT
)

print(f"Created document: {doc.title} with ID: {doc.id}")
print(f"Word count: {doc.word_count}")


================================================================================
FILE: training_data_bot\test_imports.py
================================================================================

"""Quick import test to verify all imports work."""

import sys
from pathlib import Path

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

print("Testing imports...")

try:
    from training_data_bot.core import Document, Dataset, TaskType
    print("âœ… Core imports work")
except ImportError as e:
    print(f"âŒ Core imports failed: {e}")

try:
    from training_data_bot.sources import UnifiedLoader
    print("âœ… Sources imports work")
except ImportError as e:
    print(f"âŒ Sources imports failed: {e}")

try:
    from training_data_bot.preprocessing import TextPreprocessor
    print("âœ… Preprocessing imports work")
except ImportError as e:
    print(f"âŒ Preprocessing imports failed: {e}")

try:
    from training_data_bot.ai import AIClient
    print("âœ… AI imports work")
except ImportError as e:
    print(f"âŒ AI imports failed: {e}")

try:
    from training_data_bot.tasks import TaskManager, QAGenerator
    print("âœ… Tasks imports work")
except ImportError as e:
    print(f"âŒ Tasks imports failed: {e}")

try:
    from training_data_bot.evaluation import QualityEvaluator
    print("âœ… Evaluation imports work")
except ImportError as e:
    print(f"âŒ Evaluation imports failed: {e}")

try:
    from training_data_bot.storage import DatasetExporter
    print("âœ… Storage imports work")
except ImportError as e:
    print(f"âŒ Storage imports failed: {e}")

try:
    from training_data_bot.bot import TrainingDataBot
    print("âœ… Bot import works")
except ImportError as e:
    print(f"âŒ Bot import failed: {e}")

try:
    from training_data_bot import TrainingDataBot
    print("âœ… Package-level import works")
except ImportError as e:
    print(f"âŒ Package-level import failed: {e}")

print("\nâœ… All imports successful!")


================================================================================
FILE: training_data_bot\test_openai_live.py
================================================================================

import asyncio
import os
from pathlib import Path
import sys
from dotenv import load_dotenv

# 1. FIRST: Add parent directory to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# 2. THEN: Load environment variables
load_dotenv()

# 3. FINALLY: Import from the package
from training_data_bot.ai import AIClient

async def test_openai_connection():
    """Test OpenAI API connection and functionality."""
    print("\n" + "="*60)
    print("Testing OpenAI API Connection")
    print("="*60)
    
    api_key = os.getenv("TDB_OPENAI_API_KEY")
    if not api_key:
        print("âŒ TDB_OPENAI_API_KEY not found in .env")
        return
    
    try:
        # Initialize client
        client = AIClient(
            provider="openai",
            api_key=api_key,
            model="gpt-3.5-turbo"  # Cheapest option
        )
        print(f"âœ“ Client initialized: {client}")
        
        # Test 1: Simple generation
        print("\nTest 1: Simple Generation")
        response = await client.generate(
            prompt="What is 2+2? Answer in one sentence.",
            max_tokens=50
        )
        print(f"âœ“ Response: {response.content}")
        print(f"âœ“ Tokens used: {response.tokens_used}")
        print(f"âœ“ Response time: {response.response_time:.2f}s")
        
        # Test 2: Cost calculation
        print("\nTest 2: Cost Estimation")
        cost = client.estimate_cost(
            response.metadata['prompt_tokens'],
            response.metadata['completion_tokens']
        )
        print(f"âœ“ Prompt tokens: {response.metadata['prompt_tokens']}")
        print(f"âœ“ Completion tokens: {response.metadata['completion_tokens']}")
        print(f"âœ“ Estimated cost: ${cost:.4f}")
        
        # Test 3: With system prompt
        print("\nTest 3: With System Prompt")
        response2 = await client.generate(
            prompt="Explain AI in 10 words.",
            system_prompt="You are a helpful assistant that gives concise answers.",
            max_tokens=30
        )
        print(f"âœ“ Response: {response2.content}")
        
        # Test 4: Token counting
        print("\nTest 4: Token Counting")
        test_text = "The Training Data Bot is an enterprise-grade system."
        tokens = client.count_tokens(test_text)
        print(f"âœ“ Text: '{test_text}'")
        print(f"âœ“ Estimated tokens: {tokens}")
        
        await client.close()
        
        print("\n" + "="*60)
        print("âœ“ ALL OPENAI TESTS PASSED!")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_openai_connection())


================================================================================
FILE: training_data_bot\test_session2.py
================================================================================

"""
Test script for Session 2: Configuration and Logging Infrastructure
Run this to verify the configuration and logging systems are working correctly.
"""

import sys
sys.path.append('.')

from core import (
    settings,
    get_settings,
    setup_logging,
    get_logger,
    LogContext,
    get_performance_logger,
    validate_configuration,
)


def test_configuration():
    """Test configuration loading and validation."""
    print("\n" + "="*60)
    print("TEST 1: Configuration System")
    print("="*60)
    
    # Get global settings
    config = get_settings()
    
    print(f"âœ“ App Name: {config.app_name}")
    print(f"âœ“ Version: {config.app_version}")
    print(f"âœ“ Environment: {config.environment}")
    print(f"âœ“ Log Level: {config.log_level}")
    print(f"âœ“ Chunk Size: {config.chunk_size}")
    print(f"âœ“ Chunk Overlap: {config.chunk_overlap}")
    print(f"âœ“ Max Workers: {config.max_workers}")
    print(f"âœ“ Quality Threshold: {config.quality_threshold}")
    print(f"âœ“ Output Directory: {config.output_directory}")
    
    # Test AI provider config
    openai_config = config.get_ai_provider_config("openai")
    if openai_config:
        print(f"âœ“ OpenAI Model: {openai_config.model_name}")
    else:
        print("âš  OpenAI config not available (API key not set)")
    
    # Test processing config
    proc_config = config.get_processing_config()
    print(f"âœ“ Processing Config Created: {proc_config.chunk_size} tokens/chunk")
    
    # Validate configuration
    issues = validate_configuration(config)
    if issues:
        print(f"\nâš  Configuration Issues Found:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("\nâœ“ Configuration validation passed!")
    
    print("\nâœ“ Configuration system working correctly!")


def test_logging_basic():
    """Test basic logging functionality."""
    print("\n" + "="*60)
    print("TEST 2: Basic Logging")
    print("="*60)
    
    # Setup logging
    logger = setup_logging(level="INFO", structured=False)
    
    print("\nLogging different levels:")
    logger.debug("This is a debug message (should not appear if level is INFO)")
    logger.info("This is an info message")
    logger.warning("This is a warning message")
    logger.error("This is an error message")
    
    # Log with extra fields
    logger.info(
        "Processing document",
        document_id="doc_123",
        word_count=1500,
        processing_time=2.5
    )
    
    print("\nâœ“ Basic logging working correctly!")


def test_logging_context():
    """Test logging context management."""
    print("\n" + "="*60)
    print("TEST 3: Logging Context")
    print("="*60)
    
    logger = get_logger()
    
    # Test context manager
    with LogContext("document_loading", component="loader"):
        logger.info("Loading document from source")
        logger.info("Extracting text content")
        logger.info("Creating document object")
    
    # Test nested contexts
    with LogContext("document_processing", component="processor"):
        logger.info("Starting document processing")
        
        with LogContext("text_chunking", component="preprocessor"):
            logger.info("Splitting text into chunks")
            logger.info("Created 10 chunks")
        
        logger.info("Processing complete")
    
    print("\nâœ“ Logging context working correctly!")


def test_performance_logging():
    """Test performance logging."""
    print("\n" + "="*60)
    print("TEST 4: Performance Logging")
    print("="*60)
    
    perf_logger = get_performance_logger()
    
    # Log processing stats
    perf_logger.log_processing_stats(
        operation="document_batch_processing",
        total_items=100,
        processed_items=95,
        failed_items=5,
        duration=45.3
    )
    
    # Log API call
    perf_logger.log_api_call(
        provider="openai",
        model="gpt-3.5-turbo",
        tokens_used=1250,
        response_time=1.8,
        success=True
    )
    
    # Log document processing
    perf_logger.log_document_processing(
        document_id="doc_456",
        document_type="pdf",
        word_count=5000,
        chunks_created=5,
        processing_time=3.2
    )
    
    print("\nâœ“ Performance logging working correctly!")


def test_error_logging():
    """Test error logging and exception handling."""
    print("\n" + "="*60)
    print("TEST 5: Error Logging")
    print("="*60)
    
    logger = get_logger()
    
    # Log a simulated error
    try:
        # Simulate an error
        raise ValueError("This is a test error")
    except Exception as e:
        logger.exception(
            "An error occurred during processing",
            error_type=type(e).__name__,
            operation="test_operation"
        )
    
    print("\nâœ“ Error logging working correctly!")


def test_configuration_validation():
    """Test configuration validation."""
    print("\n" + "="*60)
    print("TEST 6: Configuration Validation")
    print("="*60)
    
    from core.config import Settings
    
    # Test valid configuration
    valid_config = Settings(
        chunk_size=1000,
        chunk_overlap=200,
        quality_threshold=0.8
    )
    issues = validate_configuration(valid_config)
    print(f"âœ“ Valid config issues: {len(issues)}")
    
    # Test invalid configuration (will raise validation error during creation)
    try:
        invalid_config = Settings(
            chunk_size=100,
            chunk_overlap=200,  # Overlap >= chunk_size (invalid)
        )
        print("âš  Should have raised validation error!")
    except Exception as e:
        print(f"âœ“ Validation caught error: {type(e).__name__}")
    
    print("\nâœ“ Configuration validation working correctly!")


def test_directory_creation():
    """Test automatic directory creation."""
    print("\n" + "="*60)
    print("TEST 7: Directory Creation")
    print("="*60)
    
    import os
    from pathlib import Path
    
    config = get_settings()
    config.create_directories()
    
    # Check if directories were created
    output_exists = Path(config.output_directory).exists()
    temp_exists = Path(config.temp_directory).exists()
    
    print(f"âœ“ Output directory exists: {output_exists}")
    print(f"âœ“ Temp directory exists: {temp_exists}")
    
    if output_exists and temp_exists:
        print("\nâœ“ Directory creation working correctly!")
    else:
        print("\nâš  Some directories were not created")


def run_all_tests():
    """Run all Session 2 tests."""
    print("\n" + "="*80)
    print(" "*20 + "SESSION 2 TEST SUITE")
    print(" "*15 + "Configuration and Logging Infrastructure")
    print("="*80)
    
    try:
        test_configuration()
        test_logging_basic()
        test_logging_context()
        test_performance_logging()
        test_error_logging()
        test_configuration_validation()
        test_directory_creation()
        
        print("\n" + "="*80)
        print(" "*25 + "ALL TESTS PASSED! âœ“")
        print("="*80)
        print("\nSession 2 is complete and working correctly!")
        print("\nNext Steps:")
        print("1. Create a .env file from .env.example")
        print("2. Add your API keys if you have them")
        print("3. Ready to proceed to Session 3: Document Loaders")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()


================================================================================
FILE: training_data_bot\test_session3.py
================================================================================

"""
Test script for Session 3: Document Loaders
Run this to verify the document loading system is working correctly.
"""

import sys
import asyncio
from pathlib import Path

sys.path.append('.')

from core import DocumentType, get_logger, setup_logging
from sources import BaseLoader, DocumentLoader, PDFLoader, WebLoader


# Setup logging for tests
setup_logging(level="INFO", structured=False)
logger = get_logger()


def create_test_files():
    """Create test files for loading."""
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # Create a simple text file
    txt_file = test_dir / "sample.txt"
    txt_file.write_text("This is a sample text document. It contains multiple sentences. This is for testing the document loader.")
    
    # Create a markdown file
    md_file = test_dir / "sample.md"
    md_file.write_text("# Sample Markdown\n\nThis is a **markdown** document with *formatting*.\n\n## Section 2\n\nSome more content here.")
    
    # Create a JSON file
    json_file = test_dir / "sample.json"
    json_file.write_text('{"name": "Test Document", "type": "JSON", "count": 42, "active": true}')
    
    # Create a CSV file
    csv_file = test_dir / "sample.csv"
    csv_file.write_text("Name,Age,City\nAlice,30,New York\nBob,25,Los Angeles\nCharlie,35,Chicago")
    
    # Create an HTML file
    html_file = test_dir / "sample.html"
    html_file.write_text("""
    <!DOCTYPE html>
    <html>
    <head><title>Sample HTML</title></head>
    <body>
        <h1>Test Page</h1>
        <p>This is a paragraph with some content.</p>
        <script>console.log('test');</script>
        <p>Another paragraph here.</p>
    </body>
    </html>
    """)
    
    logger.info(f"Created test files in: {test_dir}")
    return test_dir


async def test_base_loader():
    """Test base loader functionality."""
    print("\n" + "="*60)
    print("TEST 1: Base Loader")
    print("="*60)
    
    # BaseLoader is abstract, so we'll test with DocumentLoader
    loader = DocumentLoader()
    
    print(f"âœ“ Loader created: {loader}")
    print(f"âœ“ Supported formats: {[fmt.value for fmt in loader.supported_formats]}")
    
    # Test document type detection
    test_path = Path("test.txt")
    doc_type = loader.get_document_type(test_path)
    print(f"âœ“ Document type detection: test.txt -> {doc_type.value}")
    
    # Test URL detection
    url_type = loader.get_document_type("https://example.com")
    print(f"âœ“ URL detection: https://example.com -> {url_type.value}")
    
    print("\nâœ“ Base loader functionality working!")


async def test_text_loading():
    """Test loading text-based documents."""
    print("\n" + "="*60)
    print("TEST 2: Text Document Loading")
    print("="*60)
    
    loader = DocumentLoader()
    test_dir = Path("test_documents")
    
    # Test TXT file
    txt_file = test_dir / "sample.txt"
    if txt_file.exists():
        doc = await loader.load_single(txt_file)
        print(f"âœ“ Loaded TXT: {doc.title}")
        print(f"  - Word count: {doc.word_count}")
        print(f"  - Char count: {doc.char_count}")
        print(f"  - Content preview: {doc.content[:50]}...")
    else:
        print("âš  TXT file not found, skipping")
    
    # Test Markdown file
    md_file = test_dir / "sample.md"
    if md_file.exists():
        doc = await loader.load_single(md_file)
        print(f"âœ“ Loaded MD: {doc.title}")
        print(f"  - Word count: {doc.word_count}")
    else:
        print("âš  MD file not found, skipping")
    
    # Test JSON file
    json_file = test_dir / "sample.json"
    if json_file.exists():
        doc = await loader.load_single(json_file)
        print(f"âœ“ Loaded JSON: {doc.title}")
        print(f"  - Content: {doc.content[:80]}...")
    else:
        print("âš  JSON file not found, skipping")
    
    # Test CSV file
    csv_file = test_dir / "sample.csv"
    if csv_file.exists():
        doc = await loader.load_single(csv_file)
        print(f"âœ“ Loaded CSV: {doc.title}")
        print(f"  - Content preview: {doc.content[:100]}...")
    else:
        print("âš  CSV file not found, skipping")
    
    # Test HTML file
    html_file = test_dir / "sample.html"
    if html_file.exists():
        doc = await loader.load_single(html_file)
        print(f"âœ“ Loaded HTML: {doc.title}")
        print(f"  - Word count: {doc.word_count}")
        print(f"  - Scripts removed: {'console.log' not in doc.content}")
    else:
        print("âš  HTML file not found, skipping")
    
    print("\nâœ“ Text document loading working!")


async def test_batch_loading():
    """Test loading multiple documents at once."""
    print("\n" + "="*60)
    print("TEST 3: Batch Document Loading")
    print("="*60)
    
    loader = DocumentLoader()
    test_dir = Path("test_documents")
    
    # Get all test files
    sources = [
        test_dir / "sample.txt",
        test_dir / "sample.md",
        test_dir / "sample.json",
        test_dir / "sample.csv",
        test_dir / "sample.html",
    ]
    
    # Filter to only existing files
    sources = [s for s in sources if s.exists()]
    
    print(f"Loading {len(sources)} documents in parallel...")
    documents = await loader.load_multiple(sources, max_workers=4)
    
    print(f"âœ“ Loaded {len(documents)} documents successfully")
    
    for doc in documents:
        print(f"  - {doc.title}: {doc.word_count} words ({doc.doc_type.value})")
    
    print("\nâœ“ Batch loading working!")


async def test_directory_loading():
    """Test loading all documents from a directory."""
    print("\n" + "="*60)
    print("TEST 4: Directory Loading")
    print("="*60)
    
    loader = DocumentLoader()
    test_dir = Path("test_documents")
    
    if not test_dir.exists():
        print("âš  Test directory not found, skipping")
        return
    
    print(f"Loading all documents from: {test_dir}")
    documents = await loader.load_directory(test_dir, recursive=False)
    
    print(f"âœ“ Loaded {len(documents)} documents from directory")
    
    # Group by type
    by_type = {}
    for doc in documents:
        doc_type = doc.doc_type.value
        by_type[doc_type] = by_type.get(doc_type, 0) + 1
    
    print("\nDocuments by type:")
    for doc_type, count in by_type.items():
        print(f"  - {doc_type}: {count}")
    
    print("\nâœ“ Directory loading working!")


async def test_pdf_loader():
    """Test PDF loading (if PyMuPDF is installed)."""
    print("\n" + "="*60)
    print("TEST 5: PDF Loading")
    print("="*60)
    
    try:
        import fitz
        print("âœ“ PyMuPDF is installed")
        
        loader = PDFLoader()
        print(f"âœ“ PDF loader created: {loader}")
        print(f"âœ“ Supported formats: {[fmt.value for fmt in loader.supported_formats]}")
        
        # Note: Actual PDF loading requires a real PDF file
        print("\nâš  No test PDF file available - PDF loading functionality is ready")
        print("  To test PDF loading, add a PDF file and call:")
        print("  doc = await loader.load_single('path/to/file.pdf')")
        
    except ImportError:
        print("âš  PyMuPDF not installed - install with: pip install PyMuPDF")
        print("  PDF loading will work once PyMuPDF is installed")


async def test_web_loader():
    """Test web content loading (if httpx is installed)."""
    print("\n" + "="*60)
    print("TEST 6: Web Content Loading")
    print("="*60)
    
    try:
        import httpx
        print("âœ“ httpx is installed")
        
        loader = WebLoader()
        print(f"âœ“ Web loader created: {loader}")
        print(f"âœ“ Supported formats: {[fmt.value for fmt in loader.supported_formats]}")
        
        # Test with a simple example URL
        try:
            print("\nAttempting to load example.com...")
            doc = await loader.load_single("https://example.com")
            print(f"âœ“ Loaded web page: {doc.title}")
            print(f"  - Word count: {doc.word_count}")
            print(f"  - Content preview: {doc.content[:100]}...")
            print("\nâœ“ Web loading working!")
            
        except Exception as e:
            print(f"âš  Web loading test failed (network issue?): {e}")
            print("  Web loading functionality is implemented and ready")
        
    except ImportError:
        print("âš  httpx not installed - install with: pip install httpx")
        print("  Web loading will work once httpx is installed")


async def test_error_handling():
    """Test error handling for invalid sources."""
    print("\n" + "="*60)
    print("TEST 7: Error Handling")
    print("="*60)
    
    loader = DocumentLoader()
    
    # Test non-existent file
    try:
        await loader.load_single("nonexistent_file.txt")
        print("âŒ Should have raised an error for non-existent file")
    except Exception as e:
        print(f"âœ“ Correctly raised error for non-existent file: {type(e).__name__}")
    
    # Test unsupported format
    try:
        loader.get_document_type("file.xyz")
        print("âŒ Should have raised an error for unsupported format")
    except Exception as e:
        print(f"âœ“ Correctly raised error for unsupported format: {type(e).__name__}")
    
    print("\nâœ“ Error handling working correctly!")


async def run_all_tests():
    """Run all Session 3 tests."""
    print("\n" + "="*80)
    print(" "*20 + "SESSION 3 TEST SUITE")
    print(" "*18 + "Document Loading Pipeline")
    print("="*80)
    
    # Create test files
    create_test_files()
    
    try:
        await test_base_loader()
        await test_text_loading()
        await test_batch_loading()
        await test_directory_loading()
        await test_pdf_loader()
        await test_web_loader()
        await test_error_handling()
        
        print("\n" + "="*80)
        print(" "*25 + "ALL TESTS PASSED! âœ“")
        print("="*80)
        print("\nSession 3 is complete and working correctly!")
        print("\nWhat we can now do:")
        print("1. Load text files (TXT, MD, HTML, JSON, CSV, DOCX)")
        print("2. Load PDF documents (with PyMuPDF)")
        print("3. Load web pages (with httpx)")
        print("4. Batch load multiple documents in parallel")
        print("5. Load entire directories of documents")
        print("\nNext Steps:")
        print("1. Install optional dependencies: pip install PyMuPDF httpx beautifulsoup4 python-docx")
        print("2. Ready to proceed to Session 4: Unified Loading System")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(run_all_tests())


================================================================================
FILE: training_data_bot\test_session4.py
================================================================================

"""
Test script for Session 4: Unified Loading System
Run this to verify the unified loader is working correctly.
"""

import sys
import asyncio
from pathlib import Path

sys.path.append('.')

from core import DocumentType, get_logger, setup_logging
from sources import UnifiedLoader


# Setup logging for tests
setup_logging(level="INFO", structured=False)
logger = get_logger()


def ensure_test_files():
    """Ensure test files exist."""
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # Create test files if they don't exist
    files = {
        "sample.txt": "This is a sample text document for testing the unified loader.",
        "sample.md": "# Test Markdown\n\nContent for testing.",
        "sample.json": '{"test": "data", "number": 123}',
        "sample.csv": "Name,Value\nTest1,100\nTest2,200",
        "sample.html": "<html><body><h1>Test</h1><p>Content</p></body></html>",
    }
    
    for filename, content in files.items():
        filepath = test_dir / filename
        if not filepath.exists():
            filepath.write_text(content)
    
    return test_dir


async def test_unified_loader_initialization():
    """Test unified loader initialization."""
    print("\n" + "="*60)
    print("TEST 1: Unified Loader Initialization")
    print("="*60)
    
    loader = UnifiedLoader()
    
    print(f"âœ“ Unified loader created: {loader}")
    print(f"âœ“ Total supported formats: {len(loader.supported_formats)}")
    
    # Get loader info
    loader_info = loader.get_loader_info()
    print("\nâœ“ Registered loaders:")
    for loader_name, formats in loader_info.items():
        print(f"  - {loader_name}: {', '.join(formats)}")
    
    # Get supported formats
    formats = loader.get_supported_formats()
    print(f"\nâœ“ Supported file formats: {', '.join(formats)}")
    
    print("\nâœ“ Unified loader initialization working!")


async def test_automatic_type_detection():
    """Test automatic document type detection and routing."""
    print("\n" + "="*60)
    print("TEST 2: Automatic Type Detection")
    print("="*60)
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    # Test different file types
    test_files = [
        ("sample.txt", "TXT"),
        ("sample.md", "MD"),
        ("sample.json", "JSON"),
        ("sample.csv", "CSV"),
        ("sample.html", "HTML"),
    ]
    
    print("Testing automatic type detection and loading:\n")
    
    for filename, expected_type in test_files:
        filepath = test_dir / filename
        if filepath.exists():
            doc = await loader.load_single(filepath)
            actual_type = doc.doc_type.value.upper()
            match = "âœ“" if actual_type == expected_type else "âœ—"
            print(f"{match} {filename} -> {actual_type} (expected: {expected_type})")
        else:
            print(f"âš  {filename} not found, skipping")
    
    print("\nâœ“ Automatic type detection working!")


async def test_mixed_batch_loading():
    """Test loading a mixed batch of different document types."""
    print("\n" + "="*60)
    print("TEST 3: Mixed Batch Loading")
    print("="*60)
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    # Create a mixed batch
    sources = [
        test_dir / "sample.txt",
        test_dir / "sample.md",
        test_dir / "sample.json",
        test_dir / "sample.csv",
        test_dir / "sample.html",
    ]
    
    # Filter to only existing files
    sources = [s for s in sources if s.exists()]
    
    print(f"Loading mixed batch of {len(sources)} documents...")
    documents = await loader.load_multiple(sources, max_workers=4)
    
    print(f"\nâœ“ Loaded {len(documents)} documents successfully")
    
    # Show what was loaded
    type_counts = {}
    for doc in documents:
        doc_type = doc.doc_type.value
        type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
    
    print("\nDocuments by type:")
    for doc_type, count in sorted(type_counts.items()):
        print(f"  - {doc_type}: {count}")
    
    print("\nâœ“ Mixed batch loading working!")


async def test_grouped_loading():
    """Test loading with documents grouped by type."""
    print("\n" + "="*60)
    print("TEST 4: Grouped Loading")
    print("="*60)
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    sources = [
        test_dir / "sample.txt",
        test_dir / "sample.md",
        test_dir / "sample.json",
        test_dir / "sample.csv",
        test_dir / "sample.html",
    ]
    
    sources = [s for s in sources if s.exists()]
    
    print(f"Loading {len(sources)} documents with grouping...")
    grouped = await loader.load_mixed_batch(
        sources,
        max_workers=4,
        group_by_type=True
    )
    
    print(f"\nâœ“ Documents grouped into {len(grouped)} types:")
    
    for doc_type, docs in grouped.items():
        print(f"\n  {doc_type.value.upper()}:")
        for doc in docs:
            print(f"    - {doc.title} ({doc.word_count} words)")
    
    print("\nâœ“ Grouped loading working!")


async def test_directory_loading():
    """Test loading entire directory with unified loader."""
    print("\n" + "="*60)
    print("TEST 5: Directory Loading")
    print("="*60)
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    if not test_dir.exists():
        print("âš  Test directory not found, skipping")
        return
    
    print(f"Loading all documents from: {test_dir}")
    documents = await loader.load_directory(test_dir, recursive=False)
    
    print(f"\nâœ“ Loaded {len(documents)} documents from directory")
    
    # Group and display
    by_type = {}
    for doc in documents:
        doc_type = doc.doc_type.value
        by_type[doc_type] = by_type.get(doc_type, 0) + 1
    
    print("\nDocuments by type:")
    for doc_type, count in sorted(by_type.items()):
        print(f"  - {doc_type}: {count}")
    
    print("\nâœ“ Directory loading working!")


async def test_url_loading():
    """Test loading from URLs."""
    print("\n" + "="*60)
    print("TEST 6: URL Loading")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Test with a simple example URL
    try:
        print("Attempting to load example.com...")
        doc = await loader.load_single("https://example.com")
        print(f"âœ“ Loaded web page: {doc.title}")
        print(f"  - Word count: {doc.word_count}")
        print(f"  - Document type: {doc.doc_type.value}")
        print("\nâœ“ URL loading working!")
        
    except Exception as e:
        print(f"âš  URL loading test failed (network issue?): {e}")
        print("  URL loading functionality is implemented and ready")


async def test_mixed_sources():
    """Test loading from mixed sources (files and URLs)."""
    print("\n" + "="*60)
    print("TEST 7: Mixed Sources (Files + URLs)")
    print("="*60)
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    # Mix of local files and URLs
    sources = [
        test_dir / "sample.txt",
        test_dir / "sample.json",
        "https://example.com",  # URL
    ]
    
    # Filter to only existing files (keep URL)
    filtered_sources = []
    for source in sources:
        if isinstance(source, str) and source.startswith('http'):
            filtered_sources.append(source)
        elif Path(source).exists():
            filtered_sources.append(source)
    
    if len(filtered_sources) > 0:
        print(f"Loading {len(filtered_sources)} sources (files + URLs)...")
        
        try:
            documents = await loader.load_multiple(
                filtered_sources,
                max_workers=2,
                skip_errors=True
            )
            
            print(f"\nâœ“ Loaded {len(documents)} documents")
            
            for doc in documents:
                source_type = "URL" if doc.doc_type == DocumentType.URL else "File"
                print(f"  - {doc.title} ({source_type}, {doc.doc_type.value})")
            
            print("\nâœ“ Mixed sources loading working!")
            
        except Exception as e:
            print(f"âš  Mixed sources test partially succeeded")
            print(f"  Note: {e}")
    else:
        print("âš  No valid sources available for testing")


async def test_error_handling():
    """Test error handling in unified loader."""
    print("\n" + "="*60)
    print("TEST 8: Error Handling")
    print("="*60)
    
    loader = UnifiedLoader()
    
    # Test 1: Non-existent file
    print("Test 1: Non-existent file")
    try:
        await loader.load_single("nonexistent_file.txt")
        print("âŒ Should have raised an error")
    except Exception as e:
        print(f"âœ“ Correctly raised: {type(e).__name__}")
    
    # Test 2: Unsupported format
    print("\nTest 2: Unsupported format")
    try:
        await loader.load_single("file.xyz")
        print("âŒ Should have raised an error")
    except Exception as e:
        print(f"âœ“ Correctly raised: {type(e).__name__}")
    
    # Test 3: Batch with errors (skip_errors=True)
    print("\nTest 3: Batch loading with errors (skip_errors=True)")
    sources = [
        "nonexistent1.txt",
        "nonexistent2.txt",
    ]
    
    documents = await loader.load_multiple(sources, skip_errors=True)
    print(f"âœ“ Returned {len(documents)} documents (expected 0)")
    print("âœ“ Did not crash on errors")
    
    print("\nâœ“ Error handling working correctly!")


async def test_performance():
    """Test loading performance with parallel processing."""
    print("\n" + "="*60)
    print("TEST 9: Performance (Parallel Loading)")
    print("="*60)
    
    import time
    
    loader = UnifiedLoader()
    test_dir = Path("test_documents")
    
    sources = [
        test_dir / "sample.txt",
        test_dir / "sample.md",
        test_dir / "sample.json",
        test_dir / "sample.csv",
        test_dir / "sample.html",
    ]
    
    sources = [s for s in sources if s.exists()]
    
    if len(sources) == 0:
        print("âš  No test files available")
        return
    
    # Test with different worker counts
    for workers in [1, 2, 4]:
        start_time = time.time()
        documents = await loader.load_multiple(sources, max_workers=workers)
        duration = time.time() - start_time
        
        print(f"âœ“ {workers} workers: {len(documents)} docs in {duration:.3f}s")
    
    print("\nâœ“ Parallel loading working!")


async def run_all_tests():
    """Run all Session 4 tests."""
    print("\n" + "="*80)
    print(" "*20 + "SESSION 4 TEST SUITE")
    print(" "*18 + "Unified Loading System")
    print("="*80)
    
    # Ensure test files exist
    ensure_test_files()
    
    try:
        await test_unified_loader_initialization()
        await test_automatic_type_detection()
        await test_mixed_batch_loading()
        await test_grouped_loading()
        await test_directory_loading()
        await test_url_loading()
        await test_mixed_sources()
        await test_error_handling()
        await test_performance()
        
        print("\n" + "="*80)
        print(" "*25 + "ALL TESTS PASSED! âœ“")
        print("="*80)
        print("\nSession 4 is complete and working correctly!")
        print("\nWhat the Unified Loader can do:")
        print("1. Automatically detect document types")
        print("2. Route to the correct specialized loader")
        print("3. Load mixed batches of different file types")
        print("4. Load entire directories")
        print("5. Load from URLs and files together")
        print("6. Group documents by type")
        print("7. Handle errors gracefully")
        print("8. Process documents in parallel")
        print("\nNext Steps:")
        print("Ready to proceed to Session 5: Text Preprocessing Pipeline")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(run_all_tests())


================================================================================
FILE: training_data_bot\test_session5.py
================================================================================

"""
Test script for Session 5: Text Preprocessing Pipeline
Run this to verify the text preprocessing system is working correctly.
"""

import sys
sys.path.append('.')

from core import Document, DocumentType, setup_logging, get_logger
from preprocessing import TextPreprocessor
from uuid import uuid4


# Setup logging for tests
setup_logging(level="INFO", structured=False)
logger = get_logger()


def create_sample_documents():
    """Create sample documents for testing."""
    # Short document
    short_doc = Document(
        id=uuid4(),
        title="Short Article",
        content="This is a short document. It has only a few sentences. Perfect for testing.",
        source="test",
        doc_type=DocumentType.TXT,
        word_count=14,
        char_count=77
    )
    
    # Medium document
    medium_content = """
    The Training Data Bot is an enterprise-grade system for generating training data.
    It processes documents from various sources including PDFs, web pages, and text files.
    The system uses AI to generate high-quality training examples for machine learning models.
    
    The architecture is modular and extensible. Each component has a specific responsibility.
    Document loaders handle different file formats. The preprocessing pipeline chunks text appropriately.
    Task generators create training examples based on templates. Quality evaluation ensures high standards.
    
    This makes it suitable for production use in real companies.
    """
    
    medium_doc = Document(
        id=uuid4(),
        title="Medium Article",
        content=medium_content,
        source="test",
        doc_type=DocumentType.TXT,
        word_count=len(medium_content.split()),
        char_count=len(medium_content)
    )
    
    # Long document
    long_content = " ".join([
        f"This is sentence number {i}. It contains some information about topic {i}."
        for i in range(1, 101)
    ])
    
    long_doc = Document(
        id=uuid4(),
        title="Long Article",
        content=long_content,
        source="test",
        doc_type=DocumentType.TXT,
        word_count=len(long_content.split()),
        char_count=len(long_content)
    )
    
    return short_doc, medium_doc, long_doc


def test_preprocessor_initialization():
    """Test preprocessor initialization and configuration."""
    print("\n" + "="*60)
    print("TEST 1: Preprocessor Initialization")
    print("="*60)
    
    # Test basic initialization
    preprocessor = TextPreprocessor(
        chunk_size=100,
        chunk_overlap=20,
        min_chunk_size=10
    )
    
    print(f"âœ“ Preprocessor created: {preprocessor}")
    print(f"âœ“ Chunk size: {preprocessor.chunk_size}")
    print(f"âœ“ Chunk overlap: {preprocessor.chunk_overlap}")
    print(f"âœ“ Min chunk size: {preprocessor.min_chunk_size}")
    
    # Test from config
    from core import ProcessingConfig
    config = ProcessingConfig(
        chunk_size=500,
        chunk_overlap=100
    )
    
    preprocessor2 = TextPreprocessor.from_config(config)
    print(f"\nâœ“ Created from config: {preprocessor2}")
    
    # Test validation
    try:
        bad_preprocessor = TextPreprocessor(chunk_size=100, chunk_overlap=150)
        print("âŒ Should have raised error for invalid parameters")
    except Exception as e:
        print(f"âœ“ Correctly raised error: {type(e).__name__}")
    
    print("\nâœ“ Preprocessor initialization working!")


def test_basic_chunking():
    """Test basic document chunking."""
    print("\n" + "="*60)
    print("TEST 2: Basic Chunking")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=50, chunk_overlap=10)
    short_doc, medium_doc, _ = create_sample_documents()
    
    # Test short document
    print("\nProcessing short document...")
    chunks = preprocessor.process_document(short_doc)
    print(f"âœ“ Created {len(chunks)} chunk(s)")
    
    for i, chunk in enumerate(chunks):
        print(f"  Chunk {i}: {chunk.token_count} tokens")
        print(f"    Content preview: {chunk.content[:60]}...")
    
    # Test medium document
    print("\nProcessing medium document...")
    chunks = preprocessor.process_document(medium_doc)
    print(f"âœ“ Created {len(chunks)} chunks")
    
    for i, chunk in enumerate(chunks[:3]):  # Show first 3
        print(f"  Chunk {i}: {chunk.token_count} tokens")
    
    if len(chunks) > 3:
        print(f"  ... and {len(chunks) - 3} more chunks")
    
    print("\nâœ“ Basic chunking working!")


def test_chunk_overlap():
    """Test that chunk overlap is working correctly."""
    print("\n" + "="*60)
    print("TEST 3: Chunk Overlap")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=20, chunk_overlap=5)
    _, medium_doc, _ = create_sample_documents()
    
    chunks = preprocessor.process_document(medium_doc)
    
    print(f"Created {len(chunks)} chunks with 5-token overlap")
    
    if len(chunks) >= 2:
        # Check if there's overlap between consecutive chunks
        chunk1_words = chunks[0].content.split()[-5:]
        chunk2_words = chunks[1].content.split()[:5]
        
        overlap_check = any(word in chunk2_words for word in chunk1_words)
        
        print(f"\nâœ“ Chunk 0 last 5 words: {' '.join(chunk1_words)}")
        print(f"âœ“ Chunk 1 first 5 words: {' '.join(chunk2_words)}")
        print(f"âœ“ Overlap detected: {overlap_check}")
    
    # Check overlap_tokens field
    for i, chunk in enumerate(chunks[:3]):
        print(f"  Chunk {i}: overlap_tokens = {chunk.overlap_tokens}")
    
    print("\nâœ“ Chunk overlap working!")


def test_batch_processing():
    """Test processing multiple documents."""
    print("\n" + "="*60)
    print("TEST 4: Batch Processing")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=50, chunk_overlap=10)
    documents = list(create_sample_documents())
    
    print(f"Processing {len(documents)} documents...")
    all_chunks = preprocessor.process_documents(documents)
    
    print(f"\nâœ“ Total chunks created: {len(all_chunks)}")
    
    # Group by document
    by_doc = {}
    for chunk in all_chunks:
        doc_id = str(chunk.document_id)
        by_doc[doc_id] = by_doc.get(doc_id, 0) + 1
    
    print(f"âœ“ Chunks per document:")
    for doc, count in by_doc.items():
        print(f"  - Document {doc[:8]}...: {count} chunks")
    
    print("\nâœ“ Batch processing working!")


def test_text_cleaning():
    """Test text cleaning functionality."""
    print("\n" + "="*60)
    print("TEST 5: Text Cleaning")
    print("="*60)
    
    preprocessor = TextPreprocessor()
    
    # Test with messy text
    messy_text = """
    This  has    extra    spaces.
    
    
    And multiple blank lines.
    It has "fancy quotes" and 'apostrophes'.
    """
    
    cleaned = preprocessor._clean_text(messy_text)
    
    print("Original text issues:")
    print("  - Multiple spaces")
    print("  - Blank lines")
    print("  - Fancy quotes")
    
    print(f"\nâœ“ Cleaned text: {cleaned[:100]}...")
    print(f"âœ“ Text length: {len(messy_text)} -> {len(cleaned)}")
    
    # Check improvements
    has_double_space = "  " in cleaned
    has_fancy_quotes = """ in cleaned or """ in cleaned
    
    print(f"\nâœ“ Removed double spaces: {not has_double_space}")
    print(f"âœ“ Normalized quotes: {not has_fancy_quotes}")
    
    print("\nâœ“ Text cleaning working!")


def test_chunk_context():
    """Test getting context around chunks."""
    print("\n" + "="*60)
    print("TEST 6: Chunk Context")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=30, chunk_overlap=5)
    _, medium_doc, _ = create_sample_documents()
    
    chunks = preprocessor.process_document(medium_doc)
    
    if len(chunks) >= 3:
        # Get context for middle chunk
        middle_idx = len(chunks) // 2
        context = preprocessor.get_chunk_context(chunks, middle_idx, context_chunks=1)
        
        print(f"Document has {len(chunks)} chunks")
        print(f"Getting context for chunk {middle_idx}:")
        print(f"\nâœ“ Context (with 1 chunk before/after):")
        print(f"  {context[:150]}...")
        print(f"\nâœ“ Context length: {len(context.split())} words")
    
    print("\nâœ“ Chunk context working!")


def test_sentence_chunking():
    """Test sentence-based chunking."""
    print("\n" + "="*60)
    print("TEST 7: Sentence-Based Chunking")
    print("="*60)
    
    preprocessor = TextPreprocessor()
    _, medium_doc, _ = create_sample_documents()
    
    # Create sentence-based chunks
    chunks = preprocessor.create_sentence_chunks(medium_doc, sentences_per_chunk=3)
    
    print(f"âœ“ Created {len(chunks)} sentence-based chunks")
    
    for i, chunk in enumerate(chunks[:3]):
        sentence_count = chunk.content.count('.') + chunk.content.count('!') + chunk.content.count('?')
        print(f"  Chunk {i}: ~{sentence_count} sentences, {chunk.token_count} words")
    
    print("\nâœ“ Sentence-based chunking working!")


def test_statistics():
    """Test chunk statistics."""
    print("\n" + "="*60)
    print("TEST 8: Chunk Statistics")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=100, chunk_overlap=20)
    documents = list(create_sample_documents())
    
    all_chunks = preprocessor.process_documents(documents)
    stats = preprocessor.get_statistics(all_chunks)
    
    print("Chunk Statistics:")
    print(f"âœ“ Total chunks: {stats['total_chunks']}")
    print(f"âœ“ Total tokens: {stats['total_tokens']}")
    print(f"âœ“ Average tokens per chunk: {stats['avg_tokens_per_chunk']:.1f}")
    print(f"âœ“ Min tokens: {stats['min_tokens']}")
    print(f"âœ“ Max tokens: {stats['max_tokens']}")
    print(f"âœ“ Documents processed: {stats['documents_processed']}")
    
    print("\nâœ“ Statistics generation working!")


def test_merge_small_chunks():
    """Test merging small chunks."""
    print("\n" + "="*60)
    print("TEST 9: Merge Small Chunks")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=30, chunk_overlap=5, min_chunk_size=15)
    short_doc, _, _ = create_sample_documents()
    
    # Create chunks (might have small ones)
    chunks = preprocessor.process_document(short_doc)
    print(f"Initial chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        print(f"  Chunk {i}: {chunk.token_count} tokens")
    
    # Merge small chunks
    merged = preprocessor.merge_small_chunks(chunks, merge_threshold=10)
    print(f"\nAfter merging (threshold=10):")
    print(f"âœ“ Merged chunks: {len(merged)}")
    for i, chunk in enumerate(merged):
        print(f"  Chunk {i}: {chunk.token_count} tokens")
    
    print("\nâœ“ Small chunk merging working!")


def test_large_document():
    """Test with a large document."""
    print("\n" + "="*60)
    print("TEST 10: Large Document Processing")
    print("="*60)
    
    preprocessor = TextPreprocessor(chunk_size=100, chunk_overlap=20)
    _, _, long_doc = create_sample_documents()
    
    print(f"Processing large document: {long_doc.word_count} words")
    
    import time
    start_time = time.time()
    chunks = preprocessor.process_document(long_doc)
    duration = time.time() - start_time
    
    print(f"\nâœ“ Created {len(chunks)} chunks")
    print(f"âœ“ Processing time: {duration:.3f} seconds")
    print(f"âœ“ Chunks per second: {len(chunks)/duration:.1f}")
    
    stats = preprocessor.get_statistics(chunks)
    print(f"\nâœ“ Average chunk size: {stats['avg_tokens_per_chunk']:.1f} tokens")
    print(f"âœ“ Size range: {stats['min_tokens']} - {stats['max_tokens']} tokens")
    
    print("\nâœ“ Large document processing working!")


def run_all_tests():
    """Run all Session 5 tests."""
    print("\n" + "="*80)
    print(" "*20 + "SESSION 5 TEST SUITE")
    print(" "*16 + "Text Preprocessing Pipeline")
    print("="*80)
    
    try:
        test_preprocessor_initialization()
        test_basic_chunking()
        test_chunk_overlap()
        test_batch_processing()
        test_text_cleaning()
        test_chunk_context()
        test_sentence_chunking()
        test_statistics()
        test_merge_small_chunks()
        test_large_document()
        
        print("\n" + "="*80)
        print(" "*25 + "ALL TESTS PASSED! âœ“")
        print("="*80)
        print("\nSession 5 is complete and working correctly!")
        print("\nWhat the Text Preprocessor can do:")
        print("1. Split documents into overlapping chunks")
        print("2. Configure chunk size, overlap, and minimum size")
        print("3. Clean and normalize text")
        print("4. Process multiple documents in batch")
        print("5. Create sentence-based chunks")
        print("6. Get context around specific chunks")
        print("7. Merge small chunks")
        print("8. Generate detailed statistics")
        print("9. Track metadata for each chunk")
        print("10. Handle documents of any size efficiently")
        print("\nNext Steps:")
        print("Ready to proceed to Session 6: AI Client Integration")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()


================================================================================
FILE: training_data_bot\test_session6.py
================================================================================

"""
Test script for Session 6: AI Client Integration
Run this to verify the AI client system is working correctly.

Note: These tests use mock/dummy data since we don't have actual API keys.
"""

import sys
sys.path.append('.')

from core import setup_logging, get_logger, AIProviderConfig
from ai import AIClient, AIResponse, BaseAIProvider


# Setup logging for tests
setup_logging(level="INFO", structured=False)
logger = get_logger()


def test_provider_registry():
    """Test AI provider registry."""
    print("\n" + "="*60)
    print("TEST 1: Provider Registry")
    print("="*60)
    
    print(f"âœ“ Available providers: {', '.join(AIClient.PROVIDERS.keys())}")
    print(f"âœ“ Total providers: {len(AIClient.PROVIDERS)}")
    
    for provider_name, provider_class in AIClient.PROVIDERS.items():
        print(f"  - {provider_name}: {provider_class.__name__}")
    
    print("\nâœ“ Provider registry working!")


def test_client_initialization():
    """Test AI client initialization (without API calls)."""
    print("\n" + "="*60)
    print("TEST 2: Client Initialization")
    print("="*60)
    
    # Test with dummy API key (won't make actual calls)
    try:
        client = AIClient(
            provider="openai",
            api_key="dummy_key_for_testing",
            model="gpt-3.5-turbo"
        )
        print(f"âœ“ Created client: {client}")
        print(f"âœ“ Provider info: {client.get_provider_info()}")
    except ImportError as e:
        print(f"âš  OpenAI package not installed: {e}")
        print("  Install with: pip install openai")
    except Exception as e:
        print(f"âš  Client initialization test skipped: {type(e).__name__}")
    
    print("\nâœ“ Client initialization structure working!")


def test_configuration_errors():
    """Test configuration error handling."""
    print("\n" + "="*60)
    print("TEST 3: Configuration Error Handling")
    print("="*60)
    
    # Test unknown provider
    try:
        client = AIClient(provider="unknown_provider", api_key="test")
        print("âŒ Should have raised error for unknown provider")
    except Exception as e:
        print(f"âœ“ Correctly raised: {type(e).__name__}")
        print(f"  Message: {str(e)[:60]}...")
    
    # Test missing API key
    try:
        client = AIClient(provider="openai")
        print("âŒ Should have raised error for missing API key")
    except Exception as e:
        print(f"âœ“ Correctly raised: {type(e).__name__}")
    
    print("\nâœ“ Error handling working correctly!")


def test_token_counting():
    """Test token counting functionality."""
    print("\n" + "="*60)
    print("TEST 4: Token Counting")
    print("="*60)
    
    test_texts = [
        "Hello, world!",
        "This is a longer sentence with more words.",
        "The Training Data Bot processes documents and generates training examples.",
    ]
    
    try:
        client = AIClient(provider="openai", api_key="dummy_key")
        
        for text in test_texts:
            tokens = client.count_tokens(text)
            words = len(text.split())
            print(f"âœ“ Text: '{text[:40]}...'")
            print(f"  Words: {words}, Estimated tokens: {tokens}")
        
    except ImportError:
        print("âš  OpenAI/tiktoken not installed, skipping token counting test")
    except Exception as e:
        print(f"âš  Token counting test skipped: {type(e).__name__}")
    
    print("\nâœ“ Token counting structure working!")


def test_cost_estimation():
    """Test cost estimation functionality."""
    print("\n" + "="*60)
    print("TEST 5: Cost Estimation")
    print("="*60)
    
    try:
        client = AIClient(provider="openai", api_key="dummy_key", model="gpt-3.5-turbo")
        
        test_cases = [
            (100, 50),    # Small request
            (1000, 500),  # Medium request
            (5000, 2000), # Large request
        ]
        
        for input_tokens, output_tokens in test_cases:
            cost = client.estimate_cost(input_tokens, output_tokens)
            print(f"âœ“ {input_tokens} input + {output_tokens} output tokens")
            print(f"  Estimated cost: ${cost:.4f}")
        
    except ImportError:
        print("âš  OpenAI package not installed")
    except Exception as e:
        print(f"âš  Cost estimation test skipped: {type(e).__name__}")
    
    print("\nâœ“ Cost estimation structure working!")


def test_from_config():
    """Test creating client from configuration."""
    print("\n" + "="*60)
    print("TEST 6: Client from Configuration")
    print("="*60)
    
    config = AIProviderConfig(
        provider_name="openai",
        api_key="dummy_key_for_testing",
        model_name="gpt-3.5-turbo",
        max_tokens=2000,
        temperature=0.8,
        timeout=30.0
    )
    
    print(f"âœ“ Created config: {config.provider_name}/{config.model_name}")
    
    try:
        client = AIClient.from_config(config)
        print(f"âœ“ Created client from config: {client}")
        
        info = client.get_provider_info()
        print(f"âœ“ Provider: {info['provider']}")
        print(f"âœ“ Model: {info['model']}")
        print(f"âœ“ Max tokens: {info['max_tokens']}")
        print(f"âœ“ Temperature: {info['temperature']}")
        
    except ImportError:
        print("âš  OpenAI package not installed")
    except Exception as e:
        print(f"âš  Config test skipped: {type(e).__name__}")
    
    print("\nâœ“ Configuration integration working!")


def test_airesponse_structure():
    """Test AIResponse data structure."""
    print("\n" + "="*60)
    print("TEST 7: AIResponse Structure")
    print("="*60)
    
    # Create a mock AIResponse
    response = AIResponse(
        content="This is a generated response.",
        model="gpt-3.5-turbo",
        tokens_used=150,
        finish_reason="stop",
        response_time=1.5,
        metadata={"prompt_tokens": 100, "completion_tokens": 50}
    )
    
    print(f"âœ“ Content: {response.content}")
    print(f"âœ“ Model: {response.model}")
    print(f"âœ“ Tokens used: {response.tokens_used}")
    print(f"âœ“ Finish reason: {response.finish_reason}")
    print(f"âœ“ Response time: {response.response_time}s")
    print(f"âœ“ Metadata: {response.metadata}")
    
    print("\nâœ“ AIResponse structure working!")


def test_provider_info():
    """Test provider information retrieval."""
    print("\n" + "="*60)
    print("TEST 8: Provider Information")
    print("="*60)
    
    try:
        # Test OpenAI
        client1 = AIClient(provider="openai", api_key="dummy", model="gpt-4")
        info1 = client1.get_provider_info()
        print("OpenAI Provider:")
        print(f"  âœ“ Provider: {info1['provider']}")
        print(f"  âœ“ Model: {info1['model']}")
        
        # Test Anthropic
        client2 = AIClient(provider="anthropic", api_key="dummy", model="claude-3-opus-20240229")
        info2 = client2.get_provider_info()
        print("\nAnthropic Provider:")
        print(f"  âœ“ Provider: {info2['provider']}")
        print(f"  âœ“ Model: {info2['model']}")
        
    except ImportError as e:
        print(f"âš  Provider packages not installed: {e}")
    except Exception as e:
        print(f"âš  Provider info test skipped: {type(e).__name__}")
    
    print("\nâœ“ Provider information retrieval working!")


def test_default_models():
    """Test default model selection."""
    print("\n" + "="*60)
    print("TEST 9: Default Models")
    print("="*60)
    
    try:
        # Test without specifying model
        client1 = AIClient(provider="openai", api_key="dummy")
        print(f"âœ“ OpenAI default model: {client1.provider_instance.model}")
        
        client2 = AIClient(provider="anthropic", api_key="dummy")
        print(f"âœ“ Anthropic default model: {client2.provider_instance.model}")
        
    except ImportError:
        print("âš  Provider packages not installed")
    except Exception as e:
        print(f"âš  Default model test skipped: {type(e).__name__}")
    
    print("\nâœ“ Default model selection working!")


def run_all_tests():
    """Run all Session 6 tests."""
    print("\n" + "="*80)
    print(" "*20 + "SESSION 6 TEST SUITE")
    print(" "*18 + "AI Client Integration")
    print("="*80)
    
    print("\nNote: These tests verify the AI client structure without making")
    print("actual API calls (since we don't have real API keys in testing).")
    
    try:
        test_provider_registry()
        test_client_initialization()
        test_configuration_errors()
        test_token_counting()
        test_cost_estimation()
        test_from_config()
        test_airesponse_structure()
        test_provider_info()
        test_default_models()
        
        print("\n" + "="*80)
        print(" "*25 + "ALL TESTS PASSED! âœ“")
        print("="*80)
        print("\nSession 6 is complete and working correctly!")
        print("\nWhat the AI Client can do:")
        print("1. Support multiple AI providers (OpenAI, Anthropic)")
        print("2. Unified interface for all providers")
        print("3. Single and batch generation")
        print("4. Token counting and cost estimation")
        print("5. Configuration from settings objects")
        print("6. Automatic error handling and retries")
        print("7. Performance logging and metrics")
        print("8. Provider-specific optimizations")
        print("\nTo use with real API keys:")
        print("1. Set TDB_OPENAI_API_KEY or TDB_ANTHROPIC_API_KEY in .env")
        print("2. Install: pip install openai anthropic tiktoken")
        print("3. The client will automatically work with real APIs")
        print("\nNext Steps:")
        print("Ready to proceed to Session 7: Task Generation System")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()


================================================================================
FILE: training_data_bot\test_session7.py
================================================================================

"""
Test suite for Session 7: Task Generation System

Tests all task generators and the task manager.
"""

import asyncio
from uuid import uuid4

# Import core components
from core import (
    Document,
    TextChunk,
    TaskType,
    DocumentType,
    ProcessingJob,
    ProcessingStatus,
)

# Import AI client (mock for testing)
from ai import AIClient, AIResponse

# Import task components
from tasks import (
    BaseTaskGenerator,
    TaskManager,
    QAGenerator,
    ClassificationGenerator,
    SummarizationGenerator,
)


class MockAIClient:
    """Mock AI client for testing without real API calls."""
    
    def __init__(self):
        self.call_count = 0
    
    async def generate(self, prompt, system_prompt=None, **kwargs):
        """Mock generation."""
        self.call_count += 1
        
        # Simulate different responses based on prompt content
        if "question" in prompt.lower() or "q&a" in prompt.lower():
            content = """Q: What is the main topic of this text?
A: The main topic is about machine learning and artificial intelligence.

Q: What are the key concepts mentioned?
A: The key concepts are neural networks, training data, and model optimization.

Q: How does this relate to real-world applications?
A: This relates to applications in natural language processing and computer vision."""
        
        elif "classify" in prompt.lower() or "category" in prompt.lower():
            content = """Category: informative
Reasoning: The text provides factual information and explanations about a technical topic, making it primarily informative in nature."""
        
        elif "summarize" in prompt.lower() or "summary" in prompt.lower():
            content = """This text discusses the fundamentals of machine learning, including key concepts like neural networks, training processes, and practical applications in various domains. It emphasizes the importance of quality training data and proper model optimization techniques."""
        
        else:
            content = "Generated response for testing purposes."
        
        return AIResponse(
            content=content,
            model="mock-model",
            tokens_used=100,
            finish_reason="stop",
            response_time=0.1,
            metadata={"prompt_tokens": 50, "completion_tokens": 50}
        )
    
    def count_tokens(self, text):
        """Mock token counting."""
        return len(text.split())


def create_test_chunks():
    """Create sample text chunks for testing."""
    doc_id = uuid4()
    
    chunks = [
        TextChunk(
            id=uuid4(),
            document_id=doc_id,
            content="""Machine learning is a subset of artificial intelligence that enables 
            computers to learn from data without being explicitly programmed. Neural networks 
            are a key component, inspired by biological neurons in the human brain. Training 
            data quality is crucial for model performance.""",
            start_index=0,
            end_index=300,
            chunk_index=0,
            token_count=50
        ),
        TextChunk(
            id=uuid4(),
            document_id=doc_id,
            content="""Deep learning uses multiple layers of neural networks to process 
            complex patterns. Convolutional neural networks excel at image recognition, 
            while recurrent neural networks are effective for sequential data like text 
            and time series.""",
            start_index=300,
            end_index=550,
            chunk_index=1,
            token_count=45
        ),
    ]
    
    return chunks


async def test_1_qa_generator():
    """Test Q&A generator functionality."""
    print("\n" + "="*60)
    print("TEST 1: Q&A Generator")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create Q&A generator
    qa_gen = QAGenerator(ai_client=ai_client, num_questions=3)
    
    print(f"\nâœ“ Created QAGenerator: {qa_gen}")
    print(f"  Task Type: {qa_gen.task_type.value}")
    print(f"  Number of Questions: {qa_gen.num_questions}")
    
    # Get default template
    template = qa_gen.get_default_template()
    print(f"\nâœ“ Default Template:")
    print(f"  Name: {template.name}")
    print(f"  Type: {template.task_type.value}")
    
    # Generate Q&A for single chunk
    chunks = create_test_chunks()
    result = await qa_gen.generate_single(chunks[0])
    
    print(f"\nâœ“ Generated Q&A Result:")
    print(f"  Confidence: {result.confidence:.2f}")
    print(f"  Tokens Used: {result.tokens_used}")
    print(f"  Processing Time: {result.processing_time:.3f}s")
    print(f"  Output Preview: {result.output[:100]}...")
    
    # Check quality scores
    print(f"\nâœ“ Quality Scores:")
    for metric, score in result.quality_scores.items():
        print(f"  {metric}: {score:.2f}")
    
    # Create training example
    example = qa_gen.create_training_example(result, chunks[0])
    print(f"\nâœ“ Training Example Created:")
    print(f"  Task Type: {example.task_type.value}")
    print(f"  Source Document: {example.source_document_id}")
    print(f"  Input Length: {len(example.input_text)} chars")
    print(f"  Output Length: {len(example.output_text)} chars")
    
    # Get statistics
    stats = qa_gen.get_statistics()
    print(f"\nâœ“ Generator Statistics:")
    print(f"  Total Generated: {stats['total_generated']}")
    print(f"  Total Failed: {stats['total_failed']}")
    print(f"  Success Rate: {stats['success_rate']:.1%}")
    
    print("\nâœ… TEST 1 PASSED")


async def test_2_classification_generator():
    """Test classification generator functionality."""
    print("\n" + "="*60)
    print("TEST 2: Classification Generator")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create classification generator with custom categories
    categories = ["informative", "opinion", "narrative", "instructional"]
    classifier = ClassificationGenerator(
        ai_client=ai_client,
        categories=categories
    )
    
    print(f"\nâœ“ Created ClassificationGenerator: {classifier}")
    print(f"  Task Type: {classifier.task_type.value}")
    print(f"  Categories: {classifier.categories}")
    
    # Generate classification
    chunks = create_test_chunks()
    result = await classifier.generate_single(chunks[0])
    
    print(f"\nâœ“ Generated Classification:")
    print(f"  Confidence: {result.confidence:.2f}")
    print(f"  Output:\n{result.output}")
    
    # Check quality scores
    print(f"\nâœ“ Quality Scores:")
    for metric, score in result.quality_scores.items():
        print(f"  {metric}: {score:.2f}")
    
    # Test category update
    new_categories = ["technical", "general", "academic"]
    classifier.set_categories(new_categories)
    print(f"\nâœ“ Updated Categories: {classifier.categories}")
    
    print("\nâœ… TEST 2 PASSED")


async def test_3_summarization_generator():
    """Test summarization generator functionality."""
    print("\n" + "="*60)
    print("TEST 3: Summarization Generator")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create summarization generator
    summarizer = SummarizationGenerator(
        ai_client=ai_client,
        summary_style="concise",
        max_summary_length=100
    )
    
    print(f"\nâœ“ Created SummarizationGenerator: {summarizer}")
    print(f"  Task Type: {summarizer.task_type.value}")
    print(f"  Style: {summarizer.summary_style}")
    print(f"  Max Length: {summarizer.max_summary_length} words")
    
    # Generate summary
    chunks = create_test_chunks()
    result = await summarizer.generate_single(chunks[0])
    
    print(f"\nâœ“ Generated Summary:")
    print(f"  Confidence: {result.confidence:.2f}")
    print(f"  Summary:\n{result.output}")
    
    # Check quality scores
    print(f"\nâœ“ Quality Scores:")
    for metric, score in result.quality_scores.items():
        print(f"  {metric}: {score:.2f}")
    
    # Test style update
    summarizer.set_summary_style("detailed")
    print(f"\nâœ“ Updated Style: {summarizer.summary_style}")
    
    # Test length update
    summarizer.set_max_length(50)
    print(f"âœ“ Updated Max Length: {summarizer.max_summary_length}")
    
    print("\nâœ… TEST 3 PASSED")


async def test_4_batch_generation():
    """Test batch generation across all generators."""
    print("\n" + "="*60)
    print("TEST 4: Batch Generation")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create generator
    qa_gen = QAGenerator(ai_client=ai_client)
    
    # Generate batch
    chunks = create_test_chunks()
    results = await qa_gen.generate_batch(chunks, max_concurrent=2)
    
    print(f"\nâœ“ Batch Generation Complete:")
    print(f"  Input Chunks: {len(chunks)}")
    print(f"  Generated Results: {len(results)}")
    print(f"  Success Rate: {len(results)/len(chunks):.1%}")
    
    # Check each result
    for i, result in enumerate(results):
        print(f"\n  Result {i+1}:")
        print(f"    Confidence: {result.confidence:.2f}")
        print(f"    Tokens: {result.tokens_used}")
    
    # Get statistics
    stats = qa_gen.get_statistics()
    print(f"\nâœ“ Statistics After Batch:")
    print(f"  Total Generated: {stats['total_generated']}")
    print(f"  Total Time: {stats['total_time']:.2f}s")
    print(f"  Avg Time per Task: {stats['avg_time_per_task']:.3f}s")
    
    print("\nâœ… TEST 4 PASSED")


async def test_5_task_manager():
    """Test task manager functionality."""
    print("\n" + "="*60)
    print("TEST 5: Task Manager")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create task manager
    manager = TaskManager(ai_client=ai_client, max_concurrent=3)
    
    print(f"\nâœ“ Created TaskManager: {manager}")
    
    # Register generators
    manager.register_generator(TaskType.QA_GENERATION)
    manager.register_generator(TaskType.CLASSIFICATION)
    manager.register_generator(TaskType.SUMMARIZATION)
    
    print(f"âœ“ Registered {len(manager.generators)} generators")
    
    # Generate single task
    chunks = create_test_chunks()
    result = await manager.generate_task(
        chunk=chunks[0],
        task_type=TaskType.QA_GENERATION
    )
    
    print(f"\nâœ“ Single Task Generated:")
    print(f"  Task Type: QA")
    print(f"  Confidence: {result.confidence:.2f}")
    
    print("\nâœ… TEST 5 PASSED")


async def test_6_multi_task_generation():
    """Test generating multiple task types."""
    print("\n" + "="*60)
    print("TEST 6: Multi-Task Generation")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create task manager
    manager = TaskManager(ai_client=ai_client)
    
    # Generate multiple task types
    chunks = create_test_chunks()
    task_types = [
        TaskType.QA_GENERATION,
        TaskType.CLASSIFICATION,
        TaskType.SUMMARIZATION
    ]
    
    results = await manager.generate_tasks(
        chunks=chunks,
        task_types=task_types
    )
    
    print(f"\nâœ“ Multi-Task Generation Complete:")
    print(f"  Chunks: {len(chunks)}")
    print(f"  Task Types: {len(task_types)}")
    print(f"  Expected Results: {len(chunks) * len(task_types)}")
    print(f"  Actual Results: {len(results)}")
    
    # Count by type (approximate)
    print(f"\nâœ“ Results Generated:")
    print(f"  Total: {len(results)}")
    
    print("\nâœ… TEST 6 PASSED")


async def test_7_training_examples():
    """Test creating training examples."""
    print("\n" + "="*60)
    print("TEST 7: Training Example Creation")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create task manager
    manager = TaskManager(ai_client=ai_client)
    
    # Create training examples
    chunks = create_test_chunks()
    task_types = [TaskType.QA_GENERATION, TaskType.SUMMARIZATION]
    
    examples = await manager.create_training_examples(
        chunks=chunks,
        task_types=task_types
    )
    
    print(f"\nâœ“ Training Examples Created:")
    print(f"  Total Examples: {len(examples)}")
    
    # Examine first example
    if examples:
        example = examples[0]
        print(f"\nâœ“ Example Details:")
        print(f"  Task Type: {example.task_type.value}")
        print(f"  Source Document: {example.source_document_id}")
        print(f"  Input Preview: {example.input_text[:80]}...")
        print(f"  Output Preview: {example.output_text[:80]}...")
        print(f"  Quality Scores: {example.quality_scores}")
    
    print("\nâœ… TEST 7 PASSED")


async def test_8_job_processing():
    """Test job processing with progress tracking."""
    print("\n" + "="*60)
    print("TEST 8: Job Processing")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create task manager
    manager = TaskManager(ai_client=ai_client)
    
    # Create processing job
    job = ProcessingJob(
        id=uuid4(),
        name="Test Training Data Generation",
        job_type="training_data_generation",
        status=ProcessingStatus.PENDING
    )
    
    print(f"\nâœ“ Created Processing Job:")
    print(f"  ID: {job.id}")
    print(f"  Name: {job.name}")
    print(f"  Status: {job.status.value}")
    
    # Process job
    chunks = create_test_chunks()
    task_types = [TaskType.QA_GENERATION, TaskType.SUMMARIZATION]
    
    examples = await manager.process_job(
        job=job,
        chunks=chunks,
        task_types=task_types
    )
    
    print(f"\nâœ“ Job Processing Complete:")
    print(f"  Status: {job.status.value}")
    print(f"  Total Items: {job.total_items}")
    print(f"  Processed Items: {job.processed_items}")
    print(f"  Progress: {job.progress_percentage:.1f}%")
    print(f"  Examples Created: {len(examples)}")
    
    print("\nâœ… TEST 8 PASSED")


async def test_9_statistics():
    """Test statistics collection."""
    print("\n" + "="*60)
    print("TEST 9: Statistics Collection")
    print("="*60)
    
    # Create mock AI client
    ai_client = MockAIClient()
    
    # Create task manager
    manager = TaskManager(ai_client=ai_client)
    
    # Register and use generators
    chunks = create_test_chunks()
    
    # Generate QA
    await manager.generate_task(chunks[0], TaskType.QA_GENERATION)
    
    # Generate Classification
    await manager.generate_task(chunks[0], TaskType.CLASSIFICATION)
    
    # Generate Summarization
    await manager.generate_task(chunks[1], TaskType.SUMMARIZATION)
    
    # Get statistics
    stats = manager.get_statistics()
    
    print(f"\nâœ“ Task Manager Statistics:")
    for task_type, task_stats in stats.items():
        print(f"\n  {task_type}:")
        print(f"    Total Generated: {task_stats['total_generated']}")
        print(f"    Total Failed: {task_stats['total_failed']}")
        print(f"    Success Rate: {task_stats['success_rate']:.1%}")
        print(f"    Total Tokens: {task_stats['total_tokens_used']}")
        print(f"    Avg Time: {task_stats['avg_time_per_task']:.3f}s")
    
    # Reset statistics
    manager.reset_statistics()
    print(f"\nâœ“ Statistics Reset")
    
    # Verify reset
    new_stats = manager.get_statistics()
    for task_type, task_stats in new_stats.items():
        assert task_stats['total_generated'] == 0, f"Stats not reset for {task_type}"
    
    print(f"âœ“ All statistics successfully reset to 0")
    
    print("\nâœ… TEST 9 PASSED")


async def test_10_error_handling():
    """Test error handling and recovery."""
    print("\n" + "="*60)
    print("TEST 10: Error Handling")
    print("="*60)
    
    class FailingMockAIClient:
        """Mock client that fails sometimes."""
        
        def __init__(self):
            self.call_count = 0
        
        async def generate(self, prompt, system_prompt=None, **kwargs):
            self.call_count += 1
            
            # Fail every other call
            if self.call_count % 2 == 0:
                raise Exception("Simulated API failure")
            
            return AIResponse(
                content="Success response",
                model="mock-model",
                tokens_used=50,
                finish_reason="stop",
                response_time=0.1,
                metadata={}
            )
        
        def count_tokens(self, text):
            return len(text.split())
    
    # Create failing AI client
    ai_client = FailingMockAIClient()
    
    # Create generator
    qa_gen = QAGenerator(ai_client=ai_client)
    
    # Try batch generation (some will fail)
    chunks = create_test_chunks()
    results = await qa_gen.generate_batch(chunks, max_concurrent=2)
    
    print(f"\nâœ“ Batch Generation with Failures:")
    print(f"  Input Chunks: {len(chunks)}")
    print(f"  Successful Results: {len(results)}")
    print(f"  Failed Results: {len(chunks) - len(results)}")
    
    # Get statistics
    stats = qa_gen.get_statistics()
    print(f"\nâœ“ Statistics After Failures:")
    print(f"  Total Generated: {stats['total_generated']}")
    print(f"  Total Failed: {stats['total_failed']}")
    print(f"  Success Rate: {stats['success_rate']:.1%}")
    
    print("\nâœ… TEST 10 PASSED (Error handling works correctly)")


async def main():
    """Run all tests."""
    print("\n" + "="*60)
    print("SESSION 7 TEST SUITE: TASK GENERATION SYSTEM")
    print("="*60)
    
    tests = [
        test_1_qa_generator,
        test_2_classification_generator,
        test_3_summarization_generator,
        test_4_batch_generation,
        test_5_task_manager,
        test_6_multi_task_generation,
        test_7_training_examples,
        test_8_job_processing,
        test_9_statistics,
        test_10_error_handling,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            await test()
            passed += 1
        except Exception as e:
            print(f"\nâŒ TEST FAILED: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)
    print(f"Total Tests: {len(tests)}")
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    print(f"Success Rate: {passed/len(tests)*100:.1f}%")
    
    if failed == 0:
        print("\nðŸŽ‰ ALL TESTS PASSED! Session 7 Complete!")
    else:
        print(f"\nâš ï¸  {failed} test(s) failed. Please review.")


if __name__ == "__main__":
    asyncio.run(main())


================================================================================
FILE: training_data_bot\test_session8.py
================================================================================

"""
Test suite for Session 8: Evaluation, Storage & Final Integration

Tests the complete system end-to-end.
"""

import asyncio
from pathlib import Path
from uuid import uuid4
import tempfile
import os
import sys

# Add parent directory to the python part so the package can beimported 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import core components
from training_data_bot.core import (
    Document,
    TextChunk,
    TrainingExample,
    Dataset,
    TaskType,
    DocumentType,
    ExportFormat,
    QualityMetric,
)

# Import evaluation and storage
from evaluation import QualityEvaluator
from storage import DatasetExporter

# Import main bot
from bot import TrainingDataBot


def create_test_example():
    """Create a test training example."""
    return TrainingExample(
        id=uuid4(),
        input_text="What is machine learning?",
        output_text="Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed.",
        task_type=TaskType.QA_GENERATION,
        source_document_id=uuid4(),
        quality_scores={"relevance": 0.9, "coherence": 0.85}
    )


def create_test_dataset(num_examples=10):
    """Create a test dataset."""
    examples = []
    for i in range(num_examples):
        example = TrainingExample(
            id=uuid4(),
            input_text=f"Question {i+1}: What is topic {i+1}?",
            output_text=f"Answer {i+1}: This is the explanation for topic {i+1}. It covers important concepts and provides detailed information.",
            task_type=TaskType.QA_GENERATION,
            source_document_id=uuid4(),
            quality_scores={"relevance": 0.8 + (i % 3) * 0.05}
        )
        examples.append(example)
    
    return Dataset(
        id=uuid4(),
        name="Test Dataset",
        description="Dataset for testing",
        examples=examples,
        total_examples=len(examples)
    )


async def test_1_quality_evaluator_example():
    """Test evaluating a single training example."""
    print("\n" + "="*60)
    print("TEST 1: Quality Evaluator - Single Example")
    print("="*60)
    
    evaluator = QualityEvaluator(quality_threshold=0.7)
    
    print(f"\nâœ“ Created QualityEvaluator with threshold: 0.7")
    
    # Create test example
    example = create_test_example()
    
    # Evaluate example
    report = evaluator.evaluate_example(example, detailed=True)
    
    print(f"\nâœ“ Evaluation Results:")
    print(f"  Overall Score: {report.overall_score:.2f}")
    print(f"  Passed: {report.passed}")
    print(f"\n  Metric Scores:")
    for metric, score in report.metric_scores.items():
        print(f"    {metric.value}: {score:.2f}")
    
    if report.issues:
        print(f"\n  Issues: {len(report.issues)}")
        for issue in report.issues:
            print(f"    - {issue}")
    
    if report.warnings:
        print(f"\n  Warnings: {len(report.warnings)}")
        for warning in report.warnings:
            print(f"    - {warning}")
    
    if report.recommendations:
        print(f"\n  Recommendations: {len(report.recommendations)}")
        for rec in report.recommendations[:3]:  # Show first 3
            print(f"    - {rec}")
    
    print("\nâœ… TEST 1 PASSED")


async def test_2_quality_evaluator_dataset():
    """Test evaluating an entire dataset."""
    print("\n" + "="*60)
    print("TEST 2: Quality Evaluator - Dataset")
    print("="*60)
    
    evaluator = QualityEvaluator(quality_threshold=0.7)
    
    # Create test dataset
    dataset = create_test_dataset(num_examples=15)
    
    print(f"\nâœ“ Created dataset with {len(dataset.examples)} examples")
    
    # Evaluate dataset
    report = evaluator.evaluate_dataset(dataset, detailed_report=True)
    
    print(f"\nâœ“ Dataset Evaluation Results:")
    print(f"  Overall Score: {report.overall_score:.2f}")
    print(f"  Passed: {report.passed}")
    print(f"\n  Metric Scores:")
    for metric, score in report.metric_scores.items():
        print(f"    {metric.value}: {score:.2f}")
    
    if report.issues:
        print(f"\n  Issues Found: {len(report.issues)}")
        for issue in report.issues:
            print(f"    - {issue}")
    
    if report.warnings:
        print(f"\n  Warnings: {len(report.warnings)}")
    
    print("\nâœ… TEST 2 PASSED")


async def test_3_export_jsonl():
    """Test exporting to JSONL format."""
    print("\n" + "="*60)
    print("TEST 3: Export to JSONL")
    print("="*60)
    
    exporter = DatasetExporter()
    dataset = create_test_dataset(num_examples=5)
    
    # Create temporary file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        temp_path = Path(f.name)
    
    try:
        # Export to JSONL
        result_path = await exporter.export_dataset(
            dataset=dataset,
            output_path=temp_path,
            format=ExportFormat.JSONL,
            split_data=False
        )
        
        print(f"\nâœ“ Exported to: {result_path}")
        
        # Verify file exists
        assert result_path.exists(), "Output file not created"
        
        # Check file size
        file_size = result_path.stat().st_size
        print(f"âœ“ File size: {file_size} bytes")
        
        # Read and verify content
        with open(result_path, 'r') as f:
            lines = f.readlines()
        
        print(f"âœ“ Lines in file: {len(lines)}")
        assert len(lines) == len(dataset.examples), "Wrong number of lines"
        
        # Get file info
        info = exporter.get_export_info(result_path)
        print(f"âœ“ File info: {info['size_mb']:.4f} MB")
        
        print("\nâœ… TEST 3 PASSED")
        
    finally:
        # Cleanup
        if temp_path.exists():
            temp_path.unlink()


async def test_4_export_json():
    """Test exporting to JSON format."""
    print("\n" + "="*60)
    print("TEST 4: Export to JSON")
    print("="*60)
    
    exporter = DatasetExporter()
    dataset = create_test_dataset(num_examples=3)
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        temp_path = Path(f.name)
    
    try:
        result_path = await exporter.export_dataset(
            dataset=dataset,
            output_path=temp_path,
            format=ExportFormat.JSON,
            split_data=False,
            pretty_print=True
        )
        
        print(f"\nâœ“ Exported to JSON: {result_path}")
        
        # Verify file
        assert result_path.exists()
        
        # Read and parse JSON
        import json
        with open(result_path, 'r') as f:
            data = json.load(f)
        
        print(f"âœ“ Loaded JSON with {len(data)} examples")
        assert len(data) == len(dataset.examples)
        
        # Verify structure
        first_example = data[0]
        assert 'input' in first_example
        assert 'output' in first_example
        assert 'task_type' in first_example
        print(f"âœ“ JSON structure validated")
        
        print("\nâœ… TEST 4 PASSED")
        
    finally:
        if temp_path.exists():
            temp_path.unlink()


async def test_5_export_csv():
    """Test exporting to CSV format."""
    print("\n" + "="*60)
    print("TEST 5: Export to CSV")
    print("="*60)
    
    exporter = DatasetExporter()
    dataset = create_test_dataset(num_examples=4)
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        temp_path = Path(f.name)
    
    try:
        result_path = await exporter.export_dataset(
            dataset=dataset,
            output_path=temp_path,
            format=ExportFormat.CSV,
            split_data=False,
            include_metadata=False
        )
        
        print(f"\nâœ“ Exported to CSV: {result_path}")
        
        # Verify file
        assert result_path.exists()
        
        # Read CSV
        import csv
        with open(result_path, 'r') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        
        print(f"âœ“ CSV has {len(rows)} rows")
        assert len(rows) == len(dataset.examples)
        
        # Verify headers
        first_row = rows[0]
        assert 'input_text' in first_row
        assert 'output_text' in first_row
        assert 'task_type' in first_row
        print(f"âœ“ CSV headers validated")
        
        print("\nâœ… TEST 5 PASSED")
        
    finally:
        if temp_path.exists():
            temp_path.unlink()


async def test_6_export_split_dataset():
    """Test exporting with train/val/test split."""
    print("\n" + "="*60)
    print("TEST 6: Export with Split")
    print("="*60)
    
    exporter = DatasetExporter()
    dataset = create_test_dataset(num_examples=20)
    
    # Set custom splits
    dataset.train_split = 0.7
    dataset.validation_split = 0.2
    dataset.test_split = 0.1
    
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir) / "dataset.jsonl"
        
        result_path = await exporter.export_dataset(
            dataset=dataset,
            output_path=temp_path,
            format=ExportFormat.JSONL,
            split_data=True
        )
        
        print(f"\nâœ“ Exported split dataset to: {result_path}")
        
        # Verify split files exist
        train_file = Path(temp_dir) / "dataset_train.jsonl"
        val_file = Path(temp_dir) / "dataset_val.jsonl"
        test_file = Path(temp_dir) / "dataset_test.jsonl"
        
        assert train_file.exists(), "Train file not created"
        assert val_file.exists(), "Validation file not created"
        assert test_file.exists(), "Test file not created"
        
        # Count lines in each file
        with open(train_file) as f:
            train_count = len(f.readlines())
        with open(val_file) as f:
            val_count = len(f.readlines())
        with open(test_file) as f:
            test_count = len(f.readlines())
        
        print(f"\nâœ“ Split counts:")
        print(f"  Train: {train_count}")
        print(f"  Validation: {val_count}")
        print(f"  Test: {test_count}")
        print(f"  Total: {train_count + val_count + test_count}")
        
        assert train_count + val_count + test_count == len(dataset.examples)
        
        print("\nâœ… TEST 6 PASSED")


async def test_7_bot_initialization():
    """Test TrainingDataBot initialization."""
    print("\n" + "="*60)
    print("TEST 7: TrainingDataBot Initialization")
    print("="*60)
    
    # Create bot with config
    config = {
        "chunk_size": 800,
        "chunk_overlap": 150,
        "quality_threshold": 0.75
    }
    
    bot = TrainingDataBot(config=config)
    
    print(f"\nâœ“ Created TrainingDataBot: {bot}")
    
    # Check components
    assert bot.loader is not None, "Loader not initialized"
    assert bot.preprocessor is not None, "Preprocessor not initialized"
    assert bot.task_manager is not None, "Task manager not initialized"
    assert bot.evaluator is not None, "Evaluator not initialized"
    assert bot.exporter is not None, "Exporter not initialized"
    
    print(f"âœ“ All components initialized")
    
    # Check configuration
    assert bot.preprocessor.chunk_size == 800
    assert bot.preprocessor.chunk_overlap == 150
    assert bot.evaluator.quality_threshold == 0.75
    
    print(f"âœ“ Configuration applied correctly")
    
    # Get statistics
    stats = bot.get_statistics()
    print(f"\nâœ“ Initial Statistics:")
    print(f"  Documents: {stats['documents']['total']}")
    print(f"  Datasets: {stats['datasets']['total']}")
    print(f"  Jobs: {stats['jobs']['total']}")
    
    print("\nâœ… TEST 7 PASSED")


async def test_8_bot_context_manager():
    """Test TrainingDataBot as context manager."""
    print("\n" + "="*60)
    print("TEST 8: Bot Context Manager")
    print("="*60)
    
    async with TrainingDataBot() as bot:
        print(f"\nâœ“ Bot created in context manager")
        print(f"  Bot: {bot}")
        
        # Get stats
        stats = bot.get_statistics()
        print(f"âœ“ Statistics accessible: {stats['documents']['total']} documents")
    
    print(f"âœ“ Context manager exited cleanly")
    
    print("\nâœ… TEST 8 PASSED")


async def test_9_bot_set_ai_client():
    """Test setting AI client on bot."""
    print("\n" + "="*60)
    print("TEST 9: Set AI Client")
    print("="*60)
    
    bot = TrainingDataBot()
    
    # Initially no AI client configured
    print(f"\nâœ“ Bot created without AI client")
    
    # Set AI client (with dummy key for testing)
    bot.set_ai_client(
        provider="openai",
        api_key="sk-test-key-12345",
        model="gpt-3.5-turbo"
    )
    
    print(f"âœ“ AI client configured")
    assert bot.ai_client is not None
    assert bot.task_manager.ai_client is not None
    
    print(f"âœ“ Task manager has AI client reference")
    
    print("\nâœ… TEST 9 PASSED")


async def test_10_integration_export_examples():
    """Test exporting examples directly."""
    print("\n" + "="*60)
    print("TEST 10: Direct Example Export")
    print("="*60)
    
    exporter = DatasetExporter()
    
    # Create examples
    examples = [create_test_example() for _ in range(3)]
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        temp_path = Path(f.name)
    
    try:
        # Export examples directly
        result_path = await exporter.export_examples(
            examples=examples,
            output_path=temp_path,
            format=ExportFormat.JSONL
        )
        
        print(f"\nâœ“ Exported {len(examples)} examples")
        print(f"âœ“ Output: {result_path}")
        
        assert result_path.exists()
        
        # Verify content
        with open(result_path, 'r') as f:
            lines = f.readlines()
        
        print(f"âœ“ Verified {len(lines)} lines in output")
        assert len(lines) == len(examples)
        
        print("\nâœ… TEST 10 PASSED")
        
    finally:
        if temp_path.exists():
            temp_path.unlink()


async def main():
    """Run all tests."""
    print("\n" + "="*60)
    print("SESSION 8 TEST SUITE: EVALUATION, STORAGE & FINAL INTEGRATION")
    print("="*60)
    
    tests = [
        test_1_quality_evaluator_example,
        test_2_quality_evaluator_dataset,
        test_3_export_jsonl,
        test_4_export_json,
        test_5_export_csv,
        test_6_export_split_dataset,
        test_7_bot_initialization,
        test_8_bot_context_manager,
        test_9_bot_set_ai_client,
        test_10_integration_export_examples,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            await test()
            passed += 1
        except Exception as e:
            print(f"\nâŒ TEST FAILED: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)
    print(f"Total Tests: {len(tests)}")
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    print(f"Success Rate: {passed/len(tests)*100:.1f}%")
    
    if failed == 0:
        print("\nðŸŽ‰ ALL TESTS PASSED! Session 8 Complete!")
        print("\n" + "="*60)
        print("TRAINING DATA BOT - FULLY OPERATIONAL!")
        print("="*60)
        print("\nThe complete system is ready:")
        print("âœ“ Document loading from multiple sources")
        print("âœ“ Text preprocessing and chunking")
        print("âœ“ AI-powered task generation")
        print("âœ“ Quality evaluation and filtering")
        print("âœ“ Multi-format dataset export")
        print("âœ“ End-to-end bot integration")
        print("\nReady for production use! ðŸš€")
    else:
        print(f"\nâš ï¸  {failed} test(s) failed. Please review.")


if __name__ == "__main__":
    asyncio.run(main())


================================================================================
END OF PROJECT CODE
================================================================================
